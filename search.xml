<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>MachineLearning(Ng)-LinearRegression-Programming</title>
      <link href="/2019/03/10/NgMLHomeWork-LinearRegression/"/>
      <url>/2019/03/10/NgMLHomeWork-LinearRegression/</url>
      
        <content type="html"><![CDATA[<p><strong><strong>Continue the study and update after completing the postgraduate entrance exam,more,good luck to me.</strong></strong></p><a id="more"></a><h3 id="warmUpExercise"><a href="#warmUpExercise" class="headerlink" title="warmUpExercise"></a>warmUpExercise</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A = <span class="built_in">eye</span>(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h3 id="computeCost-or-Multi"><a href="#computeCost-or-Multi" class="headerlink" title="computeCost(or Multi)"></a>computeCost(or Multi)</h3><script type="math/tex; mode=display">J_{(\theta_1,\theta_2,...,\theta_n)} = \frac 1{2m} \sum_{i=1}^n (h_{(\theta)}(x) - y)^2</script><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp =  sum(((X * theta - y).^<span class="number">2</span>));</span><br><span class="line">J = <span class="number">1</span> / (<span class="number">2</span>*m) * temp;</span><br></pre></td></tr></table></figure><h3 id="gradientDescent-or-Multi"><a href="#gradientDescent-or-Multi" class="headerlink" title="gradientDescent(or Multi)"></a>gradientDescent(or Multi)</h3><p><code>Repeat</code>{</p><script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{\partial J_{(\theta_1,\theta_2,...,\theta_n)}}{\partial \theta_j}</script><p>}</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">size</span>(X,<span class="number">2</span>)</span><br><span class="line">theta(<span class="built_in">i</span>) = GlobalTheta(<span class="built_in">i</span>) - alpha/m * sum((X*GlobalTheta-y).*X(:,<span class="built_in">i</span>));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="featureNormalize"><a href="#featureNormalize" class="headerlink" title="featureNormalize"></a>featureNormalize</h3><script type="math/tex; mode=display">X = \frac {X - mean} {avg}</script><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">size</span>(X,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">%X_norm(:,i) = (X(:,i) - (mean(X(:,i)))) / (std(X(:,i)))</span></span><br><span class="line">mu(<span class="built_in">i</span>) = <span class="built_in">mean</span>(X(:,<span class="built_in">i</span>));</span><br><span class="line">sigma(<span class="built_in">i</span>) = std(X(:,<span class="built_in">i</span>));</span><br><span class="line">X_norm(:,<span class="built_in">i</span>) = (X(:,<span class="built_in">i</span>) - mu(<span class="built_in">i</span>)) / sigma(<span class="built_in">i</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="normalEqn"><a href="#normalEqn" class="headerlink" title="normalEqn"></a>normalEqn</h3><script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^TY</script><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta = pinv(X' * X) * X' * y</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> MATLAB/Octave </tag>
            
            <tag> LinearRegression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记</title>
      <link href="/2019/03/01/TensorFlow-1/"/>
      <url>/2019/03/01/TensorFlow-1/</url>
      
        <content type="html"><![CDATA[<p><strong><strong>该笔记需要有一定的前置知识:高等数学，线性代数，概率论与数理统计，配合吴恩达&gt;机器学习，TensorFlow学习手册食用更佳( ´∀｀)σ</strong></strong></p><a id="more"></a><h1 id="什么是TensorFlow？"><a href="#什么是TensorFlow？" class="headerlink" title="什么是TensorFlow？"></a>什么是TensorFlow？</h1><blockquote><p>TensorFlow™ 是一个开放源代码软件库，用于进行高性能数值计算。借助其灵活的架构，用户可以轻松地将计算工作部署到多种平台（CPU、GPU、TPU）和设备（桌面设备、服务器集群、移动设备、边缘设备等）。TensorFlow™ 最初是由 Google Brain 团队（隶属于 Google 的 AI 部门）中的研究人员和工程师开发的，可为机器学习和深度学习提供强力支持，并且其灵活的数值计算核心广泛应用于许多其他科学领域。</p></blockquote><p>摘自<a href="https://www.tensorflow.org/?hl=zh_cn" target="_blank" rel="noopener">TensorFlow中文网</a></p><h1 id="什么是神经网络（以下简称NN）"><a href="#什么是神经网络（以下简称NN）" class="headerlink" title="什么是神经网络（以下简称NN）?"></a>什么是神经网络（以下简称NN）?</h1><blockquote><p>人工神经网络（英语：Artificial Neural Network，ANN），简称神经网络（Neural Network，NN）或类神经网络，在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统，通俗的讲就是具备学习功能。现代神经网络是一种非线性统计性数据建模工具。<br>神经网络的构筑理念是受到生物（人或其他动物）神经网络功能的运作启发而产生的。人工神经网络通常是通过一个基于数学统计学类型的学习方法（Learning Method）得以优化，所以也是数学统计学方法的一种实际应用，通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间，另一方面在人工智能学的人工感知领域，我们通过数学统计学的应用可以来做人工感知方面的决定问题（也就是说通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力），这种方法比起正式的逻辑学推理演算更具有优势。<br>和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被传统基于规则的编程所解决的。</p></blockquote><p>摘自<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener">维基百科人工神经网络</a></p><h1 id="使用TensorFlow的NN"><a href="#使用TensorFlow的NN" class="headerlink" title="使用TensorFlow的NN"></a>使用TensorFlow的NN</h1><ul><li>张量表示数据</li><li>计算图搭建网络</li><li>会话执行计算图</li><li>优化线上权重(参数)得到模型</li></ul><h2 id="张量-tensor"><a href="#张量-tensor" class="headerlink" title="张量(tensor)"></a>张量(tensor)</h2><p>tensor也是多重数组(列表)</p><p><strong><strong>阶</strong></strong>叫做tensor的维数</p><div class="table-container"><table><thead><tr><th>.</th><th>.</th><th>.</th><th>.</th></tr></thead><tbody><tr><td>0-D</td><td>0 阶</td><td>标量 scalar</td><td>1,2,3</td></tr><tr><td>1-D</td><td>1 阶</td><td>向量 vector</td><td>[1,2,3]</td></tr><tr><td>2-D</td><td>2 阶</td><td>矩阵 martix</td><td>[[1,2,3],[2,3,4],[3,4,5]]</td></tr><tr><td>n-D</td><td>n阶</td><td>张量tensor</td><td>[[[…</td></tr></tbody></table></div><h2 id="TensorFlow的第一个例子"><a href="#TensorFlow的第一个例子" class="headerlink" title="TensorFlow的第一个例子"></a>TensorFlow的第一个例子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.constant([<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result = a+b</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result</span><br><span class="line">&lt;tf.Tensor <span class="string">'add:0'</span> shape=(<span class="number">2</span>,) dtype=float32&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">      sess.run(result)</span><br><span class="line">array([<span class="number">4.</span>, <span class="number">6.</span>], dtype=float32)</span><br></pre></td></tr></table></figure><p>代码 <code>&lt;tf.Tensor &#39;add:0&#39; shape=(2,) dtype=float32&gt;</code>的解释:</p><p><code>add</code> $\to$ 节点名<br><code>0</code> $\to$ 0个输出<br><code>shape=</code> $\to$ 维度<br><code>(2,)</code> $\to$ 一维，长度2<br><code>dtype = float32</code> $\to$ 数据类型</p><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>计算图是指搭建神经网络的过程，<strong><strong>只搭建，不计算</strong></strong>。</p><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p>神经元是神经网络的基本单位</p><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1g0kp5ua3qrj30je0d6jsc.jpg" alt=""></p><h3 id="神经元的计算过程"><a href="#神经元的计算过程" class="headerlink" title="神经元的计算过程"></a>神经元的计算过程</h3><p> 上图为例,这个神经元的计算过程为：</p><script type="math/tex; mode=display">y = x_1\times w_1 + x_2 \times w_2</script><p> 写成矩阵乘法的形式：</p><script type="math/tex; mode=display">    X = \left(\matrix{x_1 \cr x_2\cr}\right),    W = \left(\matrix{w_1&w_2}\right)</script><script type="math/tex; mode=display">    Y = X \times W = \left(\matrix{x_1\cr x_2\cr } \right) \times \left(\matrix{ w_1 & w_2 }\right) =     x_1\times w_1 + x_2\times  w_2</script><h3 id="会话Session"><a href="#会话Session" class="headerlink" title="会话Session"></a>会话Session</h3><p><code>TensorFlow</code>的需要用<code>Session</code>的<code>with</code>结构来输出，执行完计算图的过程之后只能得到一个<code>TensorFlow</code>的<code>Objective</code>，如果执行输出，就会抛出一个<code>TypeError</code>的<code>exception</code></p><p><code>must be real number, not Tensor</code></p><h4 id="会话结构"><a href="#会话结构" class="headerlink" title="会话结构"></a>会话结构</h4> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(result)</span><br></pre></td></tr></table></figure><h3 id="NN结构的权重-参数"><a href="#NN结构的权重-参数" class="headerlink" title="NN结构的权重(参数)"></a>NN结构的权重(参数)</h3><p>一般情况下W由变量表示，一开始随机生成，随后通过神经网络的反向传播和损失函数（loss function）或代价函数（cost function）不断调整W直到模型最优。W的生成方式可用正态分布生成</p><p><code>w = tf.Variable(tf.random_normal([2,3],stddev = 2,mean = 0,seed = 1))</code></p><h4 id="参数解释"><a href="#参数解释" class="headerlink" title="参数解释"></a>参数解释</h4><ul><li><code>tf.random_normal()</code>指正态分布的W，还可用去掉过大偏离点的正态分布(大于标准差)<code>tf.turncated_normal()</code>和平均分布<code>tf.rnadom_uniform()</code></li><li><code>[2,3]</code>表示2*3的矩阵</li><li><code>stddev = 2</code>表示标准差为2</li><li><code>mean</code>表示均值</li><li><code>seed</code>表示随机种子</li></ul><p>其中，标准差，均值和随机种子是缺省的。</p><h2 id="神经网络的实现过程"><a href="#神经网络的实现过程" class="headerlink" title="神经网络的实现过程"></a>神经网络的实现过程</h2><ul><li><p>准备数据集，提取特征值，作为输入喂给神经网络</p></li><li><p>搭建NN结果，从输入到输出，先搭建计算图，再执行会话(向前传播过程)</p></li><li><p>大量特征数据喂给NN，迭代优化NN参数，直到模型最优(反向传播过程)</p></li><li><p>使用训练好的模型预测和分类</p></li></ul><h2 id="向前传播-搭建模型和推理的过程"><a href="#向前传播-搭建模型和推理的过程" class="headerlink" title="向前传播(搭建模型和推理的过程)"></a>向前传播(搭建模型和推理的过程)</h2><blockquote><p>例1(全连接网络)</p></blockquote><p>生产一批零件的体积$x_1$和重量$x_2$为特征值输入NN，通过NN后输出一个数值，NN已给出，要求写出计算过程。</p><p><img src="http://ww1.sinaimg.cn/large/006Uvlfagy1g0ndw0tm4gj30ye0i8afi.jpg" alt=""></p><p>计算过程：</p><script type="math/tex; mode=display">X = \left (\matrix {x_1 & x_2} \right ),W^{(1)} = \left (\matrix {w^{(1)}_{1,1} & w^{(1)}_{1,2} & w^{(1)}_{1,3} \cr w^{(1)}_{2,1} & w^{(1)}_{2,2} & w^{(1)}_{2,3} } \right)</script><script type="math/tex; mode=display">a^{(1)} = \left (\matrix {a_{1,1} & a_{1,2} & a_{1,3} } \right) = X * W^{(1)}</script><script type="math/tex; mode=display">W^{(2)} = \left (\matrix {w^{(2)}_{1,1} \cr w^{(2)}_{2,1} \cr w^{(2)}_{3,1} }\right)</script><script type="math/tex; mode=display">Y = a^{(1)} * W^{(2)}</script><h2 id="反向传播-训练神经网络参数"><a href="#反向传播-训练神经网络参数" class="headerlink" title="反向传播(训练神经网络参数)"></a>反向传播(训练神经网络参数)</h2><p>反向传播使用损失函数使得NN模型在训练函数上的损失最小</p><p>损失函数(loss) 预测值(y)和标准值(y_)的差距</p><p>均方误差MSE： </p><script type="math/tex; mode=display">MSE(y\_,y) = \frac { \sum_{i=1}^{n} (y\_ - y)^2} {n}</script><p>在<code>TensorFlow</code>的表示:</p><p><code>loss = tf.reduce_mean(tf.square(y_ - y))</code></p><blockquote><p>例1续： 设${x_1 + x_2 \leq 1表示该零件合格，要求构建神经网络(代码)}$</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#两层的简单神经网络(全连接)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment">### 反向传播算法的理解</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span> <span class="comment">#一次喂入神经网络的大小</span></span><br><span class="line">seed = <span class="number">23455</span> <span class="comment"># 随机种子</span></span><br><span class="line"><span class="comment">### 产生伪数据集</span></span><br><span class="line"><span class="comment"># 基于seed产生随机数</span></span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"><span class="comment">#随机数字返回32行2列的矩阵 表示体积和重量 作为输入数据集</span></span><br><span class="line">X = rng.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 从X这个32行2列的矩阵中 取出一行 判断 如果和小于1  赋值给Y=1 表示合格  否则Y=0表示不合格</span></span><br><span class="line"><span class="comment"># 作为输入数据集的标签(正确答案)</span></span><br><span class="line">Y = [[int(x0+x1 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x0,x1) <span class="keyword">in</span> X]</span><br><span class="line">print(<span class="string">"X:\n"</span>,X)</span><br><span class="line">print(<span class="string">"Y:\n"</span>,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的输入 参数 和输出 </span></span><br><span class="line"><span class="comment"># 向前传播过程</span></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">1</span>,seed=<span class="number">2</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">a = tf.matmul(x,w1)</span><br><span class="line">y = tf.matmul(a,w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数以及反向传播方法</span></span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"><span class="comment">#train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss)</span></span><br><span class="line"><span class="comment">#train_step = tf.train.AdadeltaOptimizer(0.001).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话 训练STEPS轮</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="comment">#输出未经训练的参数的取值</span></span><br><span class="line">    print(<span class="string">"W1:\n"</span>,sess.run(w1))</span><br><span class="line">    print(<span class="string">"W2:\n"</span>,sess.run(w2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line">    STEPS = <span class="number">6000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">        end = start +BATCH_SIZE</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x: X[start:end],y_:Y[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> ==<span class="number">0</span>:</span><br><span class="line">            total_loss = sess.run(loss,feed_dict=&#123;x:X,y_:Y&#125;)</span><br><span class="line">            print(<span class="string">"After %d traning steps .loss on all data is %g"</span> %(i,total_loss))</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 输出训练后的参数</span></span><br><span class="line">    print(<span class="string">'\n'</span>)</span><br><span class="line">    print(<span class="string">"W1:\n"</span>,sess.run(w1))</span><br><span class="line">    print(<span class="string">"W2:\n"</span>,sess.run(w2))</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">X:</span><br><span class="line"></span><br><span class="line"> [[0.83494319 0.11482951]</span><br><span class="line"></span><br><span class="line"> [0.66899751 0.46594987]</span><br><span class="line"></span><br><span class="line"> [0.60181666 0.58838408]</span><br><span class="line"></span><br><span class="line"> [0.31836656 0.20502072]</span><br><span class="line"></span><br><span class="line"> [0.87043944 0.02679395]</span><br><span class="line"></span><br><span class="line"> [0.41539811 0.43938369]</span><br><span class="line"></span><br><span class="line"> [0.68635684 0.24833404]</span><br><span class="line"></span><br><span class="line"> [0.97315228 0.68541849]</span><br><span class="line"></span><br><span class="line"> [0.03081617 0.89479913]</span><br><span class="line"></span><br><span class="line"> [0.24665715 0.28584862]</span><br><span class="line"></span><br><span class="line"> [0.31375667 0.47718349]</span><br><span class="line"></span><br><span class="line"> [0.56689254 0.77079148]</span><br><span class="line"></span><br><span class="line"> [0.7321604  0.35828963]</span><br><span class="line"></span><br><span class="line"> [0.15724842 0.94294584]</span><br><span class="line"></span><br><span class="line"> [0.34933722 0.84634483]</span><br><span class="line"></span><br><span class="line"> [0.50304053 0.81299619]</span><br><span class="line"></span><br><span class="line"> [0.23869886 0.9895604 ]</span><br><span class="line"></span><br><span class="line"> [0.4636501  0.32531094]</span><br><span class="line"></span><br><span class="line"> [0.36510487 0.97365522]</span><br><span class="line"></span><br><span class="line"> [0.73350238 0.83833013]</span><br><span class="line"></span><br><span class="line"> [0.61810158 0.12580353]</span><br><span class="line"></span><br><span class="line"> [0.59274817 0.18779828]</span><br><span class="line"></span><br><span class="line"> [0.87150299 0.34679501]</span><br><span class="line"></span><br><span class="line"> [0.25883219 0.50002932]</span><br><span class="line"></span><br><span class="line"> [0.75690948 0.83429824]</span><br><span class="line"></span><br><span class="line"> [0.29316649 0.05646578]</span><br><span class="line"></span><br><span class="line"> [0.10409134 0.88235166]</span><br><span class="line"></span><br><span class="line"> [0.06727785 0.57784761]</span><br><span class="line"></span><br><span class="line"> [0.38492705 0.48384792]</span><br><span class="line"></span><br><span class="line"> [0.69234428 0.19687348]</span><br><span class="line"></span><br><span class="line"> [0.42783492 0.73416985]</span><br><span class="line"></span><br><span class="line"> [0.09696069 0.04883936]]</span><br><span class="line"></span><br><span class="line">Y:</span><br><span class="line"></span><br><span class="line"> [[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]</span><br><span class="line"></span><br><span class="line">2019-02-27 10:49:25.877711: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line"></span><br><span class="line">W1:</span><br><span class="line"></span><br><span class="line"> [[-0.85811085 -0.19662298  0.13895045]</span><br><span class="line"></span><br><span class="line"> [-1.2212768  -0.40341285 -1.1454041 ]]</span><br><span class="line"></span><br><span class="line">W2:</span><br><span class="line"></span><br><span class="line"> [[-0.8113182 ]</span><br><span class="line"></span><br><span class="line"> [ 1.4845988 ]</span><br><span class="line"></span><br><span class="line"> [ 0.06532937]]</span><br><span class="line"></span><br><span class="line">After 0 traning steps .loss on all data is 0.396911</span><br><span class="line"></span><br><span class="line">After 500 traning steps .loss on all data is 0.390187</span><br><span class="line"></span><br><span class="line">After 1000 traning steps .loss on all data is 0.386858</span><br><span class="line"></span><br><span class="line">After 1500 traning steps .loss on all data is 0.385192</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W1:</span><br><span class="line"></span><br><span class="line"> [[-0.9150884  -0.09321376  0.14530948]</span><br><span class="line"></span><br><span class="line"> [-1.1892611  -0.4614115  -1.1490022 ]]</span><br><span class="line"></span><br><span class="line">W2:</span><br><span class="line"></span><br><span class="line"> [[-0.8259251 ]</span><br><span class="line"></span><br><span class="line"> [ 1.4913318 ]</span><br><span class="line"></span><br><span class="line"> [ 0.11993836]]</span><br></pre></td></tr></table></figure></p><h2 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h2><h3 id="普通神经元模型"><a href="#普通神经元模型" class="headerlink" title="普通神经元模型"></a>普通神经元模型</h3><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1g0ktlipq8oj30va0la76z.jpg" alt=""></p><h3 id="McCulloch-Pitts神经元模型"><a href="#McCulloch-Pitts神经元模型" class="headerlink" title="McCulloch-Pitts神经元模型"></a>McCulloch-Pitts神经元模型</h3><p><img src="http://ww1.sinaimg.cn/large/006Uvlfagy1g0ndnhlqasj30wg0q6ae9.jpg" alt=""></p><p>加入激活函数解决了线性模型不能解决的异或模型，其中$f(\sum_ {i}{} x_i*w_i + b)$是激活函数(activaction function),b是偏置项(bios)</p><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><ul><li>relu</li></ul><script type="math/tex; mode=display">f(x) = max(x,0) = \begin{cases}0 \qquad\qquad x \leq 0 \\ x \qquad\qquad x \geq 0\end{cases}</script><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1g0ku2ezaryj30ri0jcwf6.jpg" alt=""></p><ul><li>sigmoid</li></ul><script type="math/tex; mode=display">f(x) = \frac {1} {1+e^{-x} }</script><p><img src="http://ww1.sinaimg.cn/large/006Uvlfagy1g0ndo44q09j310c0hk76k.jpg" alt=""></p><ul><li>tanh</li></ul><script type="math/tex; mode=display">f(x) = \frac {1-e^{-2x} } {1 + e^{-2x} }</script><p><img src="http://ww1.sinaimg.cn/large/006Uvlfagy1g0ndoq53y1j31060iywh2.jpg" alt=""></p><h2 id="神经网络的优化"><a href="#神经网络的优化" class="headerlink" title="神经网络的优化"></a>神经网络的优化</h2><ul><li><p>损失函数loss</p></li><li><p>学习率 learning_rate</p></li><li><p>平均滑动 ema</p></li><li><p>正则化 regularization</p></li></ul><h3 id="损失函数（loss-预测值y-与已知答案-y-的差距"><a href="#损失函数（loss-预测值y-与已知答案-y-的差距" class="headerlink" title="损失函数（loss) 预测值y_与已知答案(y_)的差距"></a>损失函数（loss) 预测值y_与已知答案(y_)的差距</h3><p>NN的优化目标： loss最小</p><script type="math/tex; mode=display">损失函数 \to \begin{cases}  mse \quad(Mean Squared Error) \quad 均方误差 \\ 自定义 \\ ce \quad (Cross Entropy) \quad  交叉熵\end{cases}</script><h4 id="均方误差mse"><a href="#均方误差mse" class="headerlink" title="均方误差mse:"></a>均方误差mse:</h4><script type="math/tex; mode=display">MSE(y_,y) = \frac {\sum_ {i=1}^{n} (y\_ -y)^2} {n}</script><blockquote><p>例2：构建一层神经网络,预测酸奶日销量y, $x_1$,$x_2$ 是影响因素。</p></blockquote><p>分析: 建模前，应该采集的数据集有：每日的$x_1$和$x_2$ 销量y_。（在此例子中，我们你早数据X,Y_ : $ Y_ = X_1 + X_2 $ ，再加入-0.5~+0.5的噪声使得伪数据集更真实）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#两层的简单神经网络(全连接)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 酸奶预测</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span> <span class="comment">#一次喂入神经网络的大小</span></span><br><span class="line"></span><br><span class="line">seed = <span class="number">23455</span> <span class="comment"># 随机种子</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于seed产生随机数</span></span><br><span class="line"></span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"></span><br><span class="line"><span class="comment">#随机数字返回32行2列的矩阵 表示体积和重量 作为输入数据集</span></span><br><span class="line"></span><br><span class="line">X = rng.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">Y_ = [[x1+x2+(rng.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的输入 参数 和输出 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向前传播过程</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">y_ = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y = tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数以及反向传播方法</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#train_step = tf.train.AdadeltaOptimizer(0.001).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话 训练STEPS轮</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    init_op = tf.global_variables_initializer() <span class="comment">#初始化所有变量</span></span><br><span class="line"></span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    STEPS = <span class="number">50000</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line"></span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line"></span><br><span class="line">        end = start +BATCH_SIZE</span><br><span class="line"></span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x: X[start:end],y_:Y_[start:end]&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> ==<span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">            total_loss = sess.run(loss,feed_dict=&#123;x:X,y_:Y_&#125;)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">"After %d traning steps .loss on all data is %g"</span> %(i,total_loss))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出训练后的参数</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"W1:\n"</span>,sess.run(w1))</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">After 0 traning steps .loss on all data is 2.48876</span><br><span class="line"></span><br><span class="line">After 500 traning steps .loss on all data is 0.85528</span><br><span class="line"></span><br><span class="line">After 1000 traning steps .loss on all data is 0.302797</span><br><span class="line"></span><br><span class="line">After 1500 traning steps .loss on all data is 0.114376</span><br><span class="line"></span><br><span class="line">After 2000 traning steps .loss on all data is 0.0488448</span><br><span class="line"></span><br><span class="line">After 2500 traning steps .loss on all data is 0.0250235</span><br><span class="line"></span><br><span class="line">After 3000 traning steps .loss on all data is 0.0155473</span><br><span class="line"></span><br><span class="line">After 3500 traning steps .loss on all data is 0.0111573</span><br><span class="line"></span><br><span class="line">After 4000 traning steps .loss on all data is 0.00869355</span><br><span class="line"></span><br><span class="line">After 4500 traning steps .loss on all data is 0.00705474</span><br><span class="line"></span><br><span class="line">After 5000 traning steps .loss on all data is 0.00583891</span><br><span class="line"></span><br><span class="line">After 5500 traning steps .loss on all data is 0.00488469</span><br><span class="line"></span><br><span class="line">After 6000 traning steps .loss on all data is 0.00411648</span><br><span class="line"></span><br><span class="line">After 6500 traning steps .loss on all data is 0.00349116</span><br><span class="line"></span><br><span class="line">After 7000 traning steps .loss on all data is 0.00297992</span><br><span class="line"></span><br><span class="line">After 7500 traning steps .loss on all data is 0.00256107</span><br><span class="line"></span><br><span class="line">After 8000 traning steps .loss on all data is 0.00221769</span><br><span class="line"></span><br><span class="line">After 8500 traning steps .loss on all data is 0.00193611</span><br><span class="line"></span><br><span class="line">After 9000 traning steps .loss on all data is 0.00170515</span><br><span class="line"></span><br><span class="line">After 9500 traning steps .loss on all data is 0.00151571</span><br><span class="line"></span><br><span class="line">After 10000 traning steps .loss on all data is 0.00136033</span><br><span class="line"></span><br><span class="line">After 10500 traning steps .loss on all data is 0.00123288</span><br><span class="line"></span><br><span class="line">After 11000 traning steps .loss on all data is 0.00112833</span><br><span class="line"></span><br><span class="line">After 11500 traning steps .loss on all data is 0.00104257</span><br><span class="line"></span><br><span class="line">After 12000 traning steps .loss on all data is 0.000972239</span><br><span class="line"></span><br><span class="line">After 12500 traning steps .loss on all data is 0.000914546</span><br><span class="line"></span><br><span class="line">After 13000 traning steps .loss on all data is 0.000867223</span><br><span class="line"></span><br><span class="line">After 13500 traning steps .loss on all data is 0.000828402</span><br><span class="line"></span><br><span class="line">After 14000 traning steps .loss on all data is 0.000796561</span><br><span class="line"></span><br><span class="line">After 14500 traning steps .loss on all data is 0.000770443</span><br><span class="line"></span><br><span class="line">After 15000 traning steps .loss on all data is 0.000749021</span><br><span class="line"></span><br><span class="line">After 15500 traning steps .loss on all data is 0.000731453</span><br><span class="line"></span><br><span class="line">After 16000 traning steps .loss on all data is 0.000717037</span><br><span class="line"></span><br><span class="line">After 16500 traning steps .loss on all data is 0.000705214</span><br><span class="line"></span><br><span class="line">After 17000 traning steps .loss on all data is 0.000695519</span><br><span class="line"></span><br><span class="line">After 17500 traning steps .loss on all data is 0.000687564</span><br><span class="line"></span><br><span class="line">After 18000 traning steps .loss on all data is 0.000681038</span><br><span class="line"></span><br><span class="line">After 18500 traning steps .loss on all data is 0.000675687</span><br><span class="line"></span><br><span class="line">After 19000 traning steps .loss on all data is 0.000671295</span><br><span class="line"></span><br><span class="line">After 19500 traning steps .loss on all data is 0.000667701</span><br><span class="line"></span><br><span class="line">After 20000 traning steps .loss on all data is 0.000664741</span><br><span class="line"></span><br><span class="line">After 20500 traning steps .loss on all data is 0.000662322</span><br><span class="line"></span><br><span class="line">After 21000 traning steps .loss on all data is 0.000660335</span><br><span class="line"></span><br><span class="line">After 21500 traning steps .loss on all data is 0.000658707</span><br><span class="line"></span><br><span class="line">After 22000 traning steps .loss on all data is 0.000657368</span><br><span class="line"></span><br><span class="line">After 22500 traning steps .loss on all data is 0.000656275</span><br><span class="line"></span><br><span class="line">After 23000 traning steps .loss on all data is 0.000655372</span><br><span class="line"></span><br><span class="line">After 23500 traning steps .loss on all data is 0.000654635</span><br><span class="line"></span><br><span class="line">After 24000 traning steps .loss on all data is 0.000654034</span><br><span class="line"></span><br><span class="line">After 24500 traning steps .loss on all data is 0.000653543</span><br><span class="line"></span><br><span class="line">After 25000 traning steps .loss on all data is 0.000653136</span><br><span class="line"></span><br><span class="line">After 25500 traning steps .loss on all data is 0.000652797</span><br><span class="line"></span><br><span class="line">After 26000 traning steps .loss on all data is 0.000652516</span><br><span class="line"></span><br><span class="line">After 26500 traning steps .loss on all data is 0.000652293</span><br><span class="line"></span><br><span class="line">After 27000 traning steps .loss on all data is 0.000652109</span><br><span class="line"></span><br><span class="line">After 27500 traning steps .loss on all data is 0.000651959</span><br><span class="line"></span><br><span class="line">After 28000 traning steps .loss on all data is 0.000651836</span><br><span class="line"></span><br><span class="line">After 28500 traning steps .loss on all data is 0.000651735</span><br><span class="line"></span><br><span class="line">After 29000 traning steps .loss on all data is 0.00065165</span><br><span class="line"></span><br><span class="line">After 29500 traning steps .loss on all data is 0.000651582</span><br><span class="line"></span><br><span class="line">After 30000 traning steps .loss on all data is 0.000651528</span><br><span class="line"></span><br><span class="line">After 30500 traning steps .loss on all data is 0.000651481</span><br><span class="line"></span><br><span class="line">After 31000 traning steps .loss on all data is 0.000651442</span><br><span class="line"></span><br><span class="line">After 31500 traning steps .loss on all data is 0.000651411</span><br><span class="line"></span><br><span class="line">After 32000 traning steps .loss on all data is 0.000651385</span><br><span class="line"></span><br><span class="line">After 32500 traning steps .loss on all data is 0.000651364</span><br><span class="line"></span><br><span class="line">After 33000 traning steps .loss on all data is 0.000651348</span><br><span class="line"></span><br><span class="line">After 33500 traning steps .loss on all data is 0.000651334</span><br><span class="line"></span><br><span class="line">After 34000 traning steps .loss on all data is 0.000651323</span><br><span class="line"></span><br><span class="line">After 34500 traning steps .loss on all data is 0.000651315</span><br><span class="line"></span><br><span class="line">After 35000 traning steps .loss on all data is 0.000651308</span><br><span class="line"></span><br><span class="line">After 35500 traning steps .loss on all data is 0.000651303</span><br><span class="line"></span><br><span class="line">After 36000 traning steps .loss on all data is 0.000651296</span><br><span class="line"></span><br><span class="line">After 36500 traning steps .loss on all data is 0.000651291</span><br><span class="line"></span><br><span class="line">After 37000 traning steps .loss on all data is 0.000651286</span><br><span class="line"></span><br><span class="line">After 37500 traning steps .loss on all data is 0.000651284</span><br><span class="line"></span><br><span class="line">After 38000 traning steps .loss on all data is 0.000651282</span><br><span class="line"></span><br><span class="line">After 38500 traning steps .loss on all data is 0.00065128</span><br><span class="line"></span><br><span class="line">After 39000 traning steps .loss on all data is 0.000651279</span><br><span class="line"></span><br><span class="line">After 39500 traning steps .loss on all data is 0.000651278</span><br><span class="line"></span><br><span class="line">After 40000 traning steps .loss on all data is 0.000651277</span><br><span class="line"></span><br><span class="line">After 40500 traning steps .loss on all data is 0.000651276</span><br><span class="line"></span><br><span class="line">After 41000 traning steps .loss on all data is 0.000651275</span><br><span class="line"></span><br><span class="line">After 41500 traning steps .loss on all data is 0.000651274</span><br><span class="line"></span><br><span class="line">After 42000 traning steps .loss on all data is 0.000651273</span><br><span class="line"></span><br><span class="line">After 42500 traning steps .loss on all data is 0.000651272</span><br><span class="line"></span><br><span class="line">After 43000 traning steps .loss on all data is 0.000651272</span><br><span class="line"></span><br><span class="line">After 43500 traning steps .loss on all data is 0.000651271</span><br><span class="line"></span><br><span class="line">After 44000 traning steps .loss on all data is 0.000651271</span><br><span class="line"></span><br><span class="line">After 44500 traning steps .loss on all data is 0.000651271</span><br><span class="line"></span><br><span class="line">After 45000 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 45500 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 46000 traning steps .loss on all data is 0.000651271</span><br><span class="line"></span><br><span class="line">After 46500 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 47000 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 47500 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 48000 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 48500 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 49000 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 49500 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W1:</span><br><span class="line"></span><br><span class="line"> [[1.0042378 ]</span><br><span class="line"></span><br><span class="line"> [0.99486846]]</span><br><span class="line"></span><br><span class="line">Terminated: 15</span><br></pre></td></tr></table></figure><p>从以上结果我们可以看出，最终得到两个参数$w_1 = 1.004$ ,$w_2 = 0.995$ 。都非常接近1，也证明我们找到的参数正确(符合伪造数据集中的 $y_ = x_1 + x_2$ ).</p><blockquote><p>接例2：为了参数预测多了，损失成本COST1元，参数预测少了，损失利润PROFIT9元，为了利益最大化，我们希望尽量损失成本。</p></blockquote><p>分析：</p><p>可以自定义损失函数<script type="math/tex">loss(y\_,y) = \sum_ {n} f(y\_ - y)</script></p><script type="math/tex; mode=display">f(y\_ - y) =  \begin{cases} PROFIT * (y\_ - y) \qquad y \leq y\_  \\ COST * (y - y\_ ) \qquad \quad y\_ \leq y\end{cases}</script><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#两层的简单神经网络(全连接)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 酸奶预测</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span> <span class="comment">#一次喂入神经网络的大小</span></span><br><span class="line"></span><br><span class="line">seed = <span class="number">23455</span> <span class="comment"># 随机种子</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于seed产生随机数</span></span><br><span class="line"></span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"></span><br><span class="line"><span class="comment">#随机数字返回32行2列的矩阵 表示体积和重量 作为输入数据集</span></span><br><span class="line"></span><br><span class="line">X = rng.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">Y_ = [[x1+x2+(rng.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的输入 参数 和输出 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向前传播过程</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">y_ = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y = tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数以及反向传播方法</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#loss = tf.reduce_mean(tf.square(y - y_))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#自定义损失函数</span></span><br><span class="line"></span><br><span class="line">COST = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">PROFIT = <span class="number">9</span></span><br><span class="line"></span><br><span class="line">loss = tf.reduce_sum(tf.where(tf.greater(y,y_),COST*(y-y_),PROFIT*(y_-y)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#train_step = tf.train.AdadeltaOptimizer(0.001).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话 训练STEPS轮</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    init_op = tf.global_variables_initializer() <span class="comment">#初始化所有变量</span></span><br><span class="line"></span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    STEPS = <span class="number">50000</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line"></span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line"></span><br><span class="line">        end = start +BATCH_SIZE</span><br><span class="line"></span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x: X[start:end],y_:Y_[start:end]&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> ==<span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">            total_loss = sess.run(loss,feed_dict=&#123;x:X,y_:Y_&#125;)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">"After %d traning steps .loss on all data is %g"</span> %(i,total_loss))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出训练后的参数</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"W1:\n"</span>,sess.run(w1))</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">After 0 traning steps .loss on all data is 419.879</span><br><span class="line"></span><br><span class="line">After 500 traning steps .loss on all data is 1.91246</span><br><span class="line"></span><br><span class="line">After 1000 traning steps .loss on all data is 1.9079</span><br><span class="line"></span><br><span class="line">After 1500 traning steps .loss on all data is 1.91344</span><br><span class="line"></span><br><span class="line">After 2000 traning steps .loss on all data is 1.92183</span><br><span class="line"></span><br><span class="line">After 2500 traning steps .loss on all data is 1.92732</span><br><span class="line"></span><br><span class="line">After 3000 traning steps .loss on all data is 1.91853</span><br><span class="line"></span><br><span class="line">After 3500 traning steps .loss on all data is 1.92395</span><br><span class="line"></span><br><span class="line">After 4000 traning steps .loss on all data is 1.93173</span><br><span class="line"></span><br><span class="line">After 4500 traning steps .loss on all data is 1.92058</span><br><span class="line"></span><br><span class="line">After 5000 traning steps .loss on all data is 1.91603</span><br><span class="line"></span><br><span class="line">After 5500 traning steps .loss on all data is 1.93591</span><br><span class="line"></span><br><span class="line">After 6000 traning steps .loss on all data is 1.91265</span><br><span class="line"></span><br><span class="line">After 6500 traning steps .loss on all data is 1.91397</span><br><span class="line"></span><br><span class="line">After 7000 traning steps .loss on all data is 1.90928</span><br><span class="line"></span><br><span class="line">After 7500 traning steps .loss on all data is 1.93353</span><br><span class="line"></span><br><span class="line">After 8000 traning steps .loss on all data is 1.91814</span><br><span class="line"></span><br><span class="line">After 8500 traning steps .loss on all data is 1.92215</span><br><span class="line"></span><br><span class="line">After 9000 traning steps .loss on all data is 1.92415</span><br><span class="line"></span><br><span class="line">After 9500 traning steps .loss on all data is 1.92232</span><br><span class="line"></span><br><span class="line">After 10000 traning steps .loss on all data is 1.92602</span><br><span class="line"></span><br><span class="line">After 10500 traning steps .loss on all data is 1.91997</span><br><span class="line"></span><br><span class="line">After 11000 traning steps .loss on all data is 1.9279</span><br><span class="line"></span><br><span class="line">After 11500 traning steps .loss on all data is 1.91976</span><br><span class="line"></span><br><span class="line">After 12000 traning steps .loss on all data is 1.92452</span><br><span class="line"></span><br><span class="line">After 12500 traning steps .loss on all data is 1.91997</span><br><span class="line"></span><br><span class="line">After 13000 traning steps .loss on all data is 1.92115</span><br><span class="line"></span><br><span class="line">After 13500 traning steps .loss on all data is 1.9166</span><br><span class="line"></span><br><span class="line">After 14000 traning steps .loss on all data is 1.92266</span><br><span class="line"></span><br><span class="line">After 14500 traning steps .loss on all data is 1.92372</span><br><span class="line"></span><br><span class="line">After 15000 traning steps .loss on all data is 1.92138</span><br><span class="line"></span><br><span class="line">After 15500 traning steps .loss on all data is 1.92035</span><br><span class="line"></span><br><span class="line">After 16000 traning steps .loss on all data is 1.93678</span><br><span class="line"></span><br><span class="line">After 16500 traning steps .loss on all data is 1.91698</span><br><span class="line"></span><br><span class="line">After 17000 traning steps .loss on all data is 1.91242</span><br><span class="line"></span><br><span class="line">After 17500 traning steps .loss on all data is 1.91433</span><br><span class="line"></span><br><span class="line">After 18000 traning steps .loss on all data is 1.9214</span><br><span class="line"></span><br><span class="line">After 18500 traning steps .loss on all data is 1.91902</span><br><span class="line"></span><br><span class="line">After 19000 traning steps .loss on all data is 1.9181</span><br><span class="line"></span><br><span class="line">After 19500 traning steps .loss on all data is 1.92847</span><br><span class="line"></span><br><span class="line">After 20000 traning steps .loss on all data is 1.92319</span><br><span class="line"></span><br><span class="line">After 20500 traning steps .loss on all data is 1.9251</span><br><span class="line"></span><br><span class="line">After 21000 traning steps .loss on all data is 1.92055</span><br><span class="line"></span><br><span class="line">After 21500 traning steps .loss on all data is 1.92064</span><br><span class="line"></span><br><span class="line">After 22000 traning steps .loss on all data is 1.92688</span><br><span class="line"></span><br><span class="line">After 22500 traning steps .loss on all data is 1.91041</span><br><span class="line"></span><br><span class="line">After 23000 traning steps .loss on all data is 1.9244</span><br><span class="line"></span><br><span class="line">After 23500 traning steps .loss on all data is 1.91478</span><br><span class="line"></span><br><span class="line">After 24000 traning steps .loss on all data is 1.93026</span><br><span class="line"></span><br><span class="line">After 24500 traning steps .loss on all data is 1.92528</span><br><span class="line"></span><br><span class="line">After 25000 traning steps .loss on all data is 1.91988</span><br><span class="line"></span><br><span class="line">After 25500 traning steps .loss on all data is 1.92191</span><br><span class="line"></span><br><span class="line">After 26000 traning steps .loss on all data is 1.91879</span><br><span class="line"></span><br><span class="line">After 26500 traning steps .loss on all data is 1.92378</span><br><span class="line"></span><br><span class="line">After 27000 traning steps .loss on all data is 1.93147</span><br><span class="line"></span><br><span class="line">After 27500 traning steps .loss on all data is 1.92041</span><br><span class="line"></span><br><span class="line">After 28000 traning steps .loss on all data is 1.91586</span><br><span class="line"></span><br><span class="line">After 28500 traning steps .loss on all data is 1.93564</span><br><span class="line"></span><br><span class="line">After 29000 traning steps .loss on all data is 1.91248</span><br><span class="line"></span><br><span class="line">After 29500 traning steps .loss on all data is 1.9137</span><br><span class="line"></span><br><span class="line">After 30000 traning steps .loss on all data is 1.90911</span><br><span class="line"></span><br><span class="line">After 30500 traning steps .loss on all data is 1.93321</span><br><span class="line"></span><br><span class="line">After 31000 traning steps .loss on all data is 1.91788</span><br><span class="line"></span><br><span class="line">After 31500 traning steps .loss on all data is 1.9224</span><br><span class="line"></span><br><span class="line">After 32000 traning steps .loss on all data is 1.92398</span><br><span class="line"></span><br><span class="line">After 32500 traning steps .loss on all data is 1.92205</span><br><span class="line"></span><br><span class="line">After 33000 traning steps .loss on all data is 1.92061</span><br><span class="line"></span><br><span class="line">After 33500 traning steps .loss on all data is 1.93744</span><br><span class="line"></span><br><span class="line">After 34000 traning steps .loss on all data is 1.91723</span><br><span class="line"></span><br><span class="line">After 34500 traning steps .loss on all data is 1.91268</span><br><span class="line"></span><br><span class="line">After 35000 traning steps .loss on all data is 1.94162</span><br><span class="line"></span><br><span class="line">After 35500 traning steps .loss on all data is 1.92091</span><br><span class="line"></span><br><span class="line">After 36000 traning steps .loss on all data is 1.91968</span><br><span class="line"></span><br><span class="line">After 36500 traning steps .loss on all data is 1.90689</span><br><span class="line"></span><br><span class="line">After 37000 traning steps .loss on all data is 1.92873</span><br><span class="line"></span><br><span class="line">After 37500 traning steps .loss on all data is 1.92386</span><br><span class="line"></span><br><span class="line">After 38000 traning steps .loss on all data is 1.92536</span><br><span class="line"></span><br><span class="line">After 38500 traning steps .loss on all data is 1.9208</span><br><span class="line"></span><br><span class="line">After 39000 traning steps .loss on all data is 1.92803</span><br><span class="line"></span><br><span class="line">After 39500 traning steps .loss on all data is 1.91743</span><br><span class="line"></span><br><span class="line">After 40000 traning steps .loss on all data is 1.94342</span><br><span class="line"></span><br><span class="line">After 40500 traning steps .loss on all data is 1.91406</span><br><span class="line"></span><br><span class="line">After 41000 traning steps .loss on all data is 1.9095</span><br><span class="line"></span><br><span class="line">After 41500 traning steps .loss on all data is 1.91283</span><br><span class="line"></span><br><span class="line">After 42000 traning steps .loss on all data is 1.92122</span><br><span class="line"></span><br><span class="line">After 42500 traning steps .loss on all data is 1.94632</span><br><span class="line"></span><br><span class="line">After 43000 traning steps .loss on all data is 1.91325</span><br><span class="line"></span><br><span class="line">After 43500 traning steps .loss on all data is 1.94504</span><br><span class="line"></span><br><span class="line">After 44000 traning steps .loss on all data is 1.91904</span><br><span class="line"></span><br><span class="line">After 44500 traning steps .loss on all data is 1.92311</span><br><span class="line"></span><br><span class="line">After 45000 traning steps .loss on all data is 1.91575</span><br><span class="line"></span><br><span class="line">After 45500 traning steps .loss on all data is 1.9293</span><br><span class="line"></span><br><span class="line">After 46000 traning steps .loss on all data is 1.92475</span><br><span class="line"></span><br><span class="line">After 46500 traning steps .loss on all data is 1.92085</span><br><span class="line"></span><br><span class="line">After 47000 traning steps .loss on all data is 1.92138</span><br><span class="line"></span><br><span class="line">After 47500 traning steps .loss on all data is 1.93145</span><br><span class="line"></span><br><span class="line">After 48000 traning steps .loss on all data is 1.918</span><br><span class="line"></span><br><span class="line">After 48500 traning steps .loss on all data is 1.91345</span><br><span class="line"></span><br><span class="line">After 49000 traning steps .loss on all data is 1.91463</span><br><span class="line"></span><br><span class="line">After 49500 traning steps .loss on all data is 1.91008</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W1:</span><br><span class="line"></span><br><span class="line"> [[1.0201502]</span><br><span class="line"></span><br><span class="line"> [1.042119 ]]</span><br></pre></td></tr></table></figure><p>从结果看出，正如我们所期待的，他确实有在在往大了预测。</p><h4 id="交叉熵ce-可以表征两个概率分母之间的距离"><a href="#交叉熵ce-可以表征两个概率分母之间的距离" class="headerlink" title="交叉熵ce 可以表征两个概率分母之间的距离"></a>交叉熵ce 可以表征两个概率分母之间的距离</h4><script type="math/tex; mode=display">H(y\_,y) = - \sum_ {} y\_ * \log{y}</script><p>交叉熵的应用，二分类问题:</p><blockquote><p>已知答案y_ = (1,0), 预测 $y_1$ = (0.6,0.4) 和 $y_2$ = (0.8,0.2) 那个更接近答案?</p></blockquote><p>$H_1((1,0),(0.6,0.4)) = -(1<em>\log{0.6} + 0 </em> \log{0.4}) \approx 0.222$</p><p>$H_2((1,0),(0.8,0.2)) = -(1<em>\log{0.8} + 0 </em> \log{0.2}) \approx 0.097$</p><p>从以上结果可以看出 $H_2$ 更符合标准答案</p><p>当m分类的n个输出($y_1,y_2,y_3…y_n$)通过<code>softmax()</code>函数以满足概率分部的要求</p><script type="math/tex; mode=display">\forall x,P(X=x) \in {[0,1] \bigcap \sum_ {x} P(X = x) = 1}</script><script type="math/tex; mode=display">sorfmax(y_i) = \frac {e^{y_i} } {\sum_{i=1}^{n} e^{y_i} }</script><h3 id="学习率α-learning-rate"><a href="#学习率α-learning-rate" class="headerlink" title="学习率α learning_rate"></a>学习率α learning_rate</h3><script type="math/tex; mode=display">W_{n+1} = W_n - α·loss·\nabla</script><script type="math/tex; mode=display">更新后的参数 = 当前参数 - 学习率 \times 损失函数的导数（梯度算子）</script><p>其中,$\nabla = \frac{\partial^2 loss}{\partial w}$</p><blockquote><p>例:设损失函数$loss = (w+1)^2$ 则 $\nabla = \frac{\partial^2 loss}{\partial w} = 2w+2$</p></blockquote><p>参数w初始化为5</p><div class="table-container"><table><thead><tr><th>次数</th><th>参数</th><th>结果</th></tr></thead><tbody><tr><td>1</td><td>5</td><td>2.6</td></tr><tr><td>2</td><td>2.6</td><td>1.16</td></tr><tr><td>3</td><td>1.16</td><td>0.296</td></tr><tr><td>…</td><td>…</td><td>…</td></tr></tbody></table></div><p><img src="http://ww1.sinaimg.cn/large/006Uvlfagy1g0lbgmx9alj30g80fmwf2.jpg" alt=""></p><p>学习率大了可能不收敛，学习率小了可能收敛太慢，故引入了指数衰减学习率的算法：</p><script type="math/tex; mode=display">learning\_rate = learning\_rate\_base * learning\_rate\_decay^{\frac {global_step} {learning\_rate\_step} }</script><script type="math/tex; mode=display">学习率 = 学习率基数 * 学习率衰减率（0，1）^{\frac {运行的轮数} {多少轮更新一次}}</script><p>其中, $ learning_rate_step = \frac {SIZE} {batch_size} $ , $多少轮更新一次 = \frac {数据集大小} {每次喂入神经网络的数据集大小}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设损失函数 loss = (w+1)^2 令w的初始值是10 反向传播求w最优</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用指数衰减学习率，在迭代初期获得较高的下降速度。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.1</span> <span class="comment">#最初学习率</span></span><br><span class="line"></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span> <span class="comment">#学习率衰减率</span></span><br><span class="line"></span><br><span class="line">LEARNING_RATE_STEP = <span class="number">1</span> <span class="comment"># 喂入多少轮后更新，为了方便设为1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行了多少轮bacth_size的计数器， 初始值是0 设为不可被训练</span></span><br><span class="line"></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>,trainable=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义指数下降学习率</span></span><br><span class="line"></span><br><span class="line">learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,LEARNING_RATE_STEP,LEARNING_RATE_DECAY,staircase = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义待优化参数，初始值为10</span></span><br><span class="line"></span><br><span class="line">w  = tf.Variable(tf.constant(<span class="number">10</span>,dtype=tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数loss</span></span><br><span class="line"></span><br><span class="line">loss = tf.square(w+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义反向传播的方法</span></span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成回话 训练40轮</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    init_op = tf.global_variables_initializer() <span class="comment">#初始化所有变量</span></span><br><span class="line"></span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line"></span><br><span class="line">        sess.run(train_step)</span><br><span class="line"></span><br><span class="line">        learning_rate_val = sess.run(learning_rate)</span><br><span class="line"></span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line"></span><br><span class="line">        global_step_val = sess.run(global_step)</span><br><span class="line"></span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"After %s steps:global_step is %f,w is %f,learning_rate is %f,loss is %f"</span> %(i,global_step_val,w_val,learning_rate_val,loss_val))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">After 0 steps:global_step is 1.000000,w is 7.800000,learning_rate is 0.099000,loss is 77.440002</span><br><span class="line"></span><br><span class="line">After 1 steps:global_step is 2.000000,w is 6.057600,learning_rate is 0.098010,loss is 49.809719</span><br><span class="line"></span><br><span class="line">After 2 steps:global_step is 3.000000,w is 4.674169,learning_rate is 0.097030,loss is 32.196194</span><br><span class="line"></span><br><span class="line">After 3 steps:global_step is 4.000000,w is 3.573041,learning_rate is 0.096060,loss is 20.912704</span><br><span class="line"></span><br><span class="line">After 4 steps:global_step is 5.000000,w is 2.694472,learning_rate is 0.095099,loss is 13.649122</span><br><span class="line"></span><br><span class="line">After 5 steps:global_step is 6.000000,w is 1.991791,learning_rate is 0.094148,loss is 8.950810</span><br><span class="line"></span><br><span class="line">After 6 steps:global_step is 7.000000,w is 1.428448,learning_rate is 0.093207,loss is 5.897361</span><br><span class="line"></span><br><span class="line">After 7 steps:global_step is 8.000000,w is 0.975754,learning_rate is 0.092274,loss is 3.903603</span><br><span class="line"></span><br><span class="line">After 8 steps:global_step is 9.000000,w is 0.611130,learning_rate is 0.091352,loss is 2.595741</span><br><span class="line"></span><br><span class="line">After 9 steps:global_step is 10.000000,w is 0.316771,learning_rate is 0.090438,loss is 1.733887</span><br><span class="line"></span><br><span class="line">After 10 steps:global_step is 11.000000,w is 0.078598,learning_rate is 0.089534,loss is 1.163374</span><br><span class="line"></span><br><span class="line">After 11 steps:global_step is 12.000000,w is -0.114544,learning_rate is 0.088638,loss is 0.784033</span><br><span class="line"></span><br><span class="line">After 12 steps:global_step is 13.000000,w is -0.271515,learning_rate is 0.087752,loss is 0.530691</span><br><span class="line"></span><br><span class="line">After 13 steps:global_step is 14.000000,w is -0.399367,learning_rate is 0.086875,loss is 0.360760</span><br><span class="line"></span><br><span class="line">After 14 steps:global_step is 15.000000,w is -0.503727,learning_rate is 0.086006,loss is 0.246287</span><br><span class="line"></span><br><span class="line">After 15 steps:global_step is 16.000000,w is -0.589091,learning_rate is 0.085146,loss is 0.168846</span><br><span class="line"></span><br><span class="line">After 16 steps:global_step is 17.000000,w is -0.659066,learning_rate is 0.084294,loss is 0.116236</span><br><span class="line"></span><br><span class="line">After 17 steps:global_step is 18.000000,w is -0.716543,learning_rate is 0.083451,loss is 0.080348</span><br><span class="line"></span><br><span class="line">After 18 steps:global_step is 19.000000,w is -0.763853,learning_rate is 0.082617,loss is 0.055765</span><br><span class="line"></span><br><span class="line">After 19 steps:global_step is 20.000000,w is -0.802872,learning_rate is 0.081791,loss is 0.038859</span><br><span class="line"></span><br><span class="line">After 20 steps:global_step is 21.000000,w is -0.835119,learning_rate is 0.080973,loss is 0.027186</span><br><span class="line"></span><br><span class="line">After 21 steps:global_step is 22.000000,w is -0.861821,learning_rate is 0.080163,loss is 0.019094</span><br><span class="line"></span><br><span class="line">After 22 steps:global_step is 23.000000,w is -0.883974,learning_rate is 0.079361,loss is 0.013462</span><br><span class="line"></span><br><span class="line">After 23 steps:global_step is 24.000000,w is -0.902390,learning_rate is 0.078568,loss is 0.009528</span><br><span class="line"></span><br><span class="line">After 24 steps:global_step is 25.000000,w is -0.917728,learning_rate is 0.077782,loss is 0.006769</span><br><span class="line"></span><br><span class="line">After 25 steps:global_step is 26.000000,w is -0.930527,learning_rate is 0.077004,loss is 0.004827</span><br><span class="line"></span><br><span class="line">After 26 steps:global_step is 27.000000,w is -0.941226,learning_rate is 0.076234,loss is 0.003454</span><br><span class="line"></span><br><span class="line">After 27 steps:global_step is 28.000000,w is -0.950187,learning_rate is 0.075472,loss is 0.002481</span><br><span class="line"></span><br><span class="line">After 28 steps:global_step is 29.000000,w is -0.957706,learning_rate is 0.074717,loss is 0.001789</span><br><span class="line"></span><br><span class="line">After 29 steps:global_step is 30.000000,w is -0.964026,learning_rate is 0.073970,loss is 0.001294</span><br><span class="line"></span><br><span class="line">After 30 steps:global_step is 31.000000,w is -0.969348,learning_rate is 0.073230,loss is 0.000940</span><br><span class="line"></span><br><span class="line">After 31 steps:global_step is 32.000000,w is -0.973838,learning_rate is 0.072498,loss is 0.000684</span><br><span class="line"></span><br><span class="line">After 32 steps:global_step is 33.000000,w is -0.977631,learning_rate is 0.071773,loss is 0.000500</span><br><span class="line"></span><br><span class="line">After 33 steps:global_step is 34.000000,w is -0.980842,learning_rate is 0.071055,loss is 0.000367</span><br><span class="line"></span><br><span class="line">After 34 steps:global_step is 35.000000,w is -0.983565,learning_rate is 0.070345,loss is 0.000270</span><br><span class="line"></span><br><span class="line">After 35 steps:global_step is 36.000000,w is -0.985877,learning_rate is 0.069641,loss is 0.000199</span><br><span class="line"></span><br><span class="line">After 36 steps:global_step is 37.000000,w is -0.987844,learning_rate is 0.068945,loss is 0.000148</span><br><span class="line"></span><br><span class="line">After 37 steps:global_step is 38.000000,w is -0.989520,learning_rate is 0.068255,loss is 0.000110</span><br><span class="line"></span><br><span class="line">After 38 steps:global_step is 39.000000,w is -0.990951,learning_rate is 0.067573,loss is 0.000082</span><br><span class="line"></span><br><span class="line">After 39 steps:global_step is 40.000000,w is -0.992174,learning_rate is 0.066897,loss is 0.000061</span><br></pre></td></tr></table></figure><p>从结果我们可以看到，随着学习率不断衰减 w的变化速度不一样 最终衰减到接近-1</p><h2 id="滑动平均（影子）"><a href="#滑动平均（影子）" class="headerlink" title="滑动平均（影子）"></a>滑动平均（影子）</h2><p>滑动平均记录了每个参数的一段时间内过往的值的平均 增加了模型的泛化性 像是给参数增加了影子 参数变化 影子缓慢跟随</p><script type="math/tex; mode=display">影子 = 衰减率*影子+(1-衰减率)*参数</script><script type="math/tex; mode=display">衰减率 = min(MOVING\_AVERAGE\_BECAY,\frac {1+轮数} {10+轮数})</script><blockquote><p>例</p></blockquote><p>moving_average_becay为0.99（一般比较大）参数w1为0，轮数global_step为0，w1的滑动平均值为0</p><p>参数w1更新为1 则:</p><p>$w1的滑动平均值 = min(0.99, \frac 1{10}) \times0 + (1-min(0.99,\frac 1 {10})*1) = 0.9$</p><blockquote><p>轮数global_step更新为100，w1更新为10 则</p></blockquote><p>$w1的滑动平均值 = min(0.99, \frac {101}{110}) \times0 + (1-min(0.99,\frac {101} {110})*10) = 1.644$</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>当我们发现模型在训练集上的正确率非常高，但在从未见过的数据上表现不好 。则成这种现象见<strong><strong>过拟合</strong></strong>。</p><p>使用正则化可以有效缓解过拟合，再存是函数中引入模型的复杂度指标，利用w加权值 弱化了训练数据的噪声(一般不正则化偏置项b)</p><script type="math/tex; mode=display">loss = loss(y \&\& y\_) + REGUALRIZER * loss(w)</script><p>两种正则化方法：</p><script type="math/tex; mode=display">loss = \begin{cases} loss_{l_1}(w) = \sum_{i} \vert w_i \vert \\ loss_{l_2}(w) = \sum_{i}  \vert w_i^2 \vert \end{cases}</script>]]></content>
      
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
