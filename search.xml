<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>数据结构-C语言实现</title>
      <link href="/2019/03/19/DataStruct/"/>
      <url>/2019/03/19/DataStruct/</url>
      
        <content type="html"><![CDATA[<h2 id="数据结构-C语言实现"><a href="#数据结构-C语言实现" class="headerlink" title="数据结构-C语言实现"></a>数据结构-C语言实现</h2><p><strong><strong>顺序表，链表，顺序栈，循环队列，其他的（串，树，图，矩阵存储，排序查找等）会陆续实现</strong></strong><br><strong><strong> 还是先备考吧 </strong></strong><br><a id="more"></a></p><h3 id="工具·XCode·"><a href="#工具·XCode·" class="headerlink" title="工具·XCode·"></a>工具<code>·XCode·</code></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span>  LIST_INIT_SIZE 100 <span class="comment">//顺序表最大长度</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LISTINCREMENT 10    <span class="comment">//增加长度</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">int</span> ElemType; <span class="comment">//定义数据元素的类型</span></span><br><span class="line"><span class="keyword">int</span> InitState;       <span class="comment">//初始化标志</span></span><br><span class="line"><span class="keyword">int</span> Choose;         <span class="comment">//数据结构选择标志</span></span><br><span class="line"><span class="comment">// 已经实现的数据结构</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span>  &#123;</span></span><br><span class="line">    <span class="keyword">char</span> SqList[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> LinerList[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> Stack[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> Queue[<span class="number">50</span>];</span><br><span class="line">&#125;DataStruct = &#123;<span class="string">"顺序表"</span>,<span class="string">"单链表"</span>,<span class="string">"栈(顺序的)"</span>,<span class="string">"队列（循环的）"</span>&#125;;</span><br><span class="line"><span class="comment">//用来选择操作的函数，保证输入的操作合法</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">CheckScanf</span><span class="params">(<span class="keyword">int</span> RangeBegin,<span class="keyword">int</span> RangeEnd)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> choose;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;choose) != <span class="number">1</span> || choose &lt; RangeBegin || choose &gt; RangeEnd)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"输入非法!重新选择!\n&gt;&gt;"</span>);</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%*s"</span>);</span><br><span class="line">        fflush(<span class="built_in">stdin</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> choose;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//顺序表结构</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//顺序表可进行的操作</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line">    <span class="keyword">char</span> PrintSqList[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> AddElemInSqList[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> SelecElemInSqList[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> ReviseElemInSqList[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> DelElemInSqList[<span class="number">50</span>];</span><br><span class="line">&#125; _SqListFunction = &#123;</span><br><span class="line">    <span class="string">"打印顺序表"</span>,</span><br><span class="line">    <span class="string">"插入元素"</span>,</span><br><span class="line">    <span class="string">"查找元素"</span>,</span><br><span class="line">    <span class="string">"修改元素"</span>,</span><br><span class="line">    <span class="string">"删除元素"</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//顺序表结构</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">SqList</span>&#123;</span></span><br><span class="line">    ElemType *element; <span class="comment">//存储空间基地址</span></span><br><span class="line">    <span class="keyword">int</span> length; <span class="comment">//当前长度</span></span><br><span class="line">    <span class="keyword">int</span> listsize; <span class="comment">// 当前分配的存储容量</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//打印顺序表</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">PrintSqList</span><span class="params">(struct _SqList *L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L-&gt;length == <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"当前顺序表为空！\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    ElemType *p = L-&gt;element;</span><br><span class="line">    <span class="keyword">while</span> (p!=(L-&gt;element + L-&gt;length)) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d"</span>,*p);</span><br><span class="line">        p++;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//插入</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">AddElemInSqList</span><span class="params">(struct _SqList *L,<span class="keyword">int</span> place,ElemType e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L-&gt;length &gt;= L-&gt;listsize)&#123;</span><br><span class="line">        ElemType *newbase = (ElemType *)<span class="built_in">realloc</span>(L-&gt;element, (L-&gt;listsize + LISTINCREMENT)*<span class="keyword">sizeof</span>(ElemType));</span><br><span class="line">        <span class="keyword">if</span> (!newbase)&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"存储空间分配失败"</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        L-&gt;element = newbase;<span class="comment">//新基址</span></span><br><span class="line">        L-&gt;listsize += LISTINCREMENT;<span class="comment">//增加存储容量</span></span><br><span class="line">    &#125;</span><br><span class="line">    ElemType *_p,*_q;</span><br><span class="line">    _p = L-&gt;element + place<span class="number">-1</span>;</span><br><span class="line">    _q = L-&gt;element + L-&gt;length;</span><br><span class="line">    <span class="keyword">while</span> (_q &gt;= _p) &#123;</span><br><span class="line">        *(_q+<span class="number">1</span>) = *(_q);</span><br><span class="line">        _q--;</span><br><span class="line">    &#125;</span><br><span class="line">    *(_p) = e;</span><br><span class="line">    L-&gt;length++;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"插入%d成功!\n"</span>,e);</span><br><span class="line">    PrintSqList(L);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//查找</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">SelecElemInSqList</span><span class="params">(struct _SqList *L,ElemType e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    ElemType *p = L-&gt;element;</span><br><span class="line">    <span class="keyword">while</span> (p != L-&gt;element + L-&gt;length &amp;&amp; *p != e) &#123;</span><br><span class="line">        p++;</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(*p==e)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"已找到%d,在表中第%d个,地址是%p\n"</span>,e,count+<span class="number">1</span>,p);</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"未找到%d元素\n！"</span>,e);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//修改元素</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">ReviseElemInSqList</span><span class="params">(struct _SqList *L,ElemType ToRevise,ElemType Taget)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ElemPlace = SelecElemInSqList(L,ToRevise);</span><br><span class="line">    <span class="keyword">if</span>(ElemPlace+<span class="number">1</span>)&#123;</span><br><span class="line">        ElemType *p = L-&gt;element+ElemPlace;</span><br><span class="line">        *p = Taget;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"修改成功！，修改后的数组为：\t"</span>);</span><br><span class="line">        PrintSqList(L);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//删除元素</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">DelElemInSqList</span><span class="params">(struct _SqList *L,ElemType e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ElemPlace = SelecElemInSqList(L,e);</span><br><span class="line">    <span class="keyword">if</span>(ElemPlace+<span class="number">1</span>)&#123;</span><br><span class="line">        ElemType *p = L-&gt;element+ElemPlace;</span><br><span class="line">        ElemType *q = p;</span><br><span class="line">        <span class="keyword">while</span> (q!=L-&gt;element+L-&gt;length) &#123;</span><br><span class="line">            p++;</span><br><span class="line">            *q = *p;</span><br><span class="line">            q++;</span><br><span class="line">        &#125;</span><br><span class="line">        L-&gt;length--;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"删除成功！删除后的表为：\t"</span>);</span><br><span class="line">        PrintSqList(L);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"表长度:%d\n"</span>,L-&gt;length);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//选择顺序表的操作</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">SqListMain</span><span class="params">(struct _SqList *L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"请选择顺序表操作(输入0退出)\n1.%s\t2.%s\t\n3.%s\t4.%s\n5.%s\n"</span>,</span><br><span class="line">               _SqListFunction.PrintSqList,</span><br><span class="line">               _SqListFunction.AddElemInSqList,</span><br><span class="line">               _SqListFunction.SelecElemInSqList,</span><br><span class="line">               _SqListFunction.ReviseElemInSqList,</span><br><span class="line">               _SqListFunction.DelElemInSqList</span><br><span class="line">               );</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"&gt;&gt;"</span>);</span><br><span class="line">        Choose = CheckScanf(<span class="number">0</span>,<span class="number">5</span>);</span><br><span class="line">        <span class="keyword">switch</span> (Choose) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">1</span>:&#123;</span><br><span class="line">                PrintSqList(L);</span><br><span class="line">                fflush(<span class="built_in">stdin</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">2</span>:&#123;</span><br><span class="line">                <span class="keyword">int</span> p;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"请输入要插入的位置(1~%d):\n"</span>,L-&gt;length+<span class="number">1</span>);</span><br><span class="line">                p = CheckScanf(<span class="number">1</span>,L-&gt;length+<span class="number">1</span>);</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"请输入要插入的元素(-999~999)：\n"</span>);</span><br><span class="line">                <span class="keyword">int</span> e;</span><br><span class="line">                e = CheckScanf(<span class="number">-999</span>,<span class="number">999</span>);</span><br><span class="line">                AddElemInSqList(L,p,e);</span><br><span class="line">                fflush(<span class="built_in">stdin</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">3</span>:&#123;</span><br><span class="line">                ElemType e;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"请输入要查找的元素(-999~999)：\n"</span>);</span><br><span class="line">                e = CheckScanf(<span class="number">-999</span>,<span class="number">999</span>);</span><br><span class="line">                SelecElemInSqList(L,e);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">4</span>:&#123;</span><br><span class="line">                ElemType ToReserve;</span><br><span class="line">                ElemType Taget;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"请输入要修改的目标元素:\t"</span>);</span><br><span class="line">                ToReserve =  CheckScanf(<span class="number">-999</span>, <span class="number">999</span>);</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"将目标元素修改为？\t"</span>);</span><br><span class="line">                Taget = CheckScanf(<span class="number">-999</span>, <span class="number">999</span>);</span><br><span class="line">                ReviseElemInSqList(L, ToReserve, Taget);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">5</span>:&#123;</span><br><span class="line">                ElemType Deler;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"请输入要删除的元素\n"</span>);</span><br><span class="line">                Deler = CheckScanf(<span class="number">-999</span>,<span class="number">999</span>);</span><br><span class="line">                DelElemInSqList(L, Deler);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">                </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//初始化顺序表</span></span><br><span class="line"><span class="comment">//初始化线性列表并提供后续操作</span></span><br><span class="line"><span class="comment">//if 初始化成功 -&gt; 选择后续操作 -&gt; return 1 else return -1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">InitSqList</span><span class="params">(struct _SqList *L)</span></span>&#123;</span><br><span class="line">    L-&gt;element = (ElemType *)<span class="built_in">malloc</span>(LIST_INIT_SIZE * <span class="keyword">sizeof</span>(ElemType));</span><br><span class="line">    <span class="keyword">if</span>(!L-&gt;element)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"初始化顺序表失败！分配地址出错！\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    L-&gt;length = <span class="number">0</span>;</span><br><span class="line">    L-&gt;listsize = LIST_INIT_SIZE;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"初始化顺序表成功！顺序表的地址是:%p\n\n"</span>,L-&gt;element);</span><br><span class="line">    SqListMain(L);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//单链表节点结构</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">LinerListNode</span>&#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> _<span class="title">LinerListNode</span> *<span class="title">next</span>;</span> <span class="comment">//下一个</span></span><br><span class="line">    <span class="keyword">int</span> data;</span><br><span class="line">&#125;LinerListNode;</span><br><span class="line"></span><br><span class="line"><span class="comment">//单链表可进行的操作</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line">    <span class="keyword">char</span> PrintLinerList[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> AddElemInLinerList_H[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> AddElemInLinerList_T[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> SelecElemInLinerList[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> ReviseElemInLinerList[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> DelElemInLinerList[<span class="number">50</span>];</span><br><span class="line">&#125; _LinerListFunction = &#123;</span><br><span class="line">    <span class="string">"打印顺序表"</span>,</span><br><span class="line">    <span class="string">"头插法插入元素"</span>,</span><br><span class="line">    <span class="string">"尾插法插入元素"</span>,</span><br><span class="line">    <span class="string">"查找元素"</span>,</span><br><span class="line">    <span class="string">"修改元素"</span>,</span><br><span class="line">    <span class="string">"删除元素"</span>&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//打印单链表</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">PrintLinerList</span><span class="params">(struct _LinerListNode *L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(!L-&gt;next)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"单链表只有表头！为空！"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"(头结点)-&gt;"</span>);</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> _<span class="title">LinerListNode</span> *<span class="title">p</span> = <span class="title">L</span>-&gt;<span class="title">next</span>;</span></span><br><span class="line">    <span class="keyword">while</span> (p) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d-&gt;"</span>,p-&gt;data);</span><br><span class="line">        p = p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//头插法</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">AddElemInLinerList_H</span><span class="params">(struct _LinerListNode *L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span>  NodeNum;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"请输入预备插入节点个数(0~999):"</span>);</span><br><span class="line">    NodeNum = CheckScanf(<span class="number">0</span>, <span class="number">999</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;NodeNum;i++)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"输入第%d个节点的值:"</span>,i+<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">int</span> NodeValue = CheckScanf(<span class="number">-999</span>, <span class="number">999</span>);</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> _<span class="title">LinerListNode</span> *<span class="title">p</span> = (<span class="title">struct</span> _<span class="title">LinerListNode</span> *)<span class="title">malloc</span>(<span class="title">sizeof</span>(<span class="title">struct</span> _<span class="title">LinerListNode</span>));</span></span><br><span class="line">        p-&gt;data = NodeValue;</span><br><span class="line">        p-&gt;next = L-&gt;next;</span><br><span class="line">        L-&gt;next = p;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"第%d个节点头插成功！"</span>,i+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"插入完毕！当前链表为:\n"</span>);</span><br><span class="line">    PrintLinerList(L);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//尾插法</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">AddElemInLinerList_T</span><span class="params">(struct _LinerListNode *L)</span></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> _<span class="title">LinerListNode</span> *<span class="title">q</span> = <span class="title">L</span>;</span></span><br><span class="line">    <span class="keyword">while</span> (q-&gt;next) &#123;</span><br><span class="line">        q = q-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span>  NodeNum;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"请输入预备插入节点个数(0~999):"</span>);</span><br><span class="line">    NodeNum = CheckScanf(<span class="number">0</span>, <span class="number">999</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;NodeNum;i++)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"输入第%d个节点的值:"</span>,i+<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">int</span> NodeValue = CheckScanf(<span class="number">-999</span>, <span class="number">999</span>);</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> _<span class="title">LinerListNode</span> *<span class="title">p</span> = (<span class="title">struct</span> _<span class="title">LinerListNode</span> *)<span class="title">malloc</span>(<span class="title">sizeof</span>(<span class="title">struct</span> _<span class="title">LinerListNode</span>));</span></span><br><span class="line">        p-&gt;data = NodeValue;</span><br><span class="line">        q-&gt;next = p;</span><br><span class="line">        p-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">        q = p;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"第%d个节点头插成功！"</span>,i+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"插入完毕！当前链表为:\n"</span>);</span><br><span class="line">    PrintLinerList(L);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//查找</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">LinerListNode</span>* <span class="title">SelecElemInLinerList</span>(<span class="title">struct</span> _<span class="title">LinerListNode</span> *<span class="title">L</span> ,<span class="title">ElemType</span> <span class="title">e</span>)&#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> _<span class="title">LinerListNode</span> *<span class="title">p</span> = <span class="title">L</span>-&gt;<span class="title">next</span>;</span></span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (p &amp;&amp; p-&gt;data != e) &#123;</span><br><span class="line">        p = p-&gt;next;</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(p)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"找到元素%d,在第%d位置\n"</span>,e,count+<span class="number">1</span>);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"未找到元素%d\n"</span>,e);</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//修改</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">ReviseElemInLinerList</span><span class="params">(struct _LinerListNode *L,ElemType ToReserve,ElemType Tagert)</span></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> _<span class="title">LinerListNode</span> *<span class="title">p</span> = <span class="title">SelecElemInLinerList</span>(<span class="title">L</span>,<span class="title">ToReserve</span>);</span></span><br><span class="line">    <span class="keyword">if</span>(!p)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    p-&gt;data = Tagert;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"修改完成！当前的链表为:\t"</span>);</span><br><span class="line">    PrintLinerList(L);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//删除</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">DelElemInLinerList</span><span class="params">(struct _LinerListNode *L,ElemType e)</span></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> _<span class="title">LinerListNode</span> *<span class="title">p</span> = <span class="title">L</span>-&gt;<span class="title">next</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> _<span class="title">LinerListNode</span> *<span class="title">q</span> = <span class="title">p</span>;</span></span><br><span class="line">    <span class="keyword">while</span> (p &amp;&amp; p-&gt;data!= e) &#123;</span><br><span class="line">        q = p;</span><br><span class="line">        p = p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(p)&#123;</span><br><span class="line">        q-&gt;next = p-&gt;next;</span><br><span class="line">        <span class="built_in">free</span>(p);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"修改完成！当前链表为："</span>);</span><br><span class="line">        PrintLinerList(L);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"未找到元素%d"</span>,e);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//单链表进行的操作</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">LinerListMain</span><span class="params">(struct _LinerListNode *L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"请选择单链表表操作(输入0退出)\n1.%s\t2.%s\t\n3.%s\t4.%s\n5.%s\t6.%s\n"</span>,</span><br><span class="line">               _LinerListFunction.PrintLinerList,</span><br><span class="line">               _LinerListFunction.AddElemInLinerList_H,</span><br><span class="line">               _LinerListFunction.AddElemInLinerList_T,</span><br><span class="line">               _LinerListFunction.SelecElemInLinerList,</span><br><span class="line">               _LinerListFunction.ReviseElemInLinerList,</span><br><span class="line">               _LinerListFunction.DelElemInLinerList</span><br><span class="line">               );</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"&gt;&gt;"</span>);</span><br><span class="line">        Choose = CheckScanf(<span class="number">0</span>,<span class="number">6</span>);</span><br><span class="line">        <span class="keyword">switch</span> (Choose) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">1</span>:&#123;</span><br><span class="line">                PrintLinerList(L);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">2</span>:&#123;</span><br><span class="line">                AddElemInLinerList_H(L);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">3</span>:&#123;</span><br><span class="line">                AddElemInLinerList_T(L);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">4</span>:&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"请输入要查找的元素(-999~999):\t"</span>);</span><br><span class="line">                <span class="keyword">int</span> Value = CheckScanf(<span class="number">-999</span>, <span class="number">999</span>);</span><br><span class="line">                SelecElemInLinerList(L,Value);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">5</span>:&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"请输入要修改的元素(-999~999):\t"</span>);</span><br><span class="line">                <span class="keyword">int</span> ToReserve = CheckScanf(<span class="number">-999</span>, <span class="number">999</span>);</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"要修改为？(-999~999):\t"</span>);</span><br><span class="line">                <span class="keyword">int</span> Taget = CheckScanf(<span class="number">-999</span>, <span class="number">999</span>);</span><br><span class="line">                ReviseElemInLinerList(L,ToReserve,Taget);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">6</span>:&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"请输入要删除的元素(-999~999):\t"</span>);</span><br><span class="line">                <span class="keyword">int</span> e = CheckScanf(<span class="number">-999</span>, <span class="number">999</span>);</span><br><span class="line">                DelElemInLinerList(L,e);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">                </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化单链表</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">InitLinerList</span><span class="params">(struct _LinerListNode *L)</span></span>&#123;</span><br><span class="line">    L = (struct _LinerListNode *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(struct _LinerListNode));</span><br><span class="line">    <span class="keyword">if</span>(!L)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"初始化单链表失败！，分配地址出错！\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    L-&gt;data = <span class="number">-1</span>;</span><br><span class="line">    L-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"初始化单链表成功！表头的地址是%p,表头缺省值为%d\n"</span>,&amp;L,L-&gt;data);</span><br><span class="line">    LinerListMain(L);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//顺序栈的结构</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Stack</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> Nodes[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">int</span> top;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//顺序栈可进行的操作</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line">    <span class="keyword">char</span> PrintStack[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> Push[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> Pop[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> Top[<span class="number">50</span>];</span><br><span class="line">&#125; Stack = &#123;</span><br><span class="line">    <span class="string">"打印栈"</span>,</span><br><span class="line">    <span class="string">"进栈"</span>,</span><br><span class="line">    <span class="string">"出栈"</span>,</span><br><span class="line">    <span class="string">"取栈顶元素"</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">PrintStack</span><span class="params">(struct Stack *S)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(S-&gt;top == <span class="number">-1</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"栈空！\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">int</span> _top = S-&gt;top;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"当前栈（自顶而下）：\n"</span>);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"----\n"</span>);</span><br><span class="line">        <span class="keyword">for</span>(;_top&gt;=<span class="number">0</span>;_top--)&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%2d\n"</span>,S-&gt;Nodes[_top]);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"----\n"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Top</span><span class="params">(struct Stack *S)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(S-&gt;top == <span class="number">-1</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"栈空！\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"栈顶元素为:%d\n"</span>,S-&gt;Nodes[S-&gt;top]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> S-&gt;Nodes[S-&gt;top];</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Pop</span><span class="params">(struct Stack *S)</span></span>&#123;</span><br><span class="line">    ElemType _e = Top(S);</span><br><span class="line">    <span class="keyword">if</span>(_e == <span class="number">-1</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    S-&gt;top--;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"出栈成功！出栈元素%d,当前栈顶指针位置%d\n"</span>,_e,S-&gt;top);</span><br><span class="line">    PrintStack(S);</span><br><span class="line">    <span class="keyword">return</span> _e;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Push</span><span class="params">(struct Stack *S,ElemType e)</span></span>&#123;</span><br><span class="line">    S-&gt;Nodes[++(S-&gt;top)] = e;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"入栈成功！当前栈顶指针位置：%d\n"</span>,S-&gt;top);</span><br><span class="line">    PrintStack(S);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//栈可进行的操作</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">StackMian</span><span class="params">(struct Stack *S)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"请选择栈的操作(输入0退出):\n1.%s\t2.%s\t\n3.%s\t4.%s\n"</span>,</span><br><span class="line">               Stack.PrintStack,</span><br><span class="line">               Stack.Pop,</span><br><span class="line">               Stack.Push,</span><br><span class="line">               Stack.Top</span><br><span class="line">               );</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"&gt;&gt;"</span>);</span><br><span class="line">        Choose = CheckScanf(<span class="number">0</span>,<span class="number">4</span>);</span><br><span class="line">        <span class="keyword">switch</span> (Choose) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">1</span>:&#123;</span><br><span class="line">                PrintStack(S);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">2</span>:&#123;</span><br><span class="line">                Pop(S);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">3</span>:&#123;</span><br><span class="line">                ElemType e;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"请输入入栈元素:\n"</span>);</span><br><span class="line">                e = CheckScanf(<span class="number">-999</span>, <span class="number">999</span>);</span><br><span class="line">                Push(S,e);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">4</span>:&#123;</span><br><span class="line">                Top(S);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">                </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化栈</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">InitStack</span><span class="params">(struct Stack *S)</span></span>&#123;</span><br><span class="line">    S = (struct Stack *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(struct Stack));</span><br><span class="line">    <span class="keyword">if</span>(!S)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"分配空间失败!\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    S-&gt;top = <span class="number">-1</span>;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"初始化栈成功，栈顶指针-1"</span>);</span><br><span class="line">    StackMian(S);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//队的结构</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Queue</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> Data[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">int</span> front;</span><br><span class="line">    <span class="keyword">int</span> tail;</span><br><span class="line">    <span class="keyword">int</span> max;</span><br><span class="line">&#125;Queue;</span><br><span class="line"><span class="comment">//队可进行的操作</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line">    <span class="keyword">char</span> PrintQueue[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> PushQ[<span class="number">50</span>];</span><br><span class="line">    <span class="keyword">char</span> PopQ[<span class="number">50</span>];</span><br><span class="line">&#125; _QueueFunction = &#123;</span><br><span class="line">    <span class="string">"打印队"</span>,</span><br><span class="line">    <span class="string">"入队"</span>,</span><br><span class="line">    <span class="string">"出队"</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">PrintQueue</span><span class="params">(struct Queue *Q)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(Q-&gt;front == Q-&gt;tail)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"队空！\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> _F = Q-&gt;front;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"队列信息为：\n"</span>);</span><br><span class="line">    <span class="keyword">while</span> (_F != Q-&gt;tail) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">" %2d|"</span>,Q-&gt;Data[_F++]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"\n\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">PushQ</span><span class="params">(struct Queue *Q,ElemType e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(((Q-&gt;tail+Q-&gt;max+<span class="number">1</span>)%(Q-&gt;max)) == Q-&gt;front)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"队满！\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d,%d,%d"</span>,Q-&gt;front,Q-&gt;tail,Q-&gt;max);</span><br><span class="line">    Q-&gt;Data[Q-&gt;tail] = e;</span><br><span class="line">    Q-&gt;tail = ((Q-&gt;tail+Q-&gt;max+<span class="number">1</span>)%(Q-&gt;max));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"入队成功！当前对列状态:\n"</span>);</span><br><span class="line">    PrintQueue(Q);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">PopQ</span><span class="params">(struct Queue *Q)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(Q-&gt;front == Q-&gt;tail)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"队空！\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d,%d,%d"</span>,Q-&gt;front,Q-&gt;tail,Q-&gt;max);</span><br><span class="line">    ElemType e = Q-&gt;Data[Q-&gt;front];</span><br><span class="line">    Q-&gt;front = ((Q-&gt;front+Q-&gt;max+<span class="number">1</span>)%(Q-&gt;max));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"出队元素为:%2d\n"</span>,e);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">QueueMain</span><span class="params">(struct Queue *Q)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"请选择(输入0退出）:\n1.\t%s\t2.%s\t\n3.%s\n"</span>,</span><br><span class="line">               _QueueFunction.PrintQueue,</span><br><span class="line">               _QueueFunction.PopQ,</span><br><span class="line">               _QueueFunction.PushQ</span><br><span class="line">               );</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"&gt;&gt;"</span>);</span><br><span class="line">        Choose = CheckScanf(<span class="number">0</span>,<span class="number">3</span>);</span><br><span class="line">        <span class="keyword">switch</span> (Choose) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">1</span>:&#123;</span><br><span class="line">                PrintQueue(Q);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">2</span>:&#123;</span><br><span class="line">                PopQ(Q);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">3</span>:&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"请输入入队元素:\n"</span>);</span><br><span class="line">                ElemType e = CheckScanf(<span class="number">-999</span>, <span class="number">999</span>);</span><br><span class="line">                PushQ(Q,e);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">InitQueue</span><span class="params">(struct Queue *Q)</span></span>&#123;</span><br><span class="line">    Q = (struct Queue *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(struct Queue));</span><br><span class="line">    <span class="keyword">if</span>(!Q)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"初始化队列失败！"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    Q-&gt;front = <span class="number">0</span>;</span><br><span class="line">    Q-&gt;tail = <span class="number">0</span>;</span><br><span class="line">    Q-&gt;max = <span class="number">5</span>;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"初始化成功！\n当前对头指针位置:%d\t\t队尾指针位置:%d\n"</span>,Q-&gt;front,Q-&gt;tail);</span><br><span class="line">    QueueMain(Q);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//主函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"请选择(输入0退出）:\n1.\t%s\t2.%s\t\n3.%s\t4.%s\n"</span>,</span><br><span class="line">               DataStruct.SqList,</span><br><span class="line">               DataStruct.LinerList,</span><br><span class="line">               DataStruct.Stack,</span><br><span class="line">               DataStruct.Queue</span><br><span class="line">               );</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"&gt;&gt;"</span>);</span><br><span class="line">        Choose = CheckScanf(<span class="number">0</span>,<span class="number">4</span>);</span><br><span class="line">        <span class="keyword">switch</span> (Choose) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">1</span>:&#123;</span><br><span class="line">                <span class="class"><span class="keyword">struct</span> _<span class="title">SqList</span> <span class="title">L</span>;</span></span><br><span class="line">                InitState = InitSqList(&amp;L);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">2</span>:&#123;</span><br><span class="line">                <span class="class"><span class="keyword">struct</span> _<span class="title">LinerListNode</span> <span class="title">L</span>;</span></span><br><span class="line">                InitState = InitLinerList(&amp;L);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">3</span>:&#123;</span><br><span class="line">                <span class="class"><span class="keyword">struct</span> <span class="title">Stack</span> <span class="title">S</span>;</span></span><br><span class="line">                InitState = InitStack(&amp;S);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">4</span>:&#123;</span><br><span class="line">                <span class="class"><span class="keyword">struct</span> <span class="title">Queue</span> <span class="title">Q</span>;</span></span><br><span class="line">                InitState = InitQueue(&amp;Q);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> DataStruct </tag>
            
            <tag> C </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow-mnistd_1</title>
      <link href="/2019/03/14/TensorFlow-2/"/>
      <url>/2019/03/14/TensorFlow-2/</url>
      
        <content type="html"><![CDATA[<p><strong><strong>对TensorFlow官方教程的第一个项目稍作调整 配合官方教程食用更佳</strong></strong></p><a id="more"></a><h2 id="获取mnist数据集"><a href="#获取mnist数据集" class="headerlink" title="获取mnist数据集"></a>获取mnist数据集</h2><ul><li>数据集<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">官网</a>手动下载</li><li>使用TensorFlow提供的<a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/examples/tutorials/mnist/input_data.py" target="_blank" rel="noopener">input_data.py</a></li></ul><h2 id="mnist-train-py"><a href="#mnist-train-py" class="headerlink" title="mnist_train.py"></a>mnist_train.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>,[<span class="keyword">None</span>,<span class="number">784</span>])</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">x提供的占位符 表示把一张图展开为784维的向陆昂 None表示第一维度的形状有784决定</span></span><br><span class="line"><span class="string">W的维度是[784，10]，</span></span><br><span class="line"><span class="string">因为我们想要用784维的图片向量乘以它以得到一个10维的证据值向量，</span></span><br><span class="line"><span class="string">每一位对应不同数字类。</span></span><br><span class="line"><span class="string">b的形状是[10]，所以我们可以直接把它加到输出上面。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现模型的训练过程</span></span><br><span class="line">y =tf.nn.softmax(tf.matmul(x,W) + b)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">为了计算交叉熵 定义一个新的占位符来输入正确的值</span></span><br><span class="line"><span class="string">y 表示预测的概率分布</span></span><br><span class="line"><span class="string">y_ 表示实际的概率分布</span></span><br><span class="line"><span class="string">通过交叉熵计算损失</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">y_ = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>,<span class="number">10</span>])</span><br><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用反向传播算法来最小化模型</span></span><br><span class="line"><span class="comment"># 使用梯度下降算法 0.01的学习率来优化参数</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化所有变量</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    steps = <span class="number">1000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,steps):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    随机抓取训练数据中的100个批处理数据点，</span></span><br><span class="line"><span class="string">    然后我们用这些数据点作为参数替换之前的占位符来运行train_step</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 评估模型</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    tf.argmax 是一个非常有用的函数，</span></span><br><span class="line"><span class="string">    它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。</span></span><br><span class="line"><span class="string">    由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，</span></span><br><span class="line"><span class="string">    比如tf.argmax(y,1)返回的是模型对于任一输入x预测到的标签值，</span></span><br><span class="line"><span class="string">    而 tf.argmax(y_,1) 代表正确的标签，</span></span><br><span class="line"><span class="string">    可以用 tf.equal 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    这行代码会给我们一组布尔值。</span></span><br><span class="line"><span class="string">    为了确定正确预测项的比例，</span></span><br><span class="line"><span class="string">    我们可以把布尔值转换成浮点数，</span></span><br><span class="line"><span class="string">    然后取平均值。</span></span><br><span class="line"><span class="string">    例如，[True, False, True, True] 会变成 [1,0,1,1] ，取平均值后得到 0.75.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试准确率</span></span><br><span class="line"></span><br><span class="line">    print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MachineLearning(Ng)-LinearRegression-Programming</title>
      <link href="/2019/03/10/NgMLHomeWork-LinearRegression/"/>
      <url>/2019/03/10/NgMLHomeWork-LinearRegression/</url>
      
        <content type="html"><![CDATA[<p><strong><strong>Continue the study and update after completing the postgraduate entrance exam,more,good luck to me.</strong></strong></p><a id="more"></a><h3 id="warmUpExercise"><a href="#warmUpExercise" class="headerlink" title="warmUpExercise"></a>warmUpExercise</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A = <span class="built_in">eye</span>(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h3 id="computeCost-or-Multi"><a href="#computeCost-or-Multi" class="headerlink" title="computeCost(or Multi)"></a>computeCost(or Multi)</h3><script type="math/tex; mode=display">J_{(\theta_1,\theta_2,...,\theta_n)} = \frac 1{2m} \sum_{i=1}^n (h_{(\theta)}(x) - y)^2</script><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp =  sum(((X * theta - y).^<span class="number">2</span>));</span><br><span class="line">J = <span class="number">1</span> / (<span class="number">2</span>*m) * temp;</span><br></pre></td></tr></table></figure><h3 id="gradientDescent-or-Multi"><a href="#gradientDescent-or-Multi" class="headerlink" title="gradientDescent(or Multi)"></a>gradientDescent(or Multi)</h3><p><code>Repeat</code>{</p><script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{\partial J_{(\theta_1,\theta_2,...,\theta_n)}}{\partial \theta_j}</script><p>}</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">size</span>(X,<span class="number">2</span>)</span><br><span class="line">theta(<span class="built_in">i</span>) = GlobalTheta(<span class="built_in">i</span>) - alpha/m * sum((X*GlobalTheta-y).*X(:,<span class="built_in">i</span>));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="featureNormalize"><a href="#featureNormalize" class="headerlink" title="featureNormalize"></a>featureNormalize</h3><script type="math/tex; mode=display">X = \frac {X - mean} {avg}</script><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">size</span>(X,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">%X_norm(:,i) = (X(:,i) - (mean(X(:,i)))) / (std(X(:,i)))</span></span><br><span class="line">mu(<span class="built_in">i</span>) = <span class="built_in">mean</span>(X(:,<span class="built_in">i</span>));</span><br><span class="line">sigma(<span class="built_in">i</span>) = std(X(:,<span class="built_in">i</span>));</span><br><span class="line">X_norm(:,<span class="built_in">i</span>) = (X(:,<span class="built_in">i</span>) - mu(<span class="built_in">i</span>)) / sigma(<span class="built_in">i</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="normalEqn"><a href="#normalEqn" class="headerlink" title="normalEqn"></a>normalEqn</h3><script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^TY</script><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta = pinv(X' * X) * X' * y</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> MATLAB/Octave </tag>
            
            <tag> LinearRegression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记</title>
      <link href="/2019/03/01/TensorFlow-1/"/>
      <url>/2019/03/01/TensorFlow-1/</url>
      
        <content type="html"><![CDATA[<p><strong><strong>该笔记需要有一定的前置知识:高等数学，线性代数，概率论与数理统计，配合吴恩达&gt;机器学习，TensorFlow学习手册食用更佳( ´∀｀)σ</strong></strong></p><a id="more"></a><h1 id="什么是TensorFlow？"><a href="#什么是TensorFlow？" class="headerlink" title="什么是TensorFlow？"></a>什么是TensorFlow？</h1><blockquote><p>TensorFlow™ 是一个开放源代码软件库，用于进行高性能数值计算。借助其灵活的架构，用户可以轻松地将计算工作部署到多种平台（CPU、GPU、TPU）和设备（桌面设备、服务器集群、移动设备、边缘设备等）。TensorFlow™ 最初是由 Google Brain 团队（隶属于 Google 的 AI 部门）中的研究人员和工程师开发的，可为机器学习和深度学习提供强力支持，并且其灵活的数值计算核心广泛应用于许多其他科学领域。</p></blockquote><p>摘自<a href="https://www.tensorflow.org/?hl=zh_cn" target="_blank" rel="noopener">TensorFlow中文网</a></p><h1 id="什么是神经网络（以下简称NN）"><a href="#什么是神经网络（以下简称NN）" class="headerlink" title="什么是神经网络（以下简称NN）?"></a>什么是神经网络（以下简称NN）?</h1><blockquote><p>人工神经网络（英语：Artificial Neural Network，ANN），简称神经网络（Neural Network，NN）或类神经网络，在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统，通俗的讲就是具备学习功能。现代神经网络是一种非线性统计性数据建模工具。<br>神经网络的构筑理念是受到生物（人或其他动物）神经网络功能的运作启发而产生的。人工神经网络通常是通过一个基于数学统计学类型的学习方法（Learning Method）得以优化，所以也是数学统计学方法的一种实际应用，通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间，另一方面在人工智能学的人工感知领域，我们通过数学统计学的应用可以来做人工感知方面的决定问题（也就是说通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力），这种方法比起正式的逻辑学推理演算更具有优势。<br>和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被传统基于规则的编程所解决的。</p></blockquote><p>摘自<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener">维基百科人工神经网络</a></p><h1 id="使用TensorFlow的NN"><a href="#使用TensorFlow的NN" class="headerlink" title="使用TensorFlow的NN"></a>使用TensorFlow的NN</h1><ul><li>张量表示数据</li><li>计算图搭建网络</li><li>会话执行计算图</li><li>优化线上权重(参数)得到模型</li></ul><h2 id="张量-tensor"><a href="#张量-tensor" class="headerlink" title="张量(tensor)"></a>张量(tensor)</h2><p>tensor也是多重数组(列表)</p><p><strong><strong>阶</strong></strong>叫做tensor的维数</p><div class="table-container"><table><thead><tr><th>.</th><th>.</th><th>.</th><th>.</th></tr></thead><tbody><tr><td>0-D</td><td>0 阶</td><td>标量 scalar</td><td>1,2,3</td></tr><tr><td>1-D</td><td>1 阶</td><td>向量 vector</td><td>[1,2,3]</td></tr><tr><td>2-D</td><td>2 阶</td><td>矩阵 martix</td><td>[[1,2,3],[2,3,4],[3,4,5]]</td></tr><tr><td>n-D</td><td>n阶</td><td>张量tensor</td><td>[[[…</td></tr></tbody></table></div><h2 id="TensorFlow的第一个例子"><a href="#TensorFlow的第一个例子" class="headerlink" title="TensorFlow的第一个例子"></a>TensorFlow的第一个例子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.constant([<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result = a+b</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result</span><br><span class="line">&lt;tf.Tensor <span class="string">'add:0'</span> shape=(<span class="number">2</span>,) dtype=float32&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">      sess.run(result)</span><br><span class="line">array([<span class="number">4.</span>, <span class="number">6.</span>], dtype=float32)</span><br></pre></td></tr></table></figure><p>代码 <code>&lt;tf.Tensor &#39;add:0&#39; shape=(2,) dtype=float32&gt;</code>的解释:</p><p><code>add</code> $\to$ 节点名<br><code>0</code> $\to$ 0个输出<br><code>shape=</code> $\to$ 维度<br><code>(2,)</code> $\to$ 一维，长度2<br><code>dtype = float32</code> $\to$ 数据类型</p><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>计算图是指搭建神经网络的过程，<strong><strong>只搭建，不计算</strong></strong>。</p><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p>神经元是神经网络的基本单位</p><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1g0kp5ua3qrj30je0d6jsc.jpg" alt=""></p><h3 id="神经元的计算过程"><a href="#神经元的计算过程" class="headerlink" title="神经元的计算过程"></a>神经元的计算过程</h3><p> 上图为例,这个神经元的计算过程为：</p><script type="math/tex; mode=display">y = x_1\times w_1 + x_2 \times w_2</script><p> 写成矩阵乘法的形式：</p><script type="math/tex; mode=display">    X = \left(\matrix{x_1 \cr x_2\cr}\right),    W = \left(\matrix{w_1&w_2}\right)</script><script type="math/tex; mode=display">    Y = X \times W = \left(\matrix{x_1\cr x_2\cr } \right) \times \left(\matrix{ w_1 & w_2 }\right) =     x_1\times w_1 + x_2\times  w_2</script><h3 id="会话Session"><a href="#会话Session" class="headerlink" title="会话Session"></a>会话Session</h3><p><code>TensorFlow</code>的需要用<code>Session</code>的<code>with</code>结构来输出，执行完计算图的过程之后只能得到一个<code>TensorFlow</code>的<code>Objective</code>，如果执行输出，就会抛出一个<code>TypeError</code>的<code>exception</code></p><p><code>must be real number, not Tensor</code></p><h4 id="会话结构"><a href="#会话结构" class="headerlink" title="会话结构"></a>会话结构</h4> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(result)</span><br></pre></td></tr></table></figure><h3 id="NN结构的权重-参数"><a href="#NN结构的权重-参数" class="headerlink" title="NN结构的权重(参数)"></a>NN结构的权重(参数)</h3><p>一般情况下W由变量表示，一开始随机生成，随后通过神经网络的反向传播和损失函数（loss function）或代价函数（cost function）不断调整W直到模型最优。W的生成方式可用正态分布生成</p><p><code>w = tf.Variable(tf.random_normal([2,3],stddev = 2,mean = 0,seed = 1))</code></p><h4 id="参数解释"><a href="#参数解释" class="headerlink" title="参数解释"></a>参数解释</h4><ul><li><code>tf.random_normal()</code>指正态分布的W，还可用去掉过大偏离点的正态分布(大于标准差)<code>tf.turncated_normal()</code>和平均分布<code>tf.rnadom_uniform()</code></li><li><code>[2,3]</code>表示2*3的矩阵</li><li><code>stddev = 2</code>表示标准差为2</li><li><code>mean</code>表示均值</li><li><code>seed</code>表示随机种子</li></ul><p>其中，标准差，均值和随机种子是缺省的。</p><h2 id="神经网络的实现过程"><a href="#神经网络的实现过程" class="headerlink" title="神经网络的实现过程"></a>神经网络的实现过程</h2><ul><li><p>准备数据集，提取特征值，作为输入喂给神经网络</p></li><li><p>搭建NN结果，从输入到输出，先搭建计算图，再执行会话(向前传播过程)</p></li><li><p>大量特征数据喂给NN，迭代优化NN参数，直到模型最优(反向传播过程)</p></li><li><p>使用训练好的模型预测和分类</p></li></ul><h2 id="向前传播-搭建模型和推理的过程"><a href="#向前传播-搭建模型和推理的过程" class="headerlink" title="向前传播(搭建模型和推理的过程)"></a>向前传播(搭建模型和推理的过程)</h2><blockquote><p>例1(全连接网络)</p></blockquote><p>生产一批零件的体积$x_1$和重量$x_2$为特征值输入NN，通过NN后输出一个数值，NN已给出，要求写出计算过程。</p><p><img src="http://ww1.sinaimg.cn/large/006Uvlfagy1g0ndw0tm4gj30ye0i8afi.jpg" alt=""></p><p>计算过程：</p><script type="math/tex; mode=display">X = \left (\matrix {x_1 & x_2} \right ),W^{(1)} = \left (\matrix {w^{(1)}_{1,1} & w^{(1)}_{1,2} & w^{(1)}_{1,3} \cr w^{(1)}_{2,1} & w^{(1)}_{2,2} & w^{(1)}_{2,3} } \right)</script><script type="math/tex; mode=display">a^{(1)} = \left (\matrix {a_{1,1} & a_{1,2} & a_{1,3} } \right) = X * W^{(1)}</script><script type="math/tex; mode=display">W^{(2)} = \left (\matrix {w^{(2)}_{1,1} \cr w^{(2)}_{2,1} \cr w^{(2)}_{3,1} }\right)</script><script type="math/tex; mode=display">Y = a^{(1)} * W^{(2)}</script><h2 id="反向传播-训练神经网络参数"><a href="#反向传播-训练神经网络参数" class="headerlink" title="反向传播(训练神经网络参数)"></a>反向传播(训练神经网络参数)</h2><p>反向传播使用损失函数使得NN模型在训练函数上的损失最小</p><p>损失函数(loss) 预测值(y)和标准值(y_)的差距</p><p>均方误差MSE： </p><script type="math/tex; mode=display">MSE(y\_,y) = \frac { \sum_{i=1}^{n} (y\_ - y)^2} {n}</script><p>在<code>TensorFlow</code>的表示:</p><p><code>loss = tf.reduce_mean(tf.square(y_ - y))</code></p><blockquote><p>例1续： 设${x_1 + x_2 \leq 1表示该零件合格，要求构建神经网络(代码)}$</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#两层的简单神经网络(全连接)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment">### 反向传播算法的理解</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span> <span class="comment">#一次喂入神经网络的大小</span></span><br><span class="line">seed = <span class="number">23455</span> <span class="comment"># 随机种子</span></span><br><span class="line"><span class="comment">### 产生伪数据集</span></span><br><span class="line"><span class="comment"># 基于seed产生随机数</span></span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"><span class="comment">#随机数字返回32行2列的矩阵 表示体积和重量 作为输入数据集</span></span><br><span class="line">X = rng.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 从X这个32行2列的矩阵中 取出一行 判断 如果和小于1  赋值给Y=1 表示合格  否则Y=0表示不合格</span></span><br><span class="line"><span class="comment"># 作为输入数据集的标签(正确答案)</span></span><br><span class="line">Y = [[int(x0+x1 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x0,x1) <span class="keyword">in</span> X]</span><br><span class="line">print(<span class="string">"X:\n"</span>,X)</span><br><span class="line">print(<span class="string">"Y:\n"</span>,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的输入 参数 和输出 </span></span><br><span class="line"><span class="comment"># 向前传播过程</span></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">1</span>,seed=<span class="number">2</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">a = tf.matmul(x,w1)</span><br><span class="line">y = tf.matmul(a,w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数以及反向传播方法</span></span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"><span class="comment">#train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss)</span></span><br><span class="line"><span class="comment">#train_step = tf.train.AdadeltaOptimizer(0.001).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话 训练STEPS轮</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="comment">#输出未经训练的参数的取值</span></span><br><span class="line">    print(<span class="string">"W1:\n"</span>,sess.run(w1))</span><br><span class="line">    print(<span class="string">"W2:\n"</span>,sess.run(w2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line">    STEPS = <span class="number">6000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">        end = start +BATCH_SIZE</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x: X[start:end],y_:Y[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> ==<span class="number">0</span>:</span><br><span class="line">            total_loss = sess.run(loss,feed_dict=&#123;x:X,y_:Y&#125;)</span><br><span class="line">            print(<span class="string">"After %d traning steps .loss on all data is %g"</span> %(i,total_loss))</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 输出训练后的参数</span></span><br><span class="line">    print(<span class="string">'\n'</span>)</span><br><span class="line">    print(<span class="string">"W1:\n"</span>,sess.run(w1))</span><br><span class="line">    print(<span class="string">"W2:\n"</span>,sess.run(w2))</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">X:</span><br><span class="line"></span><br><span class="line"> [[0.83494319 0.11482951]</span><br><span class="line"></span><br><span class="line"> [0.66899751 0.46594987]</span><br><span class="line"></span><br><span class="line"> [0.60181666 0.58838408]</span><br><span class="line"></span><br><span class="line"> [0.31836656 0.20502072]</span><br><span class="line"></span><br><span class="line"> [0.87043944 0.02679395]</span><br><span class="line"></span><br><span class="line"> [0.41539811 0.43938369]</span><br><span class="line"></span><br><span class="line"> [0.68635684 0.24833404]</span><br><span class="line"></span><br><span class="line"> [0.97315228 0.68541849]</span><br><span class="line"></span><br><span class="line"> [0.03081617 0.89479913]</span><br><span class="line"></span><br><span class="line"> [0.24665715 0.28584862]</span><br><span class="line"></span><br><span class="line"> [0.31375667 0.47718349]</span><br><span class="line"></span><br><span class="line"> [0.56689254 0.77079148]</span><br><span class="line"></span><br><span class="line"> [0.7321604  0.35828963]</span><br><span class="line"></span><br><span class="line"> [0.15724842 0.94294584]</span><br><span class="line"></span><br><span class="line"> [0.34933722 0.84634483]</span><br><span class="line"></span><br><span class="line"> [0.50304053 0.81299619]</span><br><span class="line"></span><br><span class="line"> [0.23869886 0.9895604 ]</span><br><span class="line"></span><br><span class="line"> [0.4636501  0.32531094]</span><br><span class="line"></span><br><span class="line"> [0.36510487 0.97365522]</span><br><span class="line"></span><br><span class="line"> [0.73350238 0.83833013]</span><br><span class="line"></span><br><span class="line"> [0.61810158 0.12580353]</span><br><span class="line"></span><br><span class="line"> [0.59274817 0.18779828]</span><br><span class="line"></span><br><span class="line"> [0.87150299 0.34679501]</span><br><span class="line"></span><br><span class="line"> [0.25883219 0.50002932]</span><br><span class="line"></span><br><span class="line"> [0.75690948 0.83429824]</span><br><span class="line"></span><br><span class="line"> [0.29316649 0.05646578]</span><br><span class="line"></span><br><span class="line"> [0.10409134 0.88235166]</span><br><span class="line"></span><br><span class="line"> [0.06727785 0.57784761]</span><br><span class="line"></span><br><span class="line"> [0.38492705 0.48384792]</span><br><span class="line"></span><br><span class="line"> [0.69234428 0.19687348]</span><br><span class="line"></span><br><span class="line"> [0.42783492 0.73416985]</span><br><span class="line"></span><br><span class="line"> [0.09696069 0.04883936]]</span><br><span class="line"></span><br><span class="line">Y:</span><br><span class="line"></span><br><span class="line"> [[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]</span><br><span class="line"></span><br><span class="line">2019-02-27 10:49:25.877711: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line"></span><br><span class="line">W1:</span><br><span class="line"></span><br><span class="line"> [[-0.85811085 -0.19662298  0.13895045]</span><br><span class="line"></span><br><span class="line"> [-1.2212768  -0.40341285 -1.1454041 ]]</span><br><span class="line"></span><br><span class="line">W2:</span><br><span class="line"></span><br><span class="line"> [[-0.8113182 ]</span><br><span class="line"></span><br><span class="line"> [ 1.4845988 ]</span><br><span class="line"></span><br><span class="line"> [ 0.06532937]]</span><br><span class="line"></span><br><span class="line">After 0 traning steps .loss on all data is 0.396911</span><br><span class="line"></span><br><span class="line">After 500 traning steps .loss on all data is 0.390187</span><br><span class="line"></span><br><span class="line">After 1000 traning steps .loss on all data is 0.386858</span><br><span class="line"></span><br><span class="line">After 1500 traning steps .loss on all data is 0.385192</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W1:</span><br><span class="line"></span><br><span class="line"> [[-0.9150884  -0.09321376  0.14530948]</span><br><span class="line"></span><br><span class="line"> [-1.1892611  -0.4614115  -1.1490022 ]]</span><br><span class="line"></span><br><span class="line">W2:</span><br><span class="line"></span><br><span class="line"> [[-0.8259251 ]</span><br><span class="line"></span><br><span class="line"> [ 1.4913318 ]</span><br><span class="line"></span><br><span class="line"> [ 0.11993836]]</span><br></pre></td></tr></table></figure></p><h2 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h2><h3 id="普通神经元模型"><a href="#普通神经元模型" class="headerlink" title="普通神经元模型"></a>普通神经元模型</h3><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1g0ktlipq8oj30va0la76z.jpg" alt=""></p><h3 id="McCulloch-Pitts神经元模型"><a href="#McCulloch-Pitts神经元模型" class="headerlink" title="McCulloch-Pitts神经元模型"></a>McCulloch-Pitts神经元模型</h3><p><img src="http://ww1.sinaimg.cn/large/006Uvlfagy1g0ndnhlqasj30wg0q6ae9.jpg" alt=""></p><p>加入激活函数解决了线性模型不能解决的异或模型，其中$f(\sum_ {i}{} x_i*w_i + b)$是激活函数(activaction function),b是偏置项(bios)</p><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><ul><li>relu</li></ul><script type="math/tex; mode=display">f(x) = max(x,0) = \begin{cases}0 \qquad\qquad x \leq 0 \\ x \qquad\qquad x \geq 0\end{cases}</script><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1g0ku2ezaryj30ri0jcwf6.jpg" alt=""></p><ul><li>sigmoid</li></ul><script type="math/tex; mode=display">f(x) = \frac {1} {1+e^{-x} }</script><p><img src="http://ww1.sinaimg.cn/large/006Uvlfagy1g0ndo44q09j310c0hk76k.jpg" alt=""></p><ul><li>tanh</li></ul><script type="math/tex; mode=display">f(x) = \frac {1-e^{-2x} } {1 + e^{-2x} }</script><p><img src="http://ww1.sinaimg.cn/large/006Uvlfagy1g0ndoq53y1j31060iywh2.jpg" alt=""></p><h2 id="神经网络的优化"><a href="#神经网络的优化" class="headerlink" title="神经网络的优化"></a>神经网络的优化</h2><ul><li><p>损失函数loss</p></li><li><p>学习率 learning_rate</p></li><li><p>平均滑动 ema</p></li><li><p>正则化 regularization</p></li></ul><h3 id="损失函数（loss-预测值y-与已知答案-y-的差距"><a href="#损失函数（loss-预测值y-与已知答案-y-的差距" class="headerlink" title="损失函数（loss) 预测值y_与已知答案(y_)的差距"></a>损失函数（loss) 预测值y_与已知答案(y_)的差距</h3><p>NN的优化目标： loss最小</p><script type="math/tex; mode=display">损失函数 \to \begin{cases}  mse \quad(Mean Squared Error) \quad 均方误差 \\ 自定义 \\ ce \quad (Cross Entropy) \quad  交叉熵\end{cases}</script><h4 id="均方误差mse"><a href="#均方误差mse" class="headerlink" title="均方误差mse:"></a>均方误差mse:</h4><script type="math/tex; mode=display">MSE(y_,y) = \frac {\sum_ {i=1}^{n} (y\_ -y)^2} {n}</script><blockquote><p>例2：构建一层神经网络,预测酸奶日销量y, $x_1$,$x_2$ 是影响因素。</p></blockquote><p>分析: 建模前，应该采集的数据集有：每日的$x_1$和$x_2$ 销量y_。（在此例子中，我们你早数据X,Y_ : $ Y_ = X_1 + X_2 $ ，再加入-0.5~+0.5的噪声使得伪数据集更真实）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#两层的简单神经网络(全连接)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 酸奶预测</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span> <span class="comment">#一次喂入神经网络的大小</span></span><br><span class="line"></span><br><span class="line">seed = <span class="number">23455</span> <span class="comment"># 随机种子</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于seed产生随机数</span></span><br><span class="line"></span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"></span><br><span class="line"><span class="comment">#随机数字返回32行2列的矩阵 表示体积和重量 作为输入数据集</span></span><br><span class="line"></span><br><span class="line">X = rng.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">Y_ = [[x1+x2+(rng.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的输入 参数 和输出 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向前传播过程</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">y_ = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y = tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数以及反向传播方法</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#train_step = tf.train.AdadeltaOptimizer(0.001).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话 训练STEPS轮</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    init_op = tf.global_variables_initializer() <span class="comment">#初始化所有变量</span></span><br><span class="line"></span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    STEPS = <span class="number">50000</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line"></span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line"></span><br><span class="line">        end = start +BATCH_SIZE</span><br><span class="line"></span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x: X[start:end],y_:Y_[start:end]&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> ==<span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">            total_loss = sess.run(loss,feed_dict=&#123;x:X,y_:Y_&#125;)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">"After %d traning steps .loss on all data is %g"</span> %(i,total_loss))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出训练后的参数</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"W1:\n"</span>,sess.run(w1))</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">After 0 traning steps .loss on all data is 2.48876</span><br><span class="line"></span><br><span class="line">After 500 traning steps .loss on all data is 0.85528</span><br><span class="line"></span><br><span class="line">After 1000 traning steps .loss on all data is 0.302797</span><br><span class="line"></span><br><span class="line">After 1500 traning steps .loss on all data is 0.114376</span><br><span class="line"></span><br><span class="line">After 2000 traning steps .loss on all data is 0.0488448</span><br><span class="line"></span><br><span class="line">After 2500 traning steps .loss on all data is 0.0250235</span><br><span class="line"></span><br><span class="line">After 3000 traning steps .loss on all data is 0.0155473</span><br><span class="line"></span><br><span class="line">After 3500 traning steps .loss on all data is 0.0111573</span><br><span class="line"></span><br><span class="line">After 4000 traning steps .loss on all data is 0.00869355</span><br><span class="line"></span><br><span class="line">After 4500 traning steps .loss on all data is 0.00705474</span><br><span class="line"></span><br><span class="line">After 5000 traning steps .loss on all data is 0.00583891</span><br><span class="line"></span><br><span class="line">After 5500 traning steps .loss on all data is 0.00488469</span><br><span class="line"></span><br><span class="line">After 6000 traning steps .loss on all data is 0.00411648</span><br><span class="line"></span><br><span class="line">After 6500 traning steps .loss on all data is 0.00349116</span><br><span class="line"></span><br><span class="line">After 7000 traning steps .loss on all data is 0.00297992</span><br><span class="line"></span><br><span class="line">After 7500 traning steps .loss on all data is 0.00256107</span><br><span class="line"></span><br><span class="line">After 8000 traning steps .loss on all data is 0.00221769</span><br><span class="line"></span><br><span class="line">After 8500 traning steps .loss on all data is 0.00193611</span><br><span class="line"></span><br><span class="line">After 9000 traning steps .loss on all data is 0.00170515</span><br><span class="line"></span><br><span class="line">After 9500 traning steps .loss on all data is 0.00151571</span><br><span class="line"></span><br><span class="line">After 10000 traning steps .loss on all data is 0.00136033</span><br><span class="line"></span><br><span class="line">After 10500 traning steps .loss on all data is 0.00123288</span><br><span class="line"></span><br><span class="line">After 11000 traning steps .loss on all data is 0.00112833</span><br><span class="line"></span><br><span class="line">After 11500 traning steps .loss on all data is 0.00104257</span><br><span class="line"></span><br><span class="line">After 12000 traning steps .loss on all data is 0.000972239</span><br><span class="line"></span><br><span class="line">After 12500 traning steps .loss on all data is 0.000914546</span><br><span class="line"></span><br><span class="line">After 13000 traning steps .loss on all data is 0.000867223</span><br><span class="line"></span><br><span class="line">After 13500 traning steps .loss on all data is 0.000828402</span><br><span class="line"></span><br><span class="line">After 14000 traning steps .loss on all data is 0.000796561</span><br><span class="line"></span><br><span class="line">After 14500 traning steps .loss on all data is 0.000770443</span><br><span class="line"></span><br><span class="line">After 15000 traning steps .loss on all data is 0.000749021</span><br><span class="line"></span><br><span class="line">After 15500 traning steps .loss on all data is 0.000731453</span><br><span class="line"></span><br><span class="line">After 16000 traning steps .loss on all data is 0.000717037</span><br><span class="line"></span><br><span class="line">After 16500 traning steps .loss on all data is 0.000705214</span><br><span class="line"></span><br><span class="line">After 17000 traning steps .loss on all data is 0.000695519</span><br><span class="line"></span><br><span class="line">After 17500 traning steps .loss on all data is 0.000687564</span><br><span class="line"></span><br><span class="line">After 18000 traning steps .loss on all data is 0.000681038</span><br><span class="line"></span><br><span class="line">After 18500 traning steps .loss on all data is 0.000675687</span><br><span class="line"></span><br><span class="line">After 19000 traning steps .loss on all data is 0.000671295</span><br><span class="line"></span><br><span class="line">After 19500 traning steps .loss on all data is 0.000667701</span><br><span class="line"></span><br><span class="line">After 20000 traning steps .loss on all data is 0.000664741</span><br><span class="line"></span><br><span class="line">After 20500 traning steps .loss on all data is 0.000662322</span><br><span class="line"></span><br><span class="line">After 21000 traning steps .loss on all data is 0.000660335</span><br><span class="line"></span><br><span class="line">After 21500 traning steps .loss on all data is 0.000658707</span><br><span class="line"></span><br><span class="line">After 22000 traning steps .loss on all data is 0.000657368</span><br><span class="line"></span><br><span class="line">After 22500 traning steps .loss on all data is 0.000656275</span><br><span class="line"></span><br><span class="line">After 23000 traning steps .loss on all data is 0.000655372</span><br><span class="line"></span><br><span class="line">After 23500 traning steps .loss on all data is 0.000654635</span><br><span class="line"></span><br><span class="line">After 24000 traning steps .loss on all data is 0.000654034</span><br><span class="line"></span><br><span class="line">After 24500 traning steps .loss on all data is 0.000653543</span><br><span class="line"></span><br><span class="line">After 25000 traning steps .loss on all data is 0.000653136</span><br><span class="line"></span><br><span class="line">After 25500 traning steps .loss on all data is 0.000652797</span><br><span class="line"></span><br><span class="line">After 26000 traning steps .loss on all data is 0.000652516</span><br><span class="line"></span><br><span class="line">After 26500 traning steps .loss on all data is 0.000652293</span><br><span class="line"></span><br><span class="line">After 27000 traning steps .loss on all data is 0.000652109</span><br><span class="line"></span><br><span class="line">After 27500 traning steps .loss on all data is 0.000651959</span><br><span class="line"></span><br><span class="line">After 28000 traning steps .loss on all data is 0.000651836</span><br><span class="line"></span><br><span class="line">After 28500 traning steps .loss on all data is 0.000651735</span><br><span class="line"></span><br><span class="line">After 29000 traning steps .loss on all data is 0.00065165</span><br><span class="line"></span><br><span class="line">After 29500 traning steps .loss on all data is 0.000651582</span><br><span class="line"></span><br><span class="line">After 30000 traning steps .loss on all data is 0.000651528</span><br><span class="line"></span><br><span class="line">After 30500 traning steps .loss on all data is 0.000651481</span><br><span class="line"></span><br><span class="line">After 31000 traning steps .loss on all data is 0.000651442</span><br><span class="line"></span><br><span class="line">After 31500 traning steps .loss on all data is 0.000651411</span><br><span class="line"></span><br><span class="line">After 32000 traning steps .loss on all data is 0.000651385</span><br><span class="line"></span><br><span class="line">After 32500 traning steps .loss on all data is 0.000651364</span><br><span class="line"></span><br><span class="line">After 33000 traning steps .loss on all data is 0.000651348</span><br><span class="line"></span><br><span class="line">After 33500 traning steps .loss on all data is 0.000651334</span><br><span class="line"></span><br><span class="line">After 34000 traning steps .loss on all data is 0.000651323</span><br><span class="line"></span><br><span class="line">After 34500 traning steps .loss on all data is 0.000651315</span><br><span class="line"></span><br><span class="line">After 35000 traning steps .loss on all data is 0.000651308</span><br><span class="line"></span><br><span class="line">After 35500 traning steps .loss on all data is 0.000651303</span><br><span class="line"></span><br><span class="line">After 36000 traning steps .loss on all data is 0.000651296</span><br><span class="line"></span><br><span class="line">After 36500 traning steps .loss on all data is 0.000651291</span><br><span class="line"></span><br><span class="line">After 37000 traning steps .loss on all data is 0.000651286</span><br><span class="line"></span><br><span class="line">After 37500 traning steps .loss on all data is 0.000651284</span><br><span class="line"></span><br><span class="line">After 38000 traning steps .loss on all data is 0.000651282</span><br><span class="line"></span><br><span class="line">After 38500 traning steps .loss on all data is 0.00065128</span><br><span class="line"></span><br><span class="line">After 39000 traning steps .loss on all data is 0.000651279</span><br><span class="line"></span><br><span class="line">After 39500 traning steps .loss on all data is 0.000651278</span><br><span class="line"></span><br><span class="line">After 40000 traning steps .loss on all data is 0.000651277</span><br><span class="line"></span><br><span class="line">After 40500 traning steps .loss on all data is 0.000651276</span><br><span class="line"></span><br><span class="line">After 41000 traning steps .loss on all data is 0.000651275</span><br><span class="line"></span><br><span class="line">After 41500 traning steps .loss on all data is 0.000651274</span><br><span class="line"></span><br><span class="line">After 42000 traning steps .loss on all data is 0.000651273</span><br><span class="line"></span><br><span class="line">After 42500 traning steps .loss on all data is 0.000651272</span><br><span class="line"></span><br><span class="line">After 43000 traning steps .loss on all data is 0.000651272</span><br><span class="line"></span><br><span class="line">After 43500 traning steps .loss on all data is 0.000651271</span><br><span class="line"></span><br><span class="line">After 44000 traning steps .loss on all data is 0.000651271</span><br><span class="line"></span><br><span class="line">After 44500 traning steps .loss on all data is 0.000651271</span><br><span class="line"></span><br><span class="line">After 45000 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 45500 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 46000 traning steps .loss on all data is 0.000651271</span><br><span class="line"></span><br><span class="line">After 46500 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 47000 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 47500 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 48000 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 48500 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 49000 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line">After 49500 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W1:</span><br><span class="line"></span><br><span class="line"> [[1.0042378 ]</span><br><span class="line"></span><br><span class="line"> [0.99486846]]</span><br><span class="line"></span><br><span class="line">Terminated: 15</span><br></pre></td></tr></table></figure><p>从以上结果我们可以看出，最终得到两个参数$w_1 = 1.004$ ,$w_2 = 0.995$ 。都非常接近1，也证明我们找到的参数正确(符合伪造数据集中的 $y_ = x_1 + x_2$ ).</p><blockquote><p>接例2：为了参数预测多了，损失成本COST1元，参数预测少了，损失利润PROFIT9元，为了利益最大化，我们希望尽量损失成本。</p></blockquote><p>分析：</p><p>可以自定义损失函数<script type="math/tex">loss(y\_,y) = \sum_ {n} f(y\_ - y)</script></p><script type="math/tex; mode=display">f(y\_ - y) =  \begin{cases} PROFIT * (y\_ - y) \qquad y \leq y\_  \\ COST * (y - y\_ ) \qquad \quad y\_ \leq y\end{cases}</script><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#两层的简单神经网络(全连接)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 酸奶预测</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span> <span class="comment">#一次喂入神经网络的大小</span></span><br><span class="line"></span><br><span class="line">seed = <span class="number">23455</span> <span class="comment"># 随机种子</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于seed产生随机数</span></span><br><span class="line"></span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"></span><br><span class="line"><span class="comment">#随机数字返回32行2列的矩阵 表示体积和重量 作为输入数据集</span></span><br><span class="line"></span><br><span class="line">X = rng.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">Y_ = [[x1+x2+(rng.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的输入 参数 和输出 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向前传播过程</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">y_ = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y = tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数以及反向传播方法</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#loss = tf.reduce_mean(tf.square(y - y_))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#自定义损失函数</span></span><br><span class="line"></span><br><span class="line">COST = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">PROFIT = <span class="number">9</span></span><br><span class="line"></span><br><span class="line">loss = tf.reduce_sum(tf.where(tf.greater(y,y_),COST*(y-y_),PROFIT*(y_-y)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#train_step = tf.train.AdadeltaOptimizer(0.001).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话 训练STEPS轮</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    init_op = tf.global_variables_initializer() <span class="comment">#初始化所有变量</span></span><br><span class="line"></span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    STEPS = <span class="number">50000</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line"></span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line"></span><br><span class="line">        end = start +BATCH_SIZE</span><br><span class="line"></span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x: X[start:end],y_:Y_[start:end]&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> ==<span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">            total_loss = sess.run(loss,feed_dict=&#123;x:X,y_:Y_&#125;)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">"After %d traning steps .loss on all data is %g"</span> %(i,total_loss))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出训练后的参数</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"W1:\n"</span>,sess.run(w1))</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">After 0 traning steps .loss on all data is 419.879</span><br><span class="line"></span><br><span class="line">After 500 traning steps .loss on all data is 1.91246</span><br><span class="line"></span><br><span class="line">After 1000 traning steps .loss on all data is 1.9079</span><br><span class="line"></span><br><span class="line">After 1500 traning steps .loss on all data is 1.91344</span><br><span class="line"></span><br><span class="line">After 2000 traning steps .loss on all data is 1.92183</span><br><span class="line"></span><br><span class="line">After 2500 traning steps .loss on all data is 1.92732</span><br><span class="line"></span><br><span class="line">After 3000 traning steps .loss on all data is 1.91853</span><br><span class="line"></span><br><span class="line">After 3500 traning steps .loss on all data is 1.92395</span><br><span class="line"></span><br><span class="line">After 4000 traning steps .loss on all data is 1.93173</span><br><span class="line"></span><br><span class="line">After 4500 traning steps .loss on all data is 1.92058</span><br><span class="line"></span><br><span class="line">After 5000 traning steps .loss on all data is 1.91603</span><br><span class="line"></span><br><span class="line">After 5500 traning steps .loss on all data is 1.93591</span><br><span class="line"></span><br><span class="line">After 6000 traning steps .loss on all data is 1.91265</span><br><span class="line"></span><br><span class="line">After 6500 traning steps .loss on all data is 1.91397</span><br><span class="line"></span><br><span class="line">After 7000 traning steps .loss on all data is 1.90928</span><br><span class="line"></span><br><span class="line">After 7500 traning steps .loss on all data is 1.93353</span><br><span class="line"></span><br><span class="line">After 8000 traning steps .loss on all data is 1.91814</span><br><span class="line"></span><br><span class="line">After 8500 traning steps .loss on all data is 1.92215</span><br><span class="line"></span><br><span class="line">After 9000 traning steps .loss on all data is 1.92415</span><br><span class="line"></span><br><span class="line">After 9500 traning steps .loss on all data is 1.92232</span><br><span class="line"></span><br><span class="line">After 10000 traning steps .loss on all data is 1.92602</span><br><span class="line"></span><br><span class="line">After 10500 traning steps .loss on all data is 1.91997</span><br><span class="line"></span><br><span class="line">After 11000 traning steps .loss on all data is 1.9279</span><br><span class="line"></span><br><span class="line">After 11500 traning steps .loss on all data is 1.91976</span><br><span class="line"></span><br><span class="line">After 12000 traning steps .loss on all data is 1.92452</span><br><span class="line"></span><br><span class="line">After 12500 traning steps .loss on all data is 1.91997</span><br><span class="line"></span><br><span class="line">After 13000 traning steps .loss on all data is 1.92115</span><br><span class="line"></span><br><span class="line">After 13500 traning steps .loss on all data is 1.9166</span><br><span class="line"></span><br><span class="line">After 14000 traning steps .loss on all data is 1.92266</span><br><span class="line"></span><br><span class="line">After 14500 traning steps .loss on all data is 1.92372</span><br><span class="line"></span><br><span class="line">After 15000 traning steps .loss on all data is 1.92138</span><br><span class="line"></span><br><span class="line">After 15500 traning steps .loss on all data is 1.92035</span><br><span class="line"></span><br><span class="line">After 16000 traning steps .loss on all data is 1.93678</span><br><span class="line"></span><br><span class="line">After 16500 traning steps .loss on all data is 1.91698</span><br><span class="line"></span><br><span class="line">After 17000 traning steps .loss on all data is 1.91242</span><br><span class="line"></span><br><span class="line">After 17500 traning steps .loss on all data is 1.91433</span><br><span class="line"></span><br><span class="line">After 18000 traning steps .loss on all data is 1.9214</span><br><span class="line"></span><br><span class="line">After 18500 traning steps .loss on all data is 1.91902</span><br><span class="line"></span><br><span class="line">After 19000 traning steps .loss on all data is 1.9181</span><br><span class="line"></span><br><span class="line">After 19500 traning steps .loss on all data is 1.92847</span><br><span class="line"></span><br><span class="line">After 20000 traning steps .loss on all data is 1.92319</span><br><span class="line"></span><br><span class="line">After 20500 traning steps .loss on all data is 1.9251</span><br><span class="line"></span><br><span class="line">After 21000 traning steps .loss on all data is 1.92055</span><br><span class="line"></span><br><span class="line">After 21500 traning steps .loss on all data is 1.92064</span><br><span class="line"></span><br><span class="line">After 22000 traning steps .loss on all data is 1.92688</span><br><span class="line"></span><br><span class="line">After 22500 traning steps .loss on all data is 1.91041</span><br><span class="line"></span><br><span class="line">After 23000 traning steps .loss on all data is 1.9244</span><br><span class="line"></span><br><span class="line">After 23500 traning steps .loss on all data is 1.91478</span><br><span class="line"></span><br><span class="line">After 24000 traning steps .loss on all data is 1.93026</span><br><span class="line"></span><br><span class="line">After 24500 traning steps .loss on all data is 1.92528</span><br><span class="line"></span><br><span class="line">After 25000 traning steps .loss on all data is 1.91988</span><br><span class="line"></span><br><span class="line">After 25500 traning steps .loss on all data is 1.92191</span><br><span class="line"></span><br><span class="line">After 26000 traning steps .loss on all data is 1.91879</span><br><span class="line"></span><br><span class="line">After 26500 traning steps .loss on all data is 1.92378</span><br><span class="line"></span><br><span class="line">After 27000 traning steps .loss on all data is 1.93147</span><br><span class="line"></span><br><span class="line">After 27500 traning steps .loss on all data is 1.92041</span><br><span class="line"></span><br><span class="line">After 28000 traning steps .loss on all data is 1.91586</span><br><span class="line"></span><br><span class="line">After 28500 traning steps .loss on all data is 1.93564</span><br><span class="line"></span><br><span class="line">After 29000 traning steps .loss on all data is 1.91248</span><br><span class="line"></span><br><span class="line">After 29500 traning steps .loss on all data is 1.9137</span><br><span class="line"></span><br><span class="line">After 30000 traning steps .loss on all data is 1.90911</span><br><span class="line"></span><br><span class="line">After 30500 traning steps .loss on all data is 1.93321</span><br><span class="line"></span><br><span class="line">After 31000 traning steps .loss on all data is 1.91788</span><br><span class="line"></span><br><span class="line">After 31500 traning steps .loss on all data is 1.9224</span><br><span class="line"></span><br><span class="line">After 32000 traning steps .loss on all data is 1.92398</span><br><span class="line"></span><br><span class="line">After 32500 traning steps .loss on all data is 1.92205</span><br><span class="line"></span><br><span class="line">After 33000 traning steps .loss on all data is 1.92061</span><br><span class="line"></span><br><span class="line">After 33500 traning steps .loss on all data is 1.93744</span><br><span class="line"></span><br><span class="line">After 34000 traning steps .loss on all data is 1.91723</span><br><span class="line"></span><br><span class="line">After 34500 traning steps .loss on all data is 1.91268</span><br><span class="line"></span><br><span class="line">After 35000 traning steps .loss on all data is 1.94162</span><br><span class="line"></span><br><span class="line">After 35500 traning steps .loss on all data is 1.92091</span><br><span class="line"></span><br><span class="line">After 36000 traning steps .loss on all data is 1.91968</span><br><span class="line"></span><br><span class="line">After 36500 traning steps .loss on all data is 1.90689</span><br><span class="line"></span><br><span class="line">After 37000 traning steps .loss on all data is 1.92873</span><br><span class="line"></span><br><span class="line">After 37500 traning steps .loss on all data is 1.92386</span><br><span class="line"></span><br><span class="line">After 38000 traning steps .loss on all data is 1.92536</span><br><span class="line"></span><br><span class="line">After 38500 traning steps .loss on all data is 1.9208</span><br><span class="line"></span><br><span class="line">After 39000 traning steps .loss on all data is 1.92803</span><br><span class="line"></span><br><span class="line">After 39500 traning steps .loss on all data is 1.91743</span><br><span class="line"></span><br><span class="line">After 40000 traning steps .loss on all data is 1.94342</span><br><span class="line"></span><br><span class="line">After 40500 traning steps .loss on all data is 1.91406</span><br><span class="line"></span><br><span class="line">After 41000 traning steps .loss on all data is 1.9095</span><br><span class="line"></span><br><span class="line">After 41500 traning steps .loss on all data is 1.91283</span><br><span class="line"></span><br><span class="line">After 42000 traning steps .loss on all data is 1.92122</span><br><span class="line"></span><br><span class="line">After 42500 traning steps .loss on all data is 1.94632</span><br><span class="line"></span><br><span class="line">After 43000 traning steps .loss on all data is 1.91325</span><br><span class="line"></span><br><span class="line">After 43500 traning steps .loss on all data is 1.94504</span><br><span class="line"></span><br><span class="line">After 44000 traning steps .loss on all data is 1.91904</span><br><span class="line"></span><br><span class="line">After 44500 traning steps .loss on all data is 1.92311</span><br><span class="line"></span><br><span class="line">After 45000 traning steps .loss on all data is 1.91575</span><br><span class="line"></span><br><span class="line">After 45500 traning steps .loss on all data is 1.9293</span><br><span class="line"></span><br><span class="line">After 46000 traning steps .loss on all data is 1.92475</span><br><span class="line"></span><br><span class="line">After 46500 traning steps .loss on all data is 1.92085</span><br><span class="line"></span><br><span class="line">After 47000 traning steps .loss on all data is 1.92138</span><br><span class="line"></span><br><span class="line">After 47500 traning steps .loss on all data is 1.93145</span><br><span class="line"></span><br><span class="line">After 48000 traning steps .loss on all data is 1.918</span><br><span class="line"></span><br><span class="line">After 48500 traning steps .loss on all data is 1.91345</span><br><span class="line"></span><br><span class="line">After 49000 traning steps .loss on all data is 1.91463</span><br><span class="line"></span><br><span class="line">After 49500 traning steps .loss on all data is 1.91008</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W1:</span><br><span class="line"></span><br><span class="line"> [[1.0201502]</span><br><span class="line"></span><br><span class="line"> [1.042119 ]]</span><br></pre></td></tr></table></figure><p>从结果看出，正如我们所期待的，他确实有在在往大了预测。</p><h4 id="交叉熵ce-可以表征两个概率分母之间的距离"><a href="#交叉熵ce-可以表征两个概率分母之间的距离" class="headerlink" title="交叉熵ce 可以表征两个概率分母之间的距离"></a>交叉熵ce 可以表征两个概率分母之间的距离</h4><script type="math/tex; mode=display">H(y\_,y) = - \sum_ {} y\_ * \log{y}</script><p>交叉熵的应用，二分类问题:</p><blockquote><p>已知答案y_ = (1,0), 预测 $y_1$ = (0.6,0.4) 和 $y_2$ = (0.8,0.2) 那个更接近答案?</p></blockquote><p>$H_1((1,0),(0.6,0.4)) = -(1<em>\log{0.6} + 0 </em> \log{0.4}) \approx 0.222$</p><p>$H_2((1,0),(0.8,0.2)) = -(1<em>\log{0.8} + 0 </em> \log{0.2}) \approx 0.097$</p><p>从以上结果可以看出 $H_2$ 更符合标准答案</p><p>当m分类的n个输出($y_1,y_2,y_3…y_n$)通过<code>softmax()</code>函数以满足概率分部的要求</p><script type="math/tex; mode=display">\forall x,P(X=x) \in {[0,1] \bigcap \sum_ {x} P(X = x) = 1}</script><script type="math/tex; mode=display">sorfmax(y_i) = \frac {e^{y_i} } {\sum_{i=1}^{n} e^{y_i} }</script><h3 id="学习率α-learning-rate"><a href="#学习率α-learning-rate" class="headerlink" title="学习率α learning_rate"></a>学习率α learning_rate</h3><script type="math/tex; mode=display">W_{n+1} = W_n - α·loss·\nabla</script><script type="math/tex; mode=display">更新后的参数 = 当前参数 - 学习率 \times 损失函数的导数（梯度算子）</script><p>其中,$\nabla = \frac{\partial^2 loss}{\partial w}$</p><blockquote><p>例:设损失函数$loss = (w+1)^2$ 则 $\nabla = \frac{\partial^2 loss}{\partial w} = 2w+2$</p></blockquote><p>参数w初始化为5</p><div class="table-container"><table><thead><tr><th>次数</th><th>参数</th><th>结果</th></tr></thead><tbody><tr><td>1</td><td>5</td><td>2.6</td></tr><tr><td>2</td><td>2.6</td><td>1.16</td></tr><tr><td>3</td><td>1.16</td><td>0.296</td></tr><tr><td>…</td><td>…</td><td>…</td></tr></tbody></table></div><p><img src="http://ww1.sinaimg.cn/large/006Uvlfagy1g0lbgmx9alj30g80fmwf2.jpg" alt=""></p><p>学习率大了可能不收敛，学习率小了可能收敛太慢，故引入了指数衰减学习率的算法：</p><script type="math/tex; mode=display">learning\_rate = learning\_rate\_base * learning\_rate\_decay^{\frac {global_step} {learning\_rate\_step} }</script><script type="math/tex; mode=display">学习率 = 学习率基数 * 学习率衰减率（0，1）^{\frac {运行的轮数} {多少轮更新一次}}</script><p>其中, $ learning_rate_step = \frac {SIZE} {batch_size} $ , $多少轮更新一次 = \frac {数据集大小} {每次喂入神经网络的数据集大小}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设损失函数 loss = (w+1)^2 令w的初始值是10 反向传播求w最优</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用指数衰减学习率，在迭代初期获得较高的下降速度。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.1</span> <span class="comment">#最初学习率</span></span><br><span class="line"></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span> <span class="comment">#学习率衰减率</span></span><br><span class="line"></span><br><span class="line">LEARNING_RATE_STEP = <span class="number">1</span> <span class="comment"># 喂入多少轮后更新，为了方便设为1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行了多少轮bacth_size的计数器， 初始值是0 设为不可被训练</span></span><br><span class="line"></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>,trainable=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义指数下降学习率</span></span><br><span class="line"></span><br><span class="line">learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,LEARNING_RATE_STEP,LEARNING_RATE_DECAY,staircase = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义待优化参数，初始值为10</span></span><br><span class="line"></span><br><span class="line">w  = tf.Variable(tf.constant(<span class="number">10</span>,dtype=tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数loss</span></span><br><span class="line"></span><br><span class="line">loss = tf.square(w+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义反向传播的方法</span></span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成回话 训练40轮</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    init_op = tf.global_variables_initializer() <span class="comment">#初始化所有变量</span></span><br><span class="line"></span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line"></span><br><span class="line">        sess.run(train_step)</span><br><span class="line"></span><br><span class="line">        learning_rate_val = sess.run(learning_rate)</span><br><span class="line"></span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line"></span><br><span class="line">        global_step_val = sess.run(global_step)</span><br><span class="line"></span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"After %s steps:global_step is %f,w is %f,learning_rate is %f,loss is %f"</span> %(i,global_step_val,w_val,learning_rate_val,loss_val))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">After 0 steps:global_step is 1.000000,w is 7.800000,learning_rate is 0.099000,loss is 77.440002</span><br><span class="line"></span><br><span class="line">After 1 steps:global_step is 2.000000,w is 6.057600,learning_rate is 0.098010,loss is 49.809719</span><br><span class="line"></span><br><span class="line">After 2 steps:global_step is 3.000000,w is 4.674169,learning_rate is 0.097030,loss is 32.196194</span><br><span class="line"></span><br><span class="line">After 3 steps:global_step is 4.000000,w is 3.573041,learning_rate is 0.096060,loss is 20.912704</span><br><span class="line"></span><br><span class="line">After 4 steps:global_step is 5.000000,w is 2.694472,learning_rate is 0.095099,loss is 13.649122</span><br><span class="line"></span><br><span class="line">After 5 steps:global_step is 6.000000,w is 1.991791,learning_rate is 0.094148,loss is 8.950810</span><br><span class="line"></span><br><span class="line">After 6 steps:global_step is 7.000000,w is 1.428448,learning_rate is 0.093207,loss is 5.897361</span><br><span class="line"></span><br><span class="line">After 7 steps:global_step is 8.000000,w is 0.975754,learning_rate is 0.092274,loss is 3.903603</span><br><span class="line"></span><br><span class="line">After 8 steps:global_step is 9.000000,w is 0.611130,learning_rate is 0.091352,loss is 2.595741</span><br><span class="line"></span><br><span class="line">After 9 steps:global_step is 10.000000,w is 0.316771,learning_rate is 0.090438,loss is 1.733887</span><br><span class="line"></span><br><span class="line">After 10 steps:global_step is 11.000000,w is 0.078598,learning_rate is 0.089534,loss is 1.163374</span><br><span class="line"></span><br><span class="line">After 11 steps:global_step is 12.000000,w is -0.114544,learning_rate is 0.088638,loss is 0.784033</span><br><span class="line"></span><br><span class="line">After 12 steps:global_step is 13.000000,w is -0.271515,learning_rate is 0.087752,loss is 0.530691</span><br><span class="line"></span><br><span class="line">After 13 steps:global_step is 14.000000,w is -0.399367,learning_rate is 0.086875,loss is 0.360760</span><br><span class="line"></span><br><span class="line">After 14 steps:global_step is 15.000000,w is -0.503727,learning_rate is 0.086006,loss is 0.246287</span><br><span class="line"></span><br><span class="line">After 15 steps:global_step is 16.000000,w is -0.589091,learning_rate is 0.085146,loss is 0.168846</span><br><span class="line"></span><br><span class="line">After 16 steps:global_step is 17.000000,w is -0.659066,learning_rate is 0.084294,loss is 0.116236</span><br><span class="line"></span><br><span class="line">After 17 steps:global_step is 18.000000,w is -0.716543,learning_rate is 0.083451,loss is 0.080348</span><br><span class="line"></span><br><span class="line">After 18 steps:global_step is 19.000000,w is -0.763853,learning_rate is 0.082617,loss is 0.055765</span><br><span class="line"></span><br><span class="line">After 19 steps:global_step is 20.000000,w is -0.802872,learning_rate is 0.081791,loss is 0.038859</span><br><span class="line"></span><br><span class="line">After 20 steps:global_step is 21.000000,w is -0.835119,learning_rate is 0.080973,loss is 0.027186</span><br><span class="line"></span><br><span class="line">After 21 steps:global_step is 22.000000,w is -0.861821,learning_rate is 0.080163,loss is 0.019094</span><br><span class="line"></span><br><span class="line">After 22 steps:global_step is 23.000000,w is -0.883974,learning_rate is 0.079361,loss is 0.013462</span><br><span class="line"></span><br><span class="line">After 23 steps:global_step is 24.000000,w is -0.902390,learning_rate is 0.078568,loss is 0.009528</span><br><span class="line"></span><br><span class="line">After 24 steps:global_step is 25.000000,w is -0.917728,learning_rate is 0.077782,loss is 0.006769</span><br><span class="line"></span><br><span class="line">After 25 steps:global_step is 26.000000,w is -0.930527,learning_rate is 0.077004,loss is 0.004827</span><br><span class="line"></span><br><span class="line">After 26 steps:global_step is 27.000000,w is -0.941226,learning_rate is 0.076234,loss is 0.003454</span><br><span class="line"></span><br><span class="line">After 27 steps:global_step is 28.000000,w is -0.950187,learning_rate is 0.075472,loss is 0.002481</span><br><span class="line"></span><br><span class="line">After 28 steps:global_step is 29.000000,w is -0.957706,learning_rate is 0.074717,loss is 0.001789</span><br><span class="line"></span><br><span class="line">After 29 steps:global_step is 30.000000,w is -0.964026,learning_rate is 0.073970,loss is 0.001294</span><br><span class="line"></span><br><span class="line">After 30 steps:global_step is 31.000000,w is -0.969348,learning_rate is 0.073230,loss is 0.000940</span><br><span class="line"></span><br><span class="line">After 31 steps:global_step is 32.000000,w is -0.973838,learning_rate is 0.072498,loss is 0.000684</span><br><span class="line"></span><br><span class="line">After 32 steps:global_step is 33.000000,w is -0.977631,learning_rate is 0.071773,loss is 0.000500</span><br><span class="line"></span><br><span class="line">After 33 steps:global_step is 34.000000,w is -0.980842,learning_rate is 0.071055,loss is 0.000367</span><br><span class="line"></span><br><span class="line">After 34 steps:global_step is 35.000000,w is -0.983565,learning_rate is 0.070345,loss is 0.000270</span><br><span class="line"></span><br><span class="line">After 35 steps:global_step is 36.000000,w is -0.985877,learning_rate is 0.069641,loss is 0.000199</span><br><span class="line"></span><br><span class="line">After 36 steps:global_step is 37.000000,w is -0.987844,learning_rate is 0.068945,loss is 0.000148</span><br><span class="line"></span><br><span class="line">After 37 steps:global_step is 38.000000,w is -0.989520,learning_rate is 0.068255,loss is 0.000110</span><br><span class="line"></span><br><span class="line">After 38 steps:global_step is 39.000000,w is -0.990951,learning_rate is 0.067573,loss is 0.000082</span><br><span class="line"></span><br><span class="line">After 39 steps:global_step is 40.000000,w is -0.992174,learning_rate is 0.066897,loss is 0.000061</span><br></pre></td></tr></table></figure><p>从结果我们可以看到，随着学习率不断衰减 w的变化速度不一样 最终衰减到接近-1</p><h2 id="滑动平均（影子）"><a href="#滑动平均（影子）" class="headerlink" title="滑动平均（影子）"></a>滑动平均（影子）</h2><p>滑动平均记录了每个参数的一段时间内过往的值的平均 增加了模型的泛化性 像是给参数增加了影子 参数变化 影子缓慢跟随</p><script type="math/tex; mode=display">影子 = 衰减率*影子+(1-衰减率)*参数</script><script type="math/tex; mode=display">衰减率 = min(MOVING\_AVERAGE\_BECAY,\frac {1+轮数} {10+轮数})</script><blockquote><p>例</p></blockquote><p>moving_average_becay为0.99（一般比较大）参数w1为0，轮数global_step为0，w1的滑动平均值为0</p><p>参数w1更新为1 则:</p><p>$w1的滑动平均值 = min(0.99, \frac 1{10}) \times0 + (1-min(0.99,\frac 1 {10})*1) = 0.9$</p><blockquote><p>轮数global_step更新为100，w1更新为10 则</p></blockquote><p>$w1的滑动平均值 = min(0.99, \frac {101}{110}) \times0 + (1-min(0.99,\frac {101} {110})*10) = 1.644$</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>当我们发现模型在训练集上的正确率非常高，但在从未见过的数据上表现不好 。则成这种现象见<strong><strong>过拟合</strong></strong>。</p><p>使用正则化可以有效缓解过拟合，再存是函数中引入模型的复杂度指标，利用w加权值 弱化了训练数据的噪声(一般不正则化偏置项b)</p><script type="math/tex; mode=display">loss = loss(y \&\& y\_) + REGUALRIZER * loss(w)</script><p>两种正则化方法：</p><script type="math/tex; mode=display">loss = \begin{cases} loss_{l_1}(w) = \sum_{i} \vert w_i \vert \\ loss_{l_2}(w) = \sum_{i}  \vert w_i^2 \vert \end{cases}</script>]]></content>
      
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
