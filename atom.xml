<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Always Be a Student</title>
  
  <subtitle>努力提高自身的不可替代性</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-02-28T09:46:51.735Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Peter W</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TensorFlow学习笔记-更新中</title>
    <link href="http://yoursite.com/2019/02/28/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9B%B4%E6%96%B0%E4%B8%AD/"/>
    <id>http://yoursite.com/2019/02/28/TensorFlow学习笔记-更新中/</id>
    <published>2019-02-28T09:45:41.000Z</published>
    <updated>2019-02-28T09:46:51.735Z</updated>
    
    <content type="html"><![CDATA[<p><strong><strong>该笔记需要有一定的前置知识:高等数学，线性代数，概率论与数理统计，配合吴恩达机器学习，TensorFlow学习手册食用更佳( ´∀｀)σ</strong></strong></p><a id="more"></a><h1 id="什么是TensorFlow？"><a href="#什么是TensorFlow？" class="headerlink" title="什么是TensorFlow？"></a>什么是TensorFlow？</h1><p><img src="quiver-image-url/EA82027E5845C04802D5CD59FB1ADDF6.jpg =1400x635" alt="IMAGE"></p><blockquote><p>TensorFlow™ 是一个开放源代码软件库，用于进行高性能数值计算。借助其灵活的架构，用户可以轻松地将计算工作部署到多种平台（CPU、GPU、TPU）和设备（桌面设备、服务器集群、移动设备、边缘设备等）。TensorFlow™ 最初是由 Google Brain 团队（隶属于 Google 的 AI 部门）中的研究人员和工程师开发的，可为机器学习和深度学习提供强力支持，并且其灵活的数值计算核心广泛应用于许多其他科学领域。</p></blockquote><p>摘自<a href="https://www.tensorflow.org/?hl=zh_cn" target="_blank" rel="noopener">TensorFlow中文网</a></p><h1 id="什么是神经网络（以下简称NN）"><a href="#什么是神经网络（以下简称NN）" class="headerlink" title="什么是神经网络（以下简称NN）?"></a>什么是神经网络（以下简称NN）?</h1><blockquote><p>人工神经网络（英语：Artificial Neural Network，ANN），简称神经网络（Neural Network，NN）或类神经网络，在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统，通俗的讲就是具备学习功能。现代神经网络是一种非线性统计性数据建模工具。<br>神经网络的构筑理念是受到生物（人或其他动物）神经网络功能的运作启发而产生的。人工神经网络通常是通过一个基于数学统计学类型的学习方法（Learning Method）得以优化，所以也是数学统计学方法的一种实际应用，通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间，另一方面在人工智能学的人工感知领域，我们通过数学统计学的应用可以来做人工感知方面的决定问题（也就是说通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力），这种方法比起正式的逻辑学推理演算更具有优势。<br>和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被传统基于规则的编程所解决的。</p></blockquote><p>摘自<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener">维基百科人工神经网络</a></p><h1 id="使用TensorFlow的NN"><a href="#使用TensorFlow的NN" class="headerlink" title="使用TensorFlow的NN"></a>使用TensorFlow的NN</h1><ul><li>张量表示数据</li><li>计算图搭建网络</li><li>会话执行计算图</li><li>优化线上权重(参数)得到模型</li></ul><h2 id="张量-tensor"><a href="#张量-tensor" class="headerlink" title="张量(tensor)"></a>张量(tensor)</h2><p>tensor也是多重数组(列表)<br><strong><strong>阶</strong></strong>叫做tensor的维数</p><table><thead><tr><th>.</th><th>.</th><th>.</th><th>.</th></tr></thead><tbody><tr><td>0-D</td><td>0 阶</td><td>标量 scalar</td><td>1,2,3</td></tr><tr><td>1-D</td><td>1 阶</td><td>向量 vector</td><td>[1,2,3]</td></tr><tr><td>2-D</td><td>2 阶</td><td>矩阵 martix</td><td>[[1,2,3],[2,3,4],[3,4,5]]</td></tr><tr><td>n-D</td><td>n阶</td><td>张量tensor</td><td>[[[…</td></tr></tbody></table><h2 id="TensorFlow的第一个例子"><a href="#TensorFlow的第一个例子" class="headerlink" title="TensorFlow的第一个例子"></a>TensorFlow的第一个例子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.constant([<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result = a+b</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result</span><br><span class="line">&lt;tf.Tensor <span class="string">'add:0'</span> shape=(<span class="number">2</span>,) dtype=float32&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">      sess.run(result)</span><br><span class="line"></span><br><span class="line">array([<span class="number">4.</span>, <span class="number">6.</span>], dtype=float32)</span><br></pre></td></tr></table></figure><p>代码 <code>&lt;tf.Tensor &#39;add:0&#39; shape=(2,) dtype=float32&gt;</code>的解释:</p><p><code>add</code> -&gt; 节点名<br><code>0</code> -&gt; 0个输出<br><code>shape=</code> -&gt; 维度<br><code>(2,)</code> -&gt; 一维，长度2<br><code>dtype = float32</code> -&gt; 数据类型</p><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>计算图是指搭建神经网络的过程，<strong><strong>只搭建，不计算</strong></strong>。</p><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p>神经元是神经网络的基本单位</p><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1g0kp5ua3qrj30je0d6jsc.jpg" alt=""></p><h3 id="神经元的计算过程"><a href="#神经元的计算过程" class="headerlink" title="神经元的计算过程"></a>神经元的计算过程</h3><p> 上图为例,这个神经元的计算过程为：<br> $$y = x_1\times w_1 + x_2 \times w_2$$<br> 写成矩阵乘法的形式：<br>$$<br>    X = \left(\matrix{x_1 \cr x_2\cr}\right),<br>    W = \left(\matrix{w_1&amp;w_2}\right)<br>$$</p><p>$$<br>    Y = X \times W = \left(\matrix{x_1\cr x_2\cr } \right) \times \left(\matrix{ w_1 &amp; w_2 }\right) =     x_1\times w_1 + x_2\times  w_2<br>$$</p><h3 id="会话Session"><a href="#会话Session" class="headerlink" title="会话Session"></a>会话Session</h3><p><code>TensorFlow</code>的需要用<code>Session</code>的<code>with</code>结构来输出，执行完计算图的过程之后只能得到一个<code>TensorFlow</code>的<code>Objective</code>，如果执行输出，就会抛出一个<code>TypeError</code>的<code>exception</code></p><p><code>must be real number, not Tensor</code></p><h4 id="会话结构"><a href="#会话结构" class="headerlink" title="会话结构"></a>会话结构</h4> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(result)</span><br></pre></td></tr></table></figure><h3 id="NN结构的权重-参数"><a href="#NN结构的权重-参数" class="headerlink" title="NN结构的权重(参数)"></a>NN结构的权重(参数)</h3><p>一般情况下W由变量表示，一开始随机生成，随后通过神经网络的反向传播和损失函数（loss function）或代价函数（cost function）不断调整W直到模型最优。W的生成方式可用正态分布生成<br><code>w = tf.Variable(tf.random_normal([2,3],stddev = 2,mean = 0,seed = 1))</code></p><h4 id="参数解释"><a href="#参数解释" class="headerlink" title="参数解释"></a>参数解释</h4><ul><li><code>tf.random_normal()</code>指正态分布的W，还可用去掉过大偏离点的正态分布(大于标准差)<code>tf.turncated_normal()</code>和平均分布<code>tf.rnadom_uniform()</code></li></ul><ul><li><code>[2,3]</code>表示2*3的矩阵</li><li><code>stddev = 2</code>表示标准差为2</li><li><code>mean</code>表示均值</li><li><code>seed</code>表示随机种子</li></ul><p>其中，标准差，均值和随机种子是缺省的。</p><h2 id="神经网络的实现过程"><a href="#神经网络的实现过程" class="headerlink" title="神经网络的实现过程"></a>神经网络的实现过程</h2><ul><li>准备数据集，提取特征值，作为输入喂给神经网络</li><li>搭建NN结果，从输入到输出，先搭建计算图，再执行会话(向前传播过程)</li><li>大量特征数据喂给NN，迭代优化NN参数，直到模型最优(反向传播过程)</li><li>使用训练好的模型预测和分类</li></ul><h2 id="向前传播-搭建模型和推理的过程"><a href="#向前传播-搭建模型和推理的过程" class="headerlink" title="向前传播(搭建模型和推理的过程)"></a>向前传播(搭建模型和推理的过程)</h2><blockquote><p>例1(全连接网络)<br>生产一批零件的体积$x_1$和重量$x_2$为特征值输入NN，通过NN后输出一个数值，NN已给出，要求写出计算过程。</p></blockquote><p><img src="quiver-image-url/DFCF53C3E788950B2488DDC974821F33.jpg =692x381" alt="IMAGE"></p><p>计算过程：</p><p>$$X = \left(\matrix{x_1 &amp; x_2} \right ),W^{(1)} = \left(\matrix{w^{(1)}<em>{1,1} &amp; w^{(1)}</em>{1,2} &amp; w^{(1)}<em>{1,3} \cr w^{(1)}</em>{2,1} &amp; w^{(1)}<em>{2,2} &amp; w^{(1)}</em>{2,3} }\right)$$</p><p>$$a^{(1)} = \left(\matrix{a_{1,1} &amp; a_{1,2} &amp; a_{1,3}}\right) = X * W^{(1)} $$</p><p>$$W^{(2)} = \left(\matrix{w^{(2)}<em>{1,1} \cr w^{(2)}</em>{2,1} \cr w^{(2)}_{3,1}}\right)$$</p><p>$$Y = a^{(1)} * W^{(2)}$$</p><h2 id="反向传播-训练神经网络参数"><a href="#反向传播-训练神经网络参数" class="headerlink" title="反向传播(训练神经网络参数)"></a>反向传播(训练神经网络参数)</h2><p>反向传播使用损失函数使得NN模型在训练函数上的损失最小</p><p>损失函数(loss) 预测值(y)和标准值(y_)的差距</p><p>均方误差MSE： </p><p>$$MSE(y__,y) = \frac {\sum_{i=1}^{n} {(y__ - y)^2}} {n} $$</p><p>在<code>TensorFlow</code>的表示:</p><p><code>loss = tf.reduce_mean(tf.square(y_ - y))</code></p><blockquote><p>例1续： 设${x_1 + x_2 \leq 1表示该零件合格，要求构建神经网络(代码)}$</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#两层的简单神经网络(全连接)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment">### 反向传播算法的理解</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span> <span class="comment">#一次喂入神经网络的大小</span></span><br><span class="line">seed = <span class="number">23455</span> <span class="comment"># 随机种子</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 产生伪数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于seed产生随机数</span></span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"><span class="comment">#随机数字返回32行2列的矩阵 表示体积和重量 作为输入数据集</span></span><br><span class="line">X = rng.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 从X这个32行2列的矩阵中 取出一行 判断 如果和小于1  赋值给Y=1 表示合格  否则Y=0表示不合格</span></span><br><span class="line"><span class="comment"># 作为输入数据集的标签(正确答案)</span></span><br><span class="line">Y = [[int(x0+x1 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x0,x1) <span class="keyword">in</span> X]</span><br><span class="line">print(<span class="string">"X:\n"</span>,X)</span><br><span class="line">print(<span class="string">"Y:\n"</span>,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的输入 参数 和输出 </span></span><br><span class="line"><span class="comment"># 向前传播过程</span></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">1</span>,seed=<span class="number">2</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">a = tf.matmul(x,w1)</span><br><span class="line">y = tf.matmul(a,w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数以及反向传播方法</span></span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"><span class="comment">#train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss)</span></span><br><span class="line"><span class="comment">#train_step = tf.train.AdadeltaOptimizer(0.001).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话 训练STEPS轮</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="comment">#输出未经训练的参数的取值</span></span><br><span class="line">    print(<span class="string">"W1:\n"</span>,sess.run(w1))</span><br><span class="line">    print(<span class="string">"W2:\n"</span>,sess.run(w2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line">    STEPS = <span class="number">6000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">        end = start +BATCH_SIZE</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x: X[start:end],y_:Y[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> ==<span class="number">0</span>:</span><br><span class="line">            total_loss = sess.run(loss,feed_dict=&#123;x:X,y_:Y&#125;)</span><br><span class="line">            print(<span class="string">"After %d traning steps .loss on all data is %g"</span> %(i,total_loss))</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 输出训练后的参数</span></span><br><span class="line">    print(<span class="string">'\n'</span>)</span><br><span class="line">    print(<span class="string">"W1:\n"</span>,sess.run(w1))</span><br><span class="line">    print(<span class="string">"W2:\n"</span>,sess.run(w2))</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">X:</span><br><span class="line"> [[0.83494319 0.11482951]</span><br><span class="line"> [0.66899751 0.46594987]</span><br><span class="line"> [0.60181666 0.58838408]</span><br><span class="line"> [0.31836656 0.20502072]</span><br><span class="line"> [0.87043944 0.02679395]</span><br><span class="line"> [0.41539811 0.43938369]</span><br><span class="line"> [0.68635684 0.24833404]</span><br><span class="line"> [0.97315228 0.68541849]</span><br><span class="line"> [0.03081617 0.89479913]</span><br><span class="line"> [0.24665715 0.28584862]</span><br><span class="line"> [0.31375667 0.47718349]</span><br><span class="line"> [0.56689254 0.77079148]</span><br><span class="line"> [0.7321604  0.35828963]</span><br><span class="line"> [0.15724842 0.94294584]</span><br><span class="line"> [0.34933722 0.84634483]</span><br><span class="line"> [0.50304053 0.81299619]</span><br><span class="line"> [0.23869886 0.9895604 ]</span><br><span class="line"> [0.4636501  0.32531094]</span><br><span class="line"> [0.36510487 0.97365522]</span><br><span class="line"> [0.73350238 0.83833013]</span><br><span class="line"> [0.61810158 0.12580353]</span><br><span class="line"> [0.59274817 0.18779828]</span><br><span class="line"> [0.87150299 0.34679501]</span><br><span class="line"> [0.25883219 0.50002932]</span><br><span class="line"> [0.75690948 0.83429824]</span><br><span class="line"> [0.29316649 0.05646578]</span><br><span class="line"> [0.10409134 0.88235166]</span><br><span class="line"> [0.06727785 0.57784761]</span><br><span class="line"> [0.38492705 0.48384792]</span><br><span class="line"> [0.69234428 0.19687348]</span><br><span class="line"> [0.42783492 0.73416985]</span><br><span class="line"> [0.09696069 0.04883936]]</span><br><span class="line">Y:</span><br><span class="line"> [[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]</span><br><span class="line">2019-02-27 10:49:25.877711: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line">W1:</span><br><span class="line"> [[-0.85811085 -0.19662298  0.13895045]</span><br><span class="line"> [-1.2212768  -0.40341285 -1.1454041 ]]</span><br><span class="line">W2:</span><br><span class="line"> [[-0.8113182 ]</span><br><span class="line"> [ 1.4845988 ]</span><br><span class="line"> [ 0.06532937]]</span><br><span class="line">After 0 traning steps .loss on all data is 0.396911</span><br><span class="line">After 500 traning steps .loss on all data is 0.390187</span><br><span class="line">After 1000 traning steps .loss on all data is 0.386858</span><br><span class="line">After 1500 traning steps .loss on all data is 0.385192</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W1:</span><br><span class="line"> [[-0.9150884  -0.09321376  0.14530948]</span><br><span class="line"> [-1.1892611  -0.4614115  -1.1490022 ]]</span><br><span class="line">W2:</span><br><span class="line"> [[-0.8259251 ]</span><br><span class="line"> [ 1.4913318 ]</span><br><span class="line"> [ 0.11993836]]</span><br></pre></td></tr></table></figure></p><h2 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h2><h3 id="普通神经元模型"><a href="#普通神经元模型" class="headerlink" title="普通神经元模型"></a>普通神经元模型</h3><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1g0ktlipq8oj30va0la76z.jpg" alt=""></p><h3 id="McCulloch-Pitts神经元模型"><a href="#McCulloch-Pitts神经元模型" class="headerlink" title="McCulloch-Pitts神经元模型"></a>McCulloch-Pitts神经元模型</h3><p><img src="quiver-image-url/105470007921C3C4016EB12D0AC4EAC8.jpg =565x483" alt="IMAGE"></p><p>加入激活函数解决了线性模型不能解决的异或模型，其中$f(\sum_ {i}{} x_i*w_i + b)$是激活函数(activaction function),b是偏置项(bios)</p><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><ul><li>relu<br>$$f(x) = max(x,0) = \begin{cases}0 \qquad\qquad x \leq 0 \ x \qquad\qquad x \geq 0\end{cases}$$</li></ul><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1g0ku2ezaryj30ri0jcwf6.jpg" alt=""></p><ul><li>sigmoid</li></ul><p>$$f(x) = \frac {1} {1+e^{-x}}$$</p><p><img src="quiver-image-url/05B299D312C88BDAC09D58613221F1A7.jpg =827x396" alt="IMAGE"></p><ul><li>tanh</li></ul><p>$$f(x) = \frac {1-e^{-2x}} {1 + e^{-2x}}$$</p><p><img src="quiver-image-url/8B6B3867F2D0C168E25206E0CD2FD62D.jpg =824x417" alt="IMAGE"></p><h2 id="神经网络的优化"><a href="#神经网络的优化" class="headerlink" title="神经网络的优化"></a>神经网络的优化</h2><ul><li>损失函数loss</li><li>学习率 learning_rate</li><li>平均滑动 ema</li><li>正则化 regularization</li></ul><h3 id="损失函数（loss-预测值y-与已知答案-y-的差距"><a href="#损失函数（loss-预测值y-与已知答案-y-的差距" class="headerlink" title="损失函数（loss) 预测值y_与已知答案(y_)的差距"></a>损失函数（loss) 预测值y_与已知答案(y_)的差距</h3><p>NN的优化目标： loss最小</p><p>$$损失函数 \to \begin{cases}  mse \quad(Mean Squared Error) \quad 均方误差 \ 自定义 \ ce \quad (Cross Entropy) \quad  交叉熵\end{cases}$$</p><h4 id="均方误差mse"><a href="#均方误差mse" class="headerlink" title="均方误差mse:"></a>均方误差mse:</h4><p>$$ MSE(y_,y) = \frac {\sum_ {i=1}^{n} (y__ -y)^2} {n}$$</p><blockquote><p>例2：构建一层神经网络,预测酸奶日销量y, $x_1$,$x_2$ 是影响因素。</p></blockquote><p>分析: 建模前，应该采集的数据集有：每日的$x_1$和$x_2$ 销量y_。（在此例子中，我们你早数据X,Y_ : $Y__ = X_1 + X_2$，再加入-0.5~+0.5的噪声使得伪数据集更真实）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#两层的简单神经网络(全连接)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment">### 酸奶预测</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span> <span class="comment">#一次喂入神经网络的大小</span></span><br><span class="line">seed = <span class="number">23455</span> <span class="comment"># 随机种子</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于seed产生随机数</span></span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"><span class="comment">#随机数字返回32行2列的矩阵 表示体积和重量 作为输入数据集</span></span><br><span class="line">X = rng.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line">Y_ = [[x1+x2+(rng.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的输入 参数 和输出 </span></span><br><span class="line"><span class="comment"># 向前传播过程</span></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">y = tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数以及反向传播方法</span></span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"><span class="comment">#train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss)</span></span><br><span class="line"><span class="comment">#train_step = tf.train.AdadeltaOptimizer(0.001).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话 训练STEPS轮</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer() <span class="comment">#初始化所有变量</span></span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line">    STEPS = <span class="number">50000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">        end = start +BATCH_SIZE</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x: X[start:end],y_:Y_[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> ==<span class="number">0</span>:</span><br><span class="line">            total_loss = sess.run(loss,feed_dict=&#123;x:X,y_:Y_&#125;)</span><br><span class="line">            print(<span class="string">"After %d traning steps .loss on all data is %g"</span> %(i,total_loss))</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 输出训练后的参数</span></span><br><span class="line">    print(<span class="string">'\n'</span>)</span><br><span class="line">    print(<span class="string">"W1:\n"</span>,sess.run(w1))</span><br></pre></td></tr></table></figure><p>结果:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">After 0 traning steps .loss on all data is 2.48876</span><br><span class="line">After 500 traning steps .loss on all data is 0.85528</span><br><span class="line">After 1000 traning steps .loss on all data is 0.302797</span><br><span class="line">After 1500 traning steps .loss on all data is 0.114376</span><br><span class="line">After 2000 traning steps .loss on all data is 0.0488448</span><br><span class="line">After 2500 traning steps .loss on all data is 0.0250235</span><br><span class="line">After 3000 traning steps .loss on all data is 0.0155473</span><br><span class="line">After 3500 traning steps .loss on all data is 0.0111573</span><br><span class="line">After 4000 traning steps .loss on all data is 0.00869355</span><br><span class="line">After 4500 traning steps .loss on all data is 0.00705474</span><br><span class="line">After 5000 traning steps .loss on all data is 0.00583891</span><br><span class="line">After 5500 traning steps .loss on all data is 0.00488469</span><br><span class="line">After 6000 traning steps .loss on all data is 0.00411648</span><br><span class="line">After 6500 traning steps .loss on all data is 0.00349116</span><br><span class="line">After 7000 traning steps .loss on all data is 0.00297992</span><br><span class="line">After 7500 traning steps .loss on all data is 0.00256107</span><br><span class="line">After 8000 traning steps .loss on all data is 0.00221769</span><br><span class="line">After 8500 traning steps .loss on all data is 0.00193611</span><br><span class="line">After 9000 traning steps .loss on all data is 0.00170515</span><br><span class="line">After 9500 traning steps .loss on all data is 0.00151571</span><br><span class="line">After 10000 traning steps .loss on all data is 0.00136033</span><br><span class="line">After 10500 traning steps .loss on all data is 0.00123288</span><br><span class="line">After 11000 traning steps .loss on all data is 0.00112833</span><br><span class="line">After 11500 traning steps .loss on all data is 0.00104257</span><br><span class="line">After 12000 traning steps .loss on all data is 0.000972239</span><br><span class="line">After 12500 traning steps .loss on all data is 0.000914546</span><br><span class="line">After 13000 traning steps .loss on all data is 0.000867223</span><br><span class="line">After 13500 traning steps .loss on all data is 0.000828402</span><br><span class="line">After 14000 traning steps .loss on all data is 0.000796561</span><br><span class="line">After 14500 traning steps .loss on all data is 0.000770443</span><br><span class="line">After 15000 traning steps .loss on all data is 0.000749021</span><br><span class="line">After 15500 traning steps .loss on all data is 0.000731453</span><br><span class="line">After 16000 traning steps .loss on all data is 0.000717037</span><br><span class="line">After 16500 traning steps .loss on all data is 0.000705214</span><br><span class="line">After 17000 traning steps .loss on all data is 0.000695519</span><br><span class="line">After 17500 traning steps .loss on all data is 0.000687564</span><br><span class="line">After 18000 traning steps .loss on all data is 0.000681038</span><br><span class="line">After 18500 traning steps .loss on all data is 0.000675687</span><br><span class="line">After 19000 traning steps .loss on all data is 0.000671295</span><br><span class="line">After 19500 traning steps .loss on all data is 0.000667701</span><br><span class="line">After 20000 traning steps .loss on all data is 0.000664741</span><br><span class="line">After 20500 traning steps .loss on all data is 0.000662322</span><br><span class="line">After 21000 traning steps .loss on all data is 0.000660335</span><br><span class="line">After 21500 traning steps .loss on all data is 0.000658707</span><br><span class="line">After 22000 traning steps .loss on all data is 0.000657368</span><br><span class="line">After 22500 traning steps .loss on all data is 0.000656275</span><br><span class="line">After 23000 traning steps .loss on all data is 0.000655372</span><br><span class="line">After 23500 traning steps .loss on all data is 0.000654635</span><br><span class="line">After 24000 traning steps .loss on all data is 0.000654034</span><br><span class="line">After 24500 traning steps .loss on all data is 0.000653543</span><br><span class="line">After 25000 traning steps .loss on all data is 0.000653136</span><br><span class="line">After 25500 traning steps .loss on all data is 0.000652797</span><br><span class="line">After 26000 traning steps .loss on all data is 0.000652516</span><br><span class="line">After 26500 traning steps .loss on all data is 0.000652293</span><br><span class="line">After 27000 traning steps .loss on all data is 0.000652109</span><br><span class="line">After 27500 traning steps .loss on all data is 0.000651959</span><br><span class="line">After 28000 traning steps .loss on all data is 0.000651836</span><br><span class="line">After 28500 traning steps .loss on all data is 0.000651735</span><br><span class="line">After 29000 traning steps .loss on all data is 0.00065165</span><br><span class="line">After 29500 traning steps .loss on all data is 0.000651582</span><br><span class="line">After 30000 traning steps .loss on all data is 0.000651528</span><br><span class="line">After 30500 traning steps .loss on all data is 0.000651481</span><br><span class="line">After 31000 traning steps .loss on all data is 0.000651442</span><br><span class="line">After 31500 traning steps .loss on all data is 0.000651411</span><br><span class="line">After 32000 traning steps .loss on all data is 0.000651385</span><br><span class="line">After 32500 traning steps .loss on all data is 0.000651364</span><br><span class="line">After 33000 traning steps .loss on all data is 0.000651348</span><br><span class="line">After 33500 traning steps .loss on all data is 0.000651334</span><br><span class="line">After 34000 traning steps .loss on all data is 0.000651323</span><br><span class="line">After 34500 traning steps .loss on all data is 0.000651315</span><br><span class="line">After 35000 traning steps .loss on all data is 0.000651308</span><br><span class="line">After 35500 traning steps .loss on all data is 0.000651303</span><br><span class="line">After 36000 traning steps .loss on all data is 0.000651296</span><br><span class="line">After 36500 traning steps .loss on all data is 0.000651291</span><br><span class="line">After 37000 traning steps .loss on all data is 0.000651286</span><br><span class="line">After 37500 traning steps .loss on all data is 0.000651284</span><br><span class="line">After 38000 traning steps .loss on all data is 0.000651282</span><br><span class="line">After 38500 traning steps .loss on all data is 0.00065128</span><br><span class="line">After 39000 traning steps .loss on all data is 0.000651279</span><br><span class="line">After 39500 traning steps .loss on all data is 0.000651278</span><br><span class="line">After 40000 traning steps .loss on all data is 0.000651277</span><br><span class="line">After 40500 traning steps .loss on all data is 0.000651276</span><br><span class="line">After 41000 traning steps .loss on all data is 0.000651275</span><br><span class="line">After 41500 traning steps .loss on all data is 0.000651274</span><br><span class="line">After 42000 traning steps .loss on all data is 0.000651273</span><br><span class="line">After 42500 traning steps .loss on all data is 0.000651272</span><br><span class="line">After 43000 traning steps .loss on all data is 0.000651272</span><br><span class="line">After 43500 traning steps .loss on all data is 0.000651271</span><br><span class="line">After 44000 traning steps .loss on all data is 0.000651271</span><br><span class="line">After 44500 traning steps .loss on all data is 0.000651271</span><br><span class="line">After 45000 traning steps .loss on all data is 0.00065127</span><br><span class="line">After 45500 traning steps .loss on all data is 0.00065127</span><br><span class="line">After 46000 traning steps .loss on all data is 0.000651271</span><br><span class="line">After 46500 traning steps .loss on all data is 0.00065127</span><br><span class="line">After 47000 traning steps .loss on all data is 0.00065127</span><br><span class="line">After 47500 traning steps .loss on all data is 0.00065127</span><br><span class="line">After 48000 traning steps .loss on all data is 0.00065127</span><br><span class="line">After 48500 traning steps .loss on all data is 0.00065127</span><br><span class="line">After 49000 traning steps .loss on all data is 0.00065127</span><br><span class="line">After 49500 traning steps .loss on all data is 0.00065127</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W1:</span><br><span class="line"> [[1.0042378 ]</span><br><span class="line"> [0.99486846]]</span><br><span class="line">Terminated: 15</span><br></pre></td></tr></table></figure></p><p>从以上结果我们可以看出，最终得到两个参数$w_1 = 1.004$ ,$w_2 = 0.995$ 。都非常接近1，也证明我们找到的参数正确(符合伪造数据集中的$y__ = x_1 + x_2$).</p><blockquote><p>接例2：为了参数预测多了，损失成本COST1元，参数预测少了，损失利润PROFIT9元，为了利益最大化，我们希望尽量损失成本。</p></blockquote><p>分析：<br>可以自定义损失函数$$ loss(y__,y) = \sum_ {n} f(y__ - y) $$<br>$$f(y__ - y) =  \begin{cases} PROFIT <em> (y__ - y) \qquad y \leq y__  \ COST </em> (y - y__ ) \qquad \quad y__ \leq y\end{cases}$$</p><p>代码实现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#两层的简单神经网络(全连接)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment">### 酸奶预测</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span> <span class="comment">#一次喂入神经网络的大小</span></span><br><span class="line">seed = <span class="number">23455</span> <span class="comment"># 随机种子</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于seed产生随机数</span></span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"><span class="comment">#随机数字返回32行2列的矩阵 表示体积和重量 作为输入数据集</span></span><br><span class="line">X = rng.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line">Y_ = [[x1+x2+(rng.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的输入 参数 和输出 </span></span><br><span class="line"><span class="comment"># 向前传播过程</span></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">y = tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数以及反向传播方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#loss = tf.reduce_mean(tf.square(y - y_))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#自定义损失函数</span></span><br><span class="line">COST = <span class="number">1</span></span><br><span class="line">PROFIT = <span class="number">9</span></span><br><span class="line">loss = tf.reduce_sum(tf.where(tf.greater(y,y_),COST*(y-y_),PROFIT*(y_-y)))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"><span class="comment">#train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss)</span></span><br><span class="line"><span class="comment">#train_step = tf.train.AdadeltaOptimizer(0.001).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话 训练STEPS轮</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer() <span class="comment">#初始化所有变量</span></span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line">    STEPS = <span class="number">50000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">        end = start +BATCH_SIZE</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x: X[start:end],y_:Y_[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> ==<span class="number">0</span>:</span><br><span class="line">            total_loss = sess.run(loss,feed_dict=&#123;x:X,y_:Y_&#125;)</span><br><span class="line">            print(<span class="string">"After %d traning steps .loss on all data is %g"</span> %(i,total_loss))</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 输出训练后的参数</span></span><br><span class="line">    print(<span class="string">'\n'</span>)</span><br><span class="line">    print(<span class="string">"W1:\n"</span>,sess.run(w1))</span><br></pre></td></tr></table></figure></p><p>结果<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">After 0 traning steps .loss on all data is 419.879</span><br><span class="line">After 500 traning steps .loss on all data is 1.91246</span><br><span class="line">After 1000 traning steps .loss on all data is 1.9079</span><br><span class="line">After 1500 traning steps .loss on all data is 1.91344</span><br><span class="line">After 2000 traning steps .loss on all data is 1.92183</span><br><span class="line">After 2500 traning steps .loss on all data is 1.92732</span><br><span class="line">After 3000 traning steps .loss on all data is 1.91853</span><br><span class="line">After 3500 traning steps .loss on all data is 1.92395</span><br><span class="line">After 4000 traning steps .loss on all data is 1.93173</span><br><span class="line">After 4500 traning steps .loss on all data is 1.92058</span><br><span class="line">After 5000 traning steps .loss on all data is 1.91603</span><br><span class="line">After 5500 traning steps .loss on all data is 1.93591</span><br><span class="line">After 6000 traning steps .loss on all data is 1.91265</span><br><span class="line">After 6500 traning steps .loss on all data is 1.91397</span><br><span class="line">After 7000 traning steps .loss on all data is 1.90928</span><br><span class="line">After 7500 traning steps .loss on all data is 1.93353</span><br><span class="line">After 8000 traning steps .loss on all data is 1.91814</span><br><span class="line">After 8500 traning steps .loss on all data is 1.92215</span><br><span class="line">After 9000 traning steps .loss on all data is 1.92415</span><br><span class="line">After 9500 traning steps .loss on all data is 1.92232</span><br><span class="line">After 10000 traning steps .loss on all data is 1.92602</span><br><span class="line">After 10500 traning steps .loss on all data is 1.91997</span><br><span class="line">After 11000 traning steps .loss on all data is 1.9279</span><br><span class="line">After 11500 traning steps .loss on all data is 1.91976</span><br><span class="line">After 12000 traning steps .loss on all data is 1.92452</span><br><span class="line">After 12500 traning steps .loss on all data is 1.91997</span><br><span class="line">After 13000 traning steps .loss on all data is 1.92115</span><br><span class="line">After 13500 traning steps .loss on all data is 1.9166</span><br><span class="line">After 14000 traning steps .loss on all data is 1.92266</span><br><span class="line">After 14500 traning steps .loss on all data is 1.92372</span><br><span class="line">After 15000 traning steps .loss on all data is 1.92138</span><br><span class="line">After 15500 traning steps .loss on all data is 1.92035</span><br><span class="line">After 16000 traning steps .loss on all data is 1.93678</span><br><span class="line">After 16500 traning steps .loss on all data is 1.91698</span><br><span class="line">After 17000 traning steps .loss on all data is 1.91242</span><br><span class="line">After 17500 traning steps .loss on all data is 1.91433</span><br><span class="line">After 18000 traning steps .loss on all data is 1.9214</span><br><span class="line">After 18500 traning steps .loss on all data is 1.91902</span><br><span class="line">After 19000 traning steps .loss on all data is 1.9181</span><br><span class="line">After 19500 traning steps .loss on all data is 1.92847</span><br><span class="line">After 20000 traning steps .loss on all data is 1.92319</span><br><span class="line">After 20500 traning steps .loss on all data is 1.9251</span><br><span class="line">After 21000 traning steps .loss on all data is 1.92055</span><br><span class="line">After 21500 traning steps .loss on all data is 1.92064</span><br><span class="line">After 22000 traning steps .loss on all data is 1.92688</span><br><span class="line">After 22500 traning steps .loss on all data is 1.91041</span><br><span class="line">After 23000 traning steps .loss on all data is 1.9244</span><br><span class="line">After 23500 traning steps .loss on all data is 1.91478</span><br><span class="line">After 24000 traning steps .loss on all data is 1.93026</span><br><span class="line">After 24500 traning steps .loss on all data is 1.92528</span><br><span class="line">After 25000 traning steps .loss on all data is 1.91988</span><br><span class="line">After 25500 traning steps .loss on all data is 1.92191</span><br><span class="line">After 26000 traning steps .loss on all data is 1.91879</span><br><span class="line">After 26500 traning steps .loss on all data is 1.92378</span><br><span class="line">After 27000 traning steps .loss on all data is 1.93147</span><br><span class="line">After 27500 traning steps .loss on all data is 1.92041</span><br><span class="line">After 28000 traning steps .loss on all data is 1.91586</span><br><span class="line">After 28500 traning steps .loss on all data is 1.93564</span><br><span class="line">After 29000 traning steps .loss on all data is 1.91248</span><br><span class="line">After 29500 traning steps .loss on all data is 1.9137</span><br><span class="line">After 30000 traning steps .loss on all data is 1.90911</span><br><span class="line">After 30500 traning steps .loss on all data is 1.93321</span><br><span class="line">After 31000 traning steps .loss on all data is 1.91788</span><br><span class="line">After 31500 traning steps .loss on all data is 1.9224</span><br><span class="line">After 32000 traning steps .loss on all data is 1.92398</span><br><span class="line">After 32500 traning steps .loss on all data is 1.92205</span><br><span class="line">After 33000 traning steps .loss on all data is 1.92061</span><br><span class="line">After 33500 traning steps .loss on all data is 1.93744</span><br><span class="line">After 34000 traning steps .loss on all data is 1.91723</span><br><span class="line">After 34500 traning steps .loss on all data is 1.91268</span><br><span class="line">After 35000 traning steps .loss on all data is 1.94162</span><br><span class="line">After 35500 traning steps .loss on all data is 1.92091</span><br><span class="line">After 36000 traning steps .loss on all data is 1.91968</span><br><span class="line">After 36500 traning steps .loss on all data is 1.90689</span><br><span class="line">After 37000 traning steps .loss on all data is 1.92873</span><br><span class="line">After 37500 traning steps .loss on all data is 1.92386</span><br><span class="line">After 38000 traning steps .loss on all data is 1.92536</span><br><span class="line">After 38500 traning steps .loss on all data is 1.9208</span><br><span class="line">After 39000 traning steps .loss on all data is 1.92803</span><br><span class="line">After 39500 traning steps .loss on all data is 1.91743</span><br><span class="line">After 40000 traning steps .loss on all data is 1.94342</span><br><span class="line">After 40500 traning steps .loss on all data is 1.91406</span><br><span class="line">After 41000 traning steps .loss on all data is 1.9095</span><br><span class="line">After 41500 traning steps .loss on all data is 1.91283</span><br><span class="line">After 42000 traning steps .loss on all data is 1.92122</span><br><span class="line">After 42500 traning steps .loss on all data is 1.94632</span><br><span class="line">After 43000 traning steps .loss on all data is 1.91325</span><br><span class="line">After 43500 traning steps .loss on all data is 1.94504</span><br><span class="line">After 44000 traning steps .loss on all data is 1.91904</span><br><span class="line">After 44500 traning steps .loss on all data is 1.92311</span><br><span class="line">After 45000 traning steps .loss on all data is 1.91575</span><br><span class="line">After 45500 traning steps .loss on all data is 1.9293</span><br><span class="line">After 46000 traning steps .loss on all data is 1.92475</span><br><span class="line">After 46500 traning steps .loss on all data is 1.92085</span><br><span class="line">After 47000 traning steps .loss on all data is 1.92138</span><br><span class="line">After 47500 traning steps .loss on all data is 1.93145</span><br><span class="line">After 48000 traning steps .loss on all data is 1.918</span><br><span class="line">After 48500 traning steps .loss on all data is 1.91345</span><br><span class="line">After 49000 traning steps .loss on all data is 1.91463</span><br><span class="line">After 49500 traning steps .loss on all data is 1.91008</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W1:</span><br><span class="line"> [[1.0201502]</span><br><span class="line"> [1.042119 ]]</span><br></pre></td></tr></table></figure></p><p>从结果看出，正如我们所期待的，他确实有在在往大了预测。</p><h4 id="交叉熵ce-可以表征两个概率分母之间的距离"><a href="#交叉熵ce-可以表征两个概率分母之间的距离" class="headerlink" title="交叉熵ce 可以表征两个概率分母之间的距离"></a>交叉熵ce 可以表征两个概率分母之间的距离</h4><p>$$H(y__,y) = - \sum_ {} y__ * \log{y}$$</p><p>交叉熵的应用，二分类问题:</p><blockquote><p>已知答案y_ = (1,0), 预测 $y_1$ = (0.6,0.4) 和 $y_2$ = (0.8,0.2) 那个更接近答案?</p></blockquote><p>$H_1((1,0),(0.6,0.4)) = -(1<em>\log{0.6} + 0 </em> \log{0.4}) \approx 0.222$<br>$H_2((1,0),(0.8,0.2)) = -(1<em>\log{0.8} + 0 </em> \log{0.2}) \approx 0.097$</p><p>从以上结果可以看出 $H_2$ 更符合标准答案</p><p>当m分类的n个输出($y_1,y_2,y_3…y_n$)通过<code>softmax()</code>函数以满足概率分部的要求</p><p>$$\forall x,P(X=x) \in {[0,1] \bigcap \sum_ {x} P(X = x) = 1}$$</p><p>  $$sorfmax(y_i) = \frac {e^{y_i}} {\sum_{i=1}^{n} e^{y_i}}$$</p><h3 id="学习率α-learning-rate"><a href="#学习率α-learning-rate" class="headerlink" title="学习率α learning_rate"></a>学习率α learning_rate</h3><p>$$W_{n+1} = W_n - α·loss·\nabla$$</p><p>$$更新后的参数 = 当前参数 - 学习率 \times 损失函数的导数（梯度算子）$$</p><p>其中,$\nabla = \frac{\partial^2 loss}{\partial w}$</p><blockquote><p>例:设损失函数$loss = (w+1)^2$ 则 $\nabla = \frac{\partial^2 loss}{\partial w} = 2w+2$</p></blockquote><p>参数w初始化为5</p><table><thead><tr><th>次数</th><th>参数</th><th>结果</th></tr></thead><tbody><tr><td>1</td><td>5</td><td>2.6</td></tr><tr><td>2</td><td>2.6</td><td>1.16</td></tr><tr><td>3</td><td>1.16</td><td>0.296</td></tr><tr><td>…</td><td>…</td><td>…</td></tr></tbody></table><p><img src="http://ww1.sinaimg.cn/large/006Uvlfagy1g0lbgmx9alj30g80fmwf2.jpg" alt=""></p><p>学习率大了可能不收敛，学习率小了可能收敛太慢，故引入了指数衰减学习率的算法：</p><p>$$learning__rate = learning__rate__base * learning__rate__decay^{\frac {global_step} {learning__rate__step}}$$</p><p>$$学习率 = 学习率基数 * 学习率衰减率（0，1）^{\frac {运行的轮数} {多少轮更新一次}}$$</p><p>其中,$learning__rate__step = \frac {SIZE} {batch__size}$,$多少轮更新一次 = \frac {数据集大小} {每次喂入神经网络的数据集大小}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#设损失函数 loss = (w+1)^2 令w的初始值是10 反向传播求w最优</span></span><br><span class="line"><span class="comment">#使用指数衰减学习率，在迭代初期获得较高的下降速度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.1</span> <span class="comment">#最初学习率</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span> <span class="comment">#学习率衰减率</span></span><br><span class="line">LEARNING_RATE_STEP = <span class="number">1</span> <span class="comment"># 喂入多少轮后更新，为了方便设为1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行了多少轮bacth_size的计数器， 初始值是0 设为不可被训练</span></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>,trainable=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 定义指数下降学习率</span></span><br><span class="line">learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,LEARNING_RATE_STEP,LEARNING_RATE_DECAY,staircase = <span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 定义待优化参数，初始值为10</span></span><br><span class="line">w  = tf.Variable(tf.constant(<span class="number">10</span>,dtype=tf.float32))</span><br><span class="line"><span class="comment"># 定义损失函数loss</span></span><br><span class="line">loss = tf.square(w+<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 定义反向传播的方法</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成回话 训练40轮</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer() <span class="comment">#初始化所有变量</span></span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        learning_rate_val = sess.run(learning_rate)</span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line">        global_step_val = sess.run(global_step)</span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line">        print(<span class="string">"After %s steps:global_step is %f,w is %f,learning_rate is %f,loss is %f"</span> %(i,global_step_val,w_val,learning_rate_val,loss_val))</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">After 0 steps:global_step is 1.000000,w is 7.800000,learning_rate is 0.099000,loss is 77.440002</span><br><span class="line">After 1 steps:global_step is 2.000000,w is 6.057600,learning_rate is 0.098010,loss is 49.809719</span><br><span class="line">After 2 steps:global_step is 3.000000,w is 4.674169,learning_rate is 0.097030,loss is 32.196194</span><br><span class="line">After 3 steps:global_step is 4.000000,w is 3.573041,learning_rate is 0.096060,loss is 20.912704</span><br><span class="line">After 4 steps:global_step is 5.000000,w is 2.694472,learning_rate is 0.095099,loss is 13.649122</span><br><span class="line">After 5 steps:global_step is 6.000000,w is 1.991791,learning_rate is 0.094148,loss is 8.950810</span><br><span class="line">After 6 steps:global_step is 7.000000,w is 1.428448,learning_rate is 0.093207,loss is 5.897361</span><br><span class="line">After 7 steps:global_step is 8.000000,w is 0.975754,learning_rate is 0.092274,loss is 3.903603</span><br><span class="line">After 8 steps:global_step is 9.000000,w is 0.611130,learning_rate is 0.091352,loss is 2.595741</span><br><span class="line">After 9 steps:global_step is 10.000000,w is 0.316771,learning_rate is 0.090438,loss is 1.733887</span><br><span class="line">After 10 steps:global_step is 11.000000,w is 0.078598,learning_rate is 0.089534,loss is 1.163374</span><br><span class="line">After 11 steps:global_step is 12.000000,w is -0.114544,learning_rate is 0.088638,loss is 0.784033</span><br><span class="line">After 12 steps:global_step is 13.000000,w is -0.271515,learning_rate is 0.087752,loss is 0.530691</span><br><span class="line">After 13 steps:global_step is 14.000000,w is -0.399367,learning_rate is 0.086875,loss is 0.360760</span><br><span class="line">After 14 steps:global_step is 15.000000,w is -0.503727,learning_rate is 0.086006,loss is 0.246287</span><br><span class="line">After 15 steps:global_step is 16.000000,w is -0.589091,learning_rate is 0.085146,loss is 0.168846</span><br><span class="line">After 16 steps:global_step is 17.000000,w is -0.659066,learning_rate is 0.084294,loss is 0.116236</span><br><span class="line">After 17 steps:global_step is 18.000000,w is -0.716543,learning_rate is 0.083451,loss is 0.080348</span><br><span class="line">After 18 steps:global_step is 19.000000,w is -0.763853,learning_rate is 0.082617,loss is 0.055765</span><br><span class="line">After 19 steps:global_step is 20.000000,w is -0.802872,learning_rate is 0.081791,loss is 0.038859</span><br><span class="line">After 20 steps:global_step is 21.000000,w is -0.835119,learning_rate is 0.080973,loss is 0.027186</span><br><span class="line">After 21 steps:global_step is 22.000000,w is -0.861821,learning_rate is 0.080163,loss is 0.019094</span><br><span class="line">After 22 steps:global_step is 23.000000,w is -0.883974,learning_rate is 0.079361,loss is 0.013462</span><br><span class="line">After 23 steps:global_step is 24.000000,w is -0.902390,learning_rate is 0.078568,loss is 0.009528</span><br><span class="line">After 24 steps:global_step is 25.000000,w is -0.917728,learning_rate is 0.077782,loss is 0.006769</span><br><span class="line">After 25 steps:global_step is 26.000000,w is -0.930527,learning_rate is 0.077004,loss is 0.004827</span><br><span class="line">After 26 steps:global_step is 27.000000,w is -0.941226,learning_rate is 0.076234,loss is 0.003454</span><br><span class="line">After 27 steps:global_step is 28.000000,w is -0.950187,learning_rate is 0.075472,loss is 0.002481</span><br><span class="line">After 28 steps:global_step is 29.000000,w is -0.957706,learning_rate is 0.074717,loss is 0.001789</span><br><span class="line">After 29 steps:global_step is 30.000000,w is -0.964026,learning_rate is 0.073970,loss is 0.001294</span><br><span class="line">After 30 steps:global_step is 31.000000,w is -0.969348,learning_rate is 0.073230,loss is 0.000940</span><br><span class="line">After 31 steps:global_step is 32.000000,w is -0.973838,learning_rate is 0.072498,loss is 0.000684</span><br><span class="line">After 32 steps:global_step is 33.000000,w is -0.977631,learning_rate is 0.071773,loss is 0.000500</span><br><span class="line">After 33 steps:global_step is 34.000000,w is -0.980842,learning_rate is 0.071055,loss is 0.000367</span><br><span class="line">After 34 steps:global_step is 35.000000,w is -0.983565,learning_rate is 0.070345,loss is 0.000270</span><br><span class="line">After 35 steps:global_step is 36.000000,w is -0.985877,learning_rate is 0.069641,loss is 0.000199</span><br><span class="line">After 36 steps:global_step is 37.000000,w is -0.987844,learning_rate is 0.068945,loss is 0.000148</span><br><span class="line">After 37 steps:global_step is 38.000000,w is -0.989520,learning_rate is 0.068255,loss is 0.000110</span><br><span class="line">After 38 steps:global_step is 39.000000,w is -0.990951,learning_rate is 0.067573,loss is 0.000082</span><br><span class="line">After 39 steps:global_step is 40.000000,w is -0.992174,learning_rate is 0.066897,loss is 0.000061</span><br></pre></td></tr></table></figure></p><p>从结果我们可以看到，随着学习率不断衰减 w的变化速度不一样 最终衰减到接近-1</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;strong&gt;该笔记需要有一定的前置知识:高等数学，线性代数，概率论与数理统计，配合吴恩达机器学习，TensorFlow学习手册食用更佳( ´∀｀)σ&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>list</title>
    <link href="http://yoursite.com/2019/01/30/list/"/>
    <id>http://yoursite.com/2019/01/30/list/</id>
    <published>2019-01-30T09:21:13.000Z</published>
    <updated>2019-01-30T09:21:13.987Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>IOS-Objective-C开发学习笔记-四</title>
    <link href="http://yoursite.com/2019/01/29/IOS-Objective-C%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9B%9B/"/>
    <id>http://yoursite.com/2019/01/29/IOS-Objective-C开发学习笔记-四/</id>
    <published>2019-01-29T07:55:44.000Z</published>
    <updated>2019-01-29T12:14:45.039Z</updated>
    
    <content type="html"><![CDATA[<p>补上OC的基础部分 解构OC程序 解构<code>HelloWorld</code> <code>#import</code> <code>NSLog()和@“”字符串</code> <code>BOOL</code></p><a id="more"></a><h3 id="解构hello-world"><a href="#解构hello-world" class="headerlink" title="解构hello world"></a>解构<code>hello world</code></h3><p><code>main.h</code></p><figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#import <span class="meta-string">&lt;Foundation/Foundation.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> main(<span class="keyword">int</span> argc, <span class="keyword">const</span> <span class="keyword">char</span> * argv[]) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        <span class="comment">// insert code here...</span></span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@"Hello, World!"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="import-lt-Foundation-Foundation-h-gt"><a href="#import-lt-Foundation-Foundation-h-gt" class="headerlink" title="#import &lt;Foundation/Foundation.h&gt;"></a><code>#import &lt;Foundation/Foundation.h&gt;</code></h4><ul><li><code>#import</code><ul><li>和C语言一样 <code>#import</code> 用来包含元素声明，这些元素可以是结构体，符号常量，方法原型的等。</li><li><code>#include</code>也可以完成这个工作 ，但<code>#import</code>可以保证对同一个文件只进行一次包含，而无论他出现过多少次。C语言中用<code>#ifdef</code>来实现</li></ul></li><li>&lt;Foundation/Foundation.h&gt;<ul><li><code>#import &lt;Foundation/Foundation.h&gt;</code>的意思是，包含<code>Foundation</code>框架的<code>Foundation.h</code>文件。</li><li>框架是一种非常重要的技术集合</li></ul></li></ul><h4 id="NSLog-和-quot-quot"><a href="#NSLog-和-quot-quot" class="headerlink" title="NSLog()和@&quot;&quot;"></a><code>NSLog()</code>和<code>@&quot;&quot;</code></h4><ul><li><code>NSLog()</code>和<code>printf()</code>有着一样的功能，接受一个字符串作为第一个参数，可以包含<br><code>%d,%f</code>等占位符。</li><li><code>NSLog()</code>还增加了一个非常方便的特性在打印时间戳日期戳的时候会添加一个自动换行<code>\n</code>。</li><li><code>NS</code>前缀表示他是一个来自Cocoa的工具包,在写程序时，尽量避免自己的函数名带有<code>NS</code>前缀，函数使用<code>NS</code>前缀(NeXT)和人类的阑尾差不多，是一个历史遗留问题。。</li><li><code>@&quot;&quot;</code>表示一个NS类型的字符串:<code>NSString</code>。</li></ul><h4 id="BOOL"><a href="#BOOL" class="headerlink" title="BOOL"></a><code>BOOL</code></h4><ul><li>objc的bool使用<code>YES</code>和<code>NO</code>来表示。</li><li>objc编译器将BOOL类型认作8bit的二进制数，如果将一个int或者short赋值给objc，那么只有低位字符起作用:<figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#import <span class="meta-string">&lt;Foundation/Foundation.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> main(<span class="keyword">int</span> argc, <span class="keyword">const</span> <span class="keyword">char</span> * argv[]) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">8960</span>;</span><br><span class="line">        <span class="built_in">BOOL</span> b = i;</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@"b:%d"</span>,b);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>上面的程序会输出 : <code>b:0</code>.<br>所以说，在某些情况下比如:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">areIntDifferent</span><span class="params">(<span class="keyword">int</span> firstValue,<span class="keyword">int</span> secondValue)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> firstValue - secondValue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></blockquote><p>大部分语言都可以返回正确的结果，因为在C语言(等可以返回正确结果的语言)中非0都代表了了TRUE，而OC这样写就很可能埋下BUG！！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;补上OC的基础部分 解构OC程序 解构&lt;code&gt;HelloWorld&lt;/code&gt; &lt;code&gt;#import&lt;/code&gt; &lt;code&gt;NSLog()和@“”字符串&lt;/code&gt; &lt;code&gt;BOOL&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="IOS" scheme="http://yoursite.com/tags/IOS/"/>
    
      <category term="Objective" scheme="http://yoursite.com/tags/Objective/"/>
    
  </entry>
  
  <entry>
    <title>IOS-objc开发学习笔记-三</title>
    <link href="http://yoursite.com/2019/01/28/IOS-Objective-C%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%89/"/>
    <id>http://yoursite.com/2019/01/28/IOS-Objective-C开发学习笔记-三/</id>
    <published>2019-01-28T13:53:56.000Z</published>
    <updated>2019-01-29T07:55:00.962Z</updated>
    
    <content type="html"><![CDATA[<p>类和对象，如何设计，如何实现等以及蛋疼（优雅的）的OC函数命名规则</p><a id="more"></a><h2 id="面向对象基础"><a href="#面向对象基础" class="headerlink" title="面向对象基础"></a>面向对象基础</h2><ul><li>对象和类</li><li>面向对象三大特性<ul><li>封装（隐藏具体的实现逻辑 对外公开接口）</li><li>继承 （OC只有单继承）</li><li>多态（一个方法可以有多种实现）</li></ul></li><li>面向对象思维<ul><li>确定要解决的问题</li><li>根据问题确定对象</li><li>用类来描述</li><li>转换成类代码<h4 id="什么是对象"><a href="#什么是对象" class="headerlink" title="什么是对象"></a>什么是对象</h4></li></ul></li><li>对象是对客观事物的抽象 客观实体</li><li>类是对对象的抽象</li><li>类用来描述对象 是一系列方法和属性的集合</li></ul><h4 id="如何设计一个类"><a href="#如何设计一个类" class="headerlink" title="如何设计一个类"></a>如何设计一个类</h4><ul><li>类的结构</li><li>设计一个手机类</li></ul><h5 id="类的结构"><a href="#类的结构" class="headerlink" title="类的结构"></a>类的结构</h5><ul><li>.m</li><li>.h</li><li>类的结构<ul><li>类名 属性 方法</li></ul></li></ul><p>打开xcode 创建一个工程 -&gt; 再创建一个类(command + n)<br><strong><strong>创建类的时候，首字母大写 ，驼峰原则</strong></strong></p><ul><li>类的声明放在.h文件</li><li>类的实现放在.m文件</li></ul><p>在写属性声明的时候 可以先都写在<code>@interface</code>开头的<code>{}</code>内部（准确的说 叫做类的成员变量<strong><strong>声明成员变量的时候要<code>_</code>开头</strong></strong>）,凡是<code>{}</code>中声明的变量默认都是<code>protected</code></p><p><code>Phone.h</code></p><figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#import <span class="meta-string">&lt;Foundation/Foundation.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">@interface</span> <span class="title">Phone</span> : <span class="title">NSobject</span></span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">@public</span> <span class="comment">//访问域</span></span><br><span class="line">    <span class="built_in">CGFloat</span> _screenSize;</span><br><span class="line">    <span class="built_in">NSString</span> *_color;</span><br><span class="line">    <span class="built_in">CGFloat</span> _memory;</span><br><span class="line">&#125;</span><br><span class="line">- (<span class="keyword">void</span>)makeCallToSomeone:(<span class="built_in">NSString</span> *)someone;</span><br><span class="line">- (<span class="keyword">void</span>)sendMessage:(*<span class="built_in">NSStrng</span>)message toReceiver:(<span class="built_in">NSString</span> *)receiver;</span><br><span class="line"><span class="keyword">@end</span></span><br></pre></td></tr></table></figure><p><code>Phone.m</code><br><figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#import <span class="meta-string">"Phone.h"</span></span></span><br><span class="line"><span class="class"><span class="keyword">@implementation</span> <span class="title">Phone</span></span></span><br><span class="line">- (<span class="keyword">void</span>)makeCallToSomeone:(<span class="built_in">NSString</span> *)someone</span><br><span class="line">&#123;</span><br><span class="line">    MSLog(<span class="string">@"给%@"</span>,someone);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">- (<span class="keyword">void</span>)sendMessage:(<span class="built_in">NSString</span> *)message toReceiver:(<span class="built_in">NSString</span> *)receiver</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@"收件人:%@\n%@"</span>,receiver,message);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><code>main.h</code><br><figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#import <span class="meta-string">&lt;Foundation/Foundation.h&gt;</span></span></span><br><span class="line"><span class="meta">#import <span class="meta-string">"Phone.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> main(<span class="keyword">int</span> argc, consts <span class="keyword">char</span> * argv[])</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        Phone *phone = [Phone new];</span><br><span class="line"></span><br><span class="line">        phone -&gt; _screenSize = <span class="number">4.6</span>;</span><br><span class="line">        phone -&gt; _color = <span class="string">@"black"</span>;</span><br><span class="line">        phone -&gt; _memory = <span class="number">1024</span>;</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@"%@"</span>,phone);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="其次，关于OC的方法命名规范："><a href="#其次，关于OC的方法命名规范：" class="headerlink" title="其次，关于OC的方法命名规范："></a>其次，关于OC的方法命名规范：</h4><p>相比于其他语言(C++)，OC对方法名进行了拆分处理，使得开发者更清晰地知道每个变量是最什么的。非常优雅。(不优雅也要强行优雅)<br>对比：<br><code>C++</code><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">tableViewCommitEditingStyleForRowAtIndexPath</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    UITableView *tableView,</span></span></span><br><span class="line"><span class="function"><span class="params">    UITableViewCellEditingStyle editingStyle,</span></span></span><br><span class="line"><span class="function"><span class="params">    NSIndexPath *indexPath)</span></span>;</span><br></pre></td></tr></table></figure></p><p><code>objc</code><br><figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-  (<span class="keyword">void</span>)tableView:(<span class="built_in">UITableView</span> *)tableView</span><br><span class="line">commitEditingStyle:(<span class="built_in">UITableViewCellEditingStyle</span>)editingStyle</span><br><span class="line"> forRowAtIndexPath:(<span class="built_in">NSIndexPath</span> *)indexPath</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;类和对象，如何设计，如何实现等以及蛋疼（优雅的）的OC函数命名规则&lt;/p&gt;
    
    </summary>
    
    
      <category term="IOS" scheme="http://yoursite.com/tags/IOS/"/>
    
      <category term="objc" scheme="http://yoursite.com/tags/objc/"/>
    
  </entry>
  
  <entry>
    <title>IOS-objc开发学习笔记(二)</title>
    <link href="http://yoursite.com/2019/01/28/IOS-Objective-C%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C/"/>
    <id>http://yoursite.com/2019/01/28/IOS-Objective-C开发学习笔记-二/</id>
    <published>2019-01-28T07:26:58.000Z</published>
    <updated>2019-01-28T14:07:00.442Z</updated>
    
    <content type="html"><![CDATA[<p>关于<code>OC-Block</code>的定义和基本用法</p><h2 id="什么是Block？"><a href="#什么是Block？" class="headerlink" title="什么是Block？"></a>什么是<code>Block</code>？</h2><blockquote><p><code>Block</code> - 块 ；它本身封装了一段代码，并将这段代码当做变量，通过 blockname()的方法，进行回调。和c语言中的，函数指针有着异曲同工之妙。</p></blockquote><a id="more"></a><h2 id="Block的基本用法"><a href="#Block的基本用法" class="headerlink" title="Block的基本用法"></a><code>Block</code>的基本用法</h2><ul><li>C语言中的函数指针：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">int function()</span><br><span class="line">&#123;</span><br><span class="line">  /* CODE */</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int (*pIntfunction)(void);</span><br><span class="line"></span><br><span class="line">*pIntfunction = function;</span><br></pre></td></tr></table></figure></li></ul><p>上面的<code>function()</code>函数可以直接通过<code>(*pIntfunction)()</code>来调用。<br>这个思路和OC中，块的思路差不多，但有以下区别</p><ul><li>块是内联的，效率高于函数</li><li>块对外部变量默认是是只读的</li><li>块被OC当做对象处理</li></ul><p>下面是OC块的实现：<br><figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> main(<span class="keyword">int</span> argc, <span class="keyword">const</span> <span class="keyword">char</span> * argv[]) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        <span class="keyword">int</span> (^myBlock)(<span class="keyword">int</span>,<span class="keyword">int</span>) = ^(<span class="keyword">int</span> firstValue,<span class="keyword">int</span> secondValue) &#123;</span><br><span class="line">            <span class="keyword">return</span> firstValue|secondValue;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="keyword">int</span> myBlockReturn = myBlock(<span class="number">1001</span>,<span class="number">1110</span>);</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@"myBlockReturn = %d"</span>,myBlockReturn);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="块的写法和调用"><a href="#块的写法和调用" class="headerlink" title="块的写法和调用"></a>块的写法和调用</h3><p>在xcode里面，可以输入<code>inlineBlock</code>获取<code>Block</code>的基本写法：</p><figure class="highlight objc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">returnType(^blockName)(parameterTypes) = ^(parameters) &#123;</span><br><span class="line">            statements</span><br><span class="line">        &#125;;</span><br></pre></td></tr></table></figure><p>调用: <code>blockName();</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于&lt;code&gt;OC-Block&lt;/code&gt;的定义和基本用法&lt;/p&gt;
&lt;h2 id=&quot;什么是Block？&quot;&gt;&lt;a href=&quot;#什么是Block？&quot; class=&quot;headerlink&quot; title=&quot;什么是Block？&quot;&gt;&lt;/a&gt;什么是&lt;code&gt;Block&lt;/code&gt;？&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;Block&lt;/code&gt; - 块 ；它本身封装了一段代码，并将这段代码当做变量，通过 blockname()的方法，进行回调。和c语言中的，函数指针有着异曲同工之妙。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="IOS" scheme="http://yoursite.com/tags/IOS/"/>
    
      <category term="objc" scheme="http://yoursite.com/tags/objc/"/>
    
  </entry>
  
  <entry>
    <title>IOS/objc开发学习笔记（一）</title>
    <link href="http://yoursite.com/2019/01/28/IOS-Objective-C%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2019/01/28/IOS-Objective-C开发学习笔记（一）/</id>
    <published>2019-01-27T16:05:00.000Z</published>
    <updated>2019-01-28T14:07:01.579Z</updated>
    
    <content type="html"><![CDATA[<p>打算入门IOS开发，在ReactNative，Swift和objc之间抉择之后，选用OC作为开发的工具。把学习笔记整理发布，以便随时复习。<br><a id="more"></a></p><h2 id="OC-Hello-World"><a href="#OC-Hello-World" class="headerlink" title="OC Hello World"></a>OC Hello World</h2><blockquote><p>一个OC的程序基本有以下部分</p><ul><li>预处理</li><li>接口</li><li>实现</li><li>方法</li><li>变量</li><li>表达式</li><li>注释</li></ul></blockquote><hr><p>OC的简单代码示例（由XCode自动生成）：<br><figure class="highlight objectivec"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#import <span class="meta-string">&lt;Foundation/Foundation.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> main(<span class="keyword">int</span> argc, <span class="keyword">const</span> <span class="keyword">char</span> * argv[]) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        <span class="comment">// insert code here...</span></span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@"Hello, World!"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>控制台输出:<br><code>2019-01-27 23:27:13.818927+0800 OCStudy[1444:3995540] Hello, World!</code></p><p>现在稍微加一点东西：<br><figure class="highlight objectivec"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#import <span class="meta-string">&lt;Foundation/Foundation.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">@interface</span> <span class="title">myClass</span> : <span class="title">NSObject</span></span></span><br><span class="line">- (<span class="keyword">void</span>)myFunction;</span><br><span class="line"><span class="keyword">@end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">@implementation</span> <span class="title">myClass</span></span></span><br><span class="line">- (<span class="keyword">void</span>)myFunction&#123;</span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@"hello world!"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">@end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> main(<span class="keyword">int</span> argc, <span class="keyword">const</span> <span class="keyword">char</span> * argv[]) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        myClass *_myClass = [[myClass alloc] init];</span><br><span class="line">        [_myClass myFunction];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>打印输出仍然是：</p><p><code>2019-01-27 23:27:13.818927+0800 OCStudy[1444:3995540] Hello, World!</code></p><p>关于程序的几个解释：</p><ul><li><code>#import &lt;Foundation/Foundation.h&gt;</code>是一个预处理命令，它告诉<code>objc</code>语言编译器去实际编译之前包含<code>Foundation.h</code>文件</li><li><code>@interface myClass:NSObject</code> 显示了如何创建一个类。它继<code>承NSObject</code>，这是所有对象的基类。</li><li><code>- (void)myFunction</code>声明一个方法。</li><li><code>@end</code>实现结束</li><li><code>NSLog();</code>在控制台打印信息。</li><li><code>myClass *_myClass = [[myClass alloc] init];</code>实例化<code>myClass</code>类，获得一个<code>_myClass</code>对象,<strong><strong>注意:这里不能写作：<code>myClass *myClass = [[myClass alloc] init];</code></strong></strong></li><li><code>[_myClass myFunction]</code>调用<code>_myClass</code> 对象的<code>myFunction</code>方法。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;打算入门IOS开发，在ReactNative，Swift和objc之间抉择之后，选用OC作为开发的工具。把学习笔记整理发布，以便随时复习。&lt;br&gt;
    
    </summary>
    
    
      <category term="IOS" scheme="http://yoursite.com/tags/IOS/"/>
    
      <category term="objc" scheme="http://yoursite.com/tags/objc/"/>
    
  </entry>
  
  <entry>
    <title>linux/osx-把rm命令改为废纸篓</title>
    <link href="http://yoursite.com/2019/01/26/linux-osx-%E6%8A%8Arm%E5%91%BD%E4%BB%A4%E6%94%B9%E4%B8%BA%E5%BA%9F%E7%BA%B8%E7%AF%93/"/>
    <id>http://yoursite.com/2019/01/26/linux-osx-把rm命令改为废纸篓/</id>
    <published>2019-01-26T07:07:18.000Z</published>
    <updated>2019-01-26T07:08:28.637Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>linux的设计哲学假定了用户知道自己干了什么并为自己的行为负责(其实并不知道)，甚至是 <code>sudo rm -rf /</code>,当然我一般是不会愚蠢到执行这个指令，然而昨天还是误删了一大堆重要的东西（还没有备份到时间机器。。。），为了纪念一早上恢复的血泪史和避免今后出现这种问题。决定将rm命令改为移动到回收站.</p></blockquote><a id="more"></a><p>有了这个想法后发现网上已经有了非常成熟的作品了，安装下来试用一番：</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>wget --no-check-certificate https://raw.githubusercontent.com/LaiJingli/rmtrash/master/rmtrash.sh</code><br><code>mv rmtrash.sh /bin/</code><br><code>chmod +x /bin/rmtrash.sh</code></p><h3 id="如果仅对单个用户启用回收站，只需第一次执行如下命令即可："><a href="#如果仅对单个用户启用回收站，只需第一次执行如下命令即可：" class="headerlink" title="如果仅对单个用户启用回收站，只需第一次执行如下命令即可："></a>如果仅对单个用户启用回收站，只需第一次执行如下命令即可：</h3><p><code>/bin/rmtrash.sh</code></p><h3 id="如果想对全局所有用户启用回收站，需要修改bashrc全局配置文件后即可："><a href="#如果想对全局所有用户启用回收站，需要修改bashrc全局配置文件后即可：" class="headerlink" title="如果想对全局所有用户启用回收站，需要修改bashrc全局配置文件后即可："></a>如果想对全局所有用户启用回收站，需要修改bashrc全局配置文件后即可：</h3><p><code>echo &quot;alias rm=/bin/rmtrash.sh&quot; &gt;&gt;/etc/bashrc</code></p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p><code>rm -h</code><br><code>Usage1: rmtrash.sh file1 [file2] [dir3] [....] delete the files or dirs,and mv them to the rmtrash recycle bin</code><br><code>Usage2: rm file1 [file2] [dir3] [....] delete the files or dirs,and mv them to the rmtrash recycle bin</code><br><code>rm is alias to rmtrash.sh.</code></p><h3 id="options"><a href="#options" class="headerlink" title="options:"></a>options:</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-f mv one or more files to the rmtrash recycle bin</span><br><span class="line">-r mv one or more files to the rmtrash recycle bin</span><br><span class="line">-fr mv one or more files to the rmtrash recycle bin</span><br><span class="line">-rf mv one or more files to the rmtrash recycle bin</span><br><span class="line">-R Restore selected files to the originalpath from rmtrash recycle bin</span><br><span class="line">-l list the contens of rmtrash recycle bin</span><br><span class="line">-i show detailed log of the deleted file history</span><br><span class="line">-d delete one or more files by user's input file name from the trash</span><br><span class="line">-e empty the rmtrash recycle bin</span><br><span class="line">-h display this help menu</span><br></pre></td></tr></table></figure><h3 id="如果有问题，执行以下2条命令排查-或者退出重新登录系统"><a href="#如果有问题，执行以下2条命令排查-或者退出重新登录系统" class="headerlink" title="如果有问题，执行以下2条命令排查,或者退出重新登录系统"></a>如果有问题，执行以下2条命令排查,或者退出重新登录系统</h3><p><code>source ~/.bashrc</code><br><code>alias |grep rm</code></p><h2 id="如果需要彻底删除文件"><a href="#如果需要彻底删除文件" class="headerlink" title="如果需要彻底删除文件"></a>如果需要彻底删除文件</h2><ol><li><h1 id="rm-e-清空回收站"><a href="#rm-e-清空回收站" class="headerlink" title="rm -e 清空回收站"></a>rm -e 清空回收站</h1></li><li><h1 id="bin-rm-file-直接删除文件而不经过回收站"><a href="#bin-rm-file-直接删除文件而不经过回收站" class="headerlink" title="/bin/rm file 直接删除文件而不经过回收站"></a>/bin/rm file 直接删除文件而不经过回收站</h1></li></ol><h2 id="适用系统linux、mac-osx"><a href="#适用系统linux、mac-osx" class="headerlink" title="适用系统linux、mac osx"></a>适用系统linux、mac osx</h2>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;linux的设计哲学假定了用户知道自己干了什么并为自己的行为负责(其实并不知道)，甚至是 &lt;code&gt;sudo rm -rf /&lt;/code&gt;,当然我一般是不会愚蠢到执行这个指令，然而昨天还是误删了一大堆重要的东西（还没有备份到时间机器。。。），为了纪念一早上恢复的血泪史和避免今后出现这种问题。决定将rm命令改为移动到回收站.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="OS X" scheme="http://yoursite.com/tags/OS-X/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达机器学习-多变量线性回归</title>
    <link href="http://yoursite.com/2019/01/26/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/01/26/吴恩达机器学习-多变量线性回归/</id>
    <published>2019-01-26T04:41:52.000Z</published>
    <updated>2019-01-26T06:20:13.755Z</updated>
    
    <content type="html"><![CDATA[<h2 id="多特征值"><a href="#多特征值" class="headerlink" title="多特征值"></a>多特征值</h2><h3 id="表示方法"><a href="#表示方法" class="headerlink" title="表示方法"></a>表示方法</h3><a id="more"></a><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxpaf3fj30u60hkmzf.jpg" alt=""></p><ul><li>n <em>特征值的数目</em></li><li>m <em>表示训练集样本数</em></li><li>x_1,x_2…x_n <em>表示n个特征值</em></li><li>y <em>输出变量</em>。</li></ul><ul><li><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp0zttj302a01kq2t.jpg" alt=""> 表示第i个特征向量，他是一个<strong><strong>n</strong></strong>维度向量。</li><li><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxoxmfbj302402a746.jpg" alt="">第i个向量的第j个特征值<blockquote><p>eg:<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxoutsvj30d009k3yw.jpg" alt=""></p></blockquote></li></ul><h3 id="假设形式"><a href="#假设形式" class="headerlink" title="假设形式"></a>假设形式</h3><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxoux62j30wc06kaaf.jpg" alt=""></p><h4 id="例"><a href="#例" class="headerlink" title="例"></a>例</h4><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp3qpsj31640a8q4e.jpg" alt=""><br>某房子的价格 = 基本价格80k+x1平方米<em>0.1k+有x2层</em>0.01k+有x3卧室<em>3k-使用年数</em>2k</p><h4 id="等价形式"><a href="#等价形式" class="headerlink" title="等价形式"></a>等价形式</h4><p>为了方便表示，我们增加一个x0，定义为1，假设此时变成了:</p><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp9n1lj30rm0fk404.jpg" alt=""><br>这就是多元线性回归。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><ul><li>这里不把θ(0-n)看做是n+1个独立的变量，而是考虑看做n+1维的θ向量。</li><li>梯度下降<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp2rfbj30x209w3zb.jpg" alt=""><br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp7i9gj31h00sy782.jpg" alt=""><br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxovvcvj30l40bamy4.jpg" alt=""></li></ul><h2 id="多元梯度下降算法演练"><a href="#多元梯度下降算法演练" class="headerlink" title="多元梯度下降算法演练"></a>多元梯度下降算法演练</h2><h3 id="特征缩放（不需要过于精确，目的只是让梯度下降更快一些）"><a href="#特征缩放（不需要过于精确，目的只是让梯度下降更快一些）" class="headerlink" title="特征缩放（不需要过于精确，目的只是让梯度下降更快一些）"></a>特征缩放（不需要过于精确，目的只是让梯度下降更快一些）</h3><ul><li>当特征值的取值范围相差过多时，代价函数的等值曲线会非常的扁，会导致梯度下降非常慢。</li></ul><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp0c67j30lk0kc413.jpg" alt=""></p><ul><li><p>此时需要对特征值进行放缩处理，使得特征值的取值范围更加接近。<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxovvoij30mm0kkabq.jpg" alt=""></p></li><li><p>一般的，尽量让特征值的取值范围都在(-1,1)之间。（如果不在，但区间接近，问题也不是很大。只要不是过分的大或过分的小。）</p></li></ul><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp5oczj318m0nwtb6.jpg" alt=""></p><p>这些都是可以接受的范围</p><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp0llrj30f609c0sy.jpg" alt=""></p><ul><li>除了除以取值范围之外，我们还可以进行均值归一化处理。<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp8r6jj30pm0bk3zr.jpg" alt=""></li><li>具体做法：减去平均值再除以取值范围<ul><li>例子：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxpei8nj315m0f0jua.jpg" alt=""></li></ul></li></ul><h3 id="学习率α"><a href="#学习率α" class="headerlink" title="学习率α"></a>学习率α</h3><ul><li>我们可以画出J关于迭代次数的函数图像，如果梯度下降算法正常进行，那么每次迭代之后的J的值都应该变小。如果在某一段区间内，函数的图像已经变得非常平缓了，那么说明此时梯度下降算法差不多已经收敛了。<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxpa2lgj30sy0ia0um.jpg" alt=""></li><li><p>这样的函数曲线也可以帮助我们判断梯度下降算法是否正常工作，比如，学习率α过大时：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp6fz8j30z20i6abv.jpg" alt=""></p></li><li><p>尝试一系列的α：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxpbh0ij30yq07s754.jpg" alt=""></p></li></ul><h3 id="可供选择的特征以及如何得到不同的学习算法以及多特征回归"><a href="#可供选择的特征以及如何得到不同的学习算法以及多特征回归" class="headerlink" title="可供选择的特征以及如何得到不同的学习算法以及多特征回归"></a>可供选择的特征以及如何得到不同的学习算法以及多特征回归</h3><ul><li>同一个问题可以有多种特征的选择方法<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxpakngj312i0jswgy.jpg" alt=""></li></ul><h4 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h4><ul><li>如何更好的拟合<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp79kqj316i0j8tay.jpg" alt=""></li></ul><p><strong><strong>如果采取上面的方法，那么放缩法就显得尤为重要！</strong></strong></p><ul><li>选取特征的方法有很多<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxpcxikj312w0jsgnc.jpg" alt=""></li></ul><h4 id="正规方程（不需要放缩）"><a href="#正规方程（不需要放缩）" class="headerlink" title="正规方程（不需要放缩）"></a>正规方程（不需要放缩）</h4><blockquote><p>对于某些线性回归问题，有更好的方法来求得参数θ的最优值。</p></blockquote><p>正规方程区别于迭代的方法求得最优θ，它可以一步求得最优的θ。</p><p>引例：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp11iej319y0ocn0p.jpg" alt=""></p><p>实际上，我们不对向量θ中每一个都求偏导，这样的话过于冗杂。而是用一种等价的方法。</p><ul><li>如何实现正规方程法？<br>给定一个训练集，我们加一列x0 = (1,1,1,…1) 然后构建矩阵X和向量y。使得代价函数最小的θ即可求出。</li></ul><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxpcla4j31ag0pgn1p.jpg" alt=""></p><blockquote><p>证明：<br>设假设函<img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxov5xpj30gw02edg4.jpg" alt=""><br>即<img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxpbpowj304w01o3yh.jpg" alt=""></p></blockquote><p>在拟合成功时，输出函数y约等于<img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp2v69j302k01s747.jpg" alt=""></p><p>故<img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp81m2j304c02amx3.jpg" alt=""><br>在X为方阵的时候，才可逆，即就是：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxoy20aj306a01uaa2.jpg" alt=""></p><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp1wgkj308k02874c.jpg" alt=""></p><ul><li>X (设计矩阵)<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp722cj319y0nytcp.jpg" alt=""></li></ul><h4 id="何时使用正规方程和梯度下降？"><a href="#何时使用正规方程和梯度下降？" class="headerlink" title="何时使用正规方程和梯度下降？"></a>何时使用正规方程和梯度下降？</h4><p>梯度下降和正规方程的优缺点对比：<br>只要特征的数量不是很大 (&lt;10000)都可以采用正规方程法</p><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjvxp4u4rj31e20nqtdn.jpg" alt=""></p><h4 id="正规方程不可逆情况下的解决办法"><a href="#正规方程不可逆情况下的解决办法" class="headerlink" title="正规方程不可逆情况下的解决办法"></a>正规方程不可逆情况下的解决办法</h4><ul><li>检查是否有多余（线性相关）特征（线代知识）</li><li>如果特征过多的话，试着删除一些特征(不影响的情况下)，或者正则化（later）。 </li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;多特征值&quot;&gt;&lt;a href=&quot;#多特征值&quot; class=&quot;headerlink&quot; title=&quot;多特征值&quot;&gt;&lt;/a&gt;多特征值&lt;/h2&gt;&lt;h3 id=&quot;表示方法&quot;&gt;&lt;a href=&quot;#表示方法&quot; class=&quot;headerlink&quot; title=&quot;表示方法&quot;&gt;&lt;/a&gt;表示方法&lt;/h3&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="多变量线性回归" scheme="http://yoursite.com/tags/%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达机器学习-单变量线性回归</title>
    <link href="http://yoursite.com/2019/01/26/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/01/26/吴恩达机器学习-单变量线性回归/</id>
    <published>2019-01-26T03:55:39.000Z</published>
    <updated>2019-01-26T06:24:00.049Z</updated>
    
    <content type="html"><![CDATA[<h3 id="符号定义"><a href="#符号定义" class="headerlink" title="符号定义"></a>符号定义</h3><ul><li>m <em>训练集样本的数量</em></li><li>x <em>输入的特征值</em></li><li>y <em>输出的变量</em></li><li>(x,y) <em>一个训练样本</em></li><li>(x(i), y(i)) <em>第i个训练样本</em><a id="more"></a><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjulzdpb9j30u80e8gmu.jpg" alt=""></li></ul><blockquote><p>我们通常使用h（hypothesis）表示一个函数。在预测房价的例子中，x对应房屋面积，y对应房屋价格，根据监督学习我们可以构建一个函数表达式h：<br>　　hθ(x) = θ0 + θ1x<br>我们将这种可以构建出线性函数的模型称为线性回归模型。在这里只有一个特征变量x，因此我们称这种单一变量模型为单变量线性回归。</p></blockquote><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><h4 id="代价函数的数学定义"><a href="#代价函数的数学定义" class="headerlink" title="代价函数的数学定义"></a>代价函数的数学定义</h4><p>现在我们要做的就是确<br>定θ0和θ1的值，使得h(x)的结果和y相差尽量小，<br>算做平方差尽量小,即让<img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjumi4z2xj30cs03kaa5.jpg" alt="">量小。</p><hr><p>定义代价函数为<img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjun1bzwcj30qo05mt94.jpg" alt="">（前面的求和等是为了让表达式更清晰）</p><hr><p>吴恩达视频笔记：</p><h2 id="IMAGE"><a href="#IMAGE" class="headerlink" title="![IMAGE]"></a>![IMAGE]<img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjunjxzjqj31eu0ukq8e.jpg" alt=""></h2><h4 id="代价函数是用来做什么的以及如何使用"><a href="#代价函数是用来做什么的以及如何使用" class="headerlink" title="代价函数是用来做什么的以及如何使用"></a>代价函数是用来做什么的以及如何使用</h4><ul><li>用简单的代价函数来理解其概念<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjunscf8uj30eg0h6jsb.jpg" alt=""></li></ul><p>当θ=0时，假设函数h(x)和代价函数j(θ1)的情况如图：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjuoimvoej30ve0ksmzr.jpg" alt=""><br>当θ=0.5时，<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjuoz3souj30wu0l8wgp.jpg" alt=""><br>当θ=0时，<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjupgfanjj30v80jc75x.jpg" alt=""></p><p>….</p><p>不同的θ1对应不同的h(x)曲线，对应不同的j(θ)值，在选取出最小的j(θ)，就是线性回归的目标函数。（此例中，θ=1）也就是可以完美的拟合数据。</p><ul><li>进一步理解代价函数（保留两个θ）<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjups43crj30i80hmabl.jpg" alt=""></li></ul><p>此时，代价函数图像变为：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjur4fh3bj30u80iqtax.jpg" alt=""><br>其对应的高度即为goal,可用等高线图表表示：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjuriquc9j30ws0fs76z.jpg" alt=""></p><hr><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>![IMAGE]<img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjus6718aj30uk0em40r.jpg" alt=""><br>给定初始化θ0=0，θ1=0，不停地改变其值，直到求出最小值J或者局部最小值J。</p><h4 id="梯度下降法工作原理"><a href="#梯度下降法工作原理" class="headerlink" title="梯度下降法工作原理"></a>梯度下降法工作原理</h4><p>goal：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjusmk36kj30t20giwgd.jpg" alt=""><br>从某一点出发开始，向四周下降最快的方向迈出一小步。逐步收敛至最低点：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjusvvmhtj30wm0f8mz6.jpg" alt=""><br>如此重复，每次都到达局部最优。<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjuugz9f5j30sy0gcdht.jpg" alt=""></p><h4 id="梯度下降法的数学原理"><a href="#梯度下降法的数学原理" class="headerlink" title="梯度下降法的数学原理"></a>梯度下降法的数学原理</h4><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjuv1zvq8j30t20fgdhb.jpg" alt=""></p><h5 id="符号定义-1"><a href="#符号定义-1" class="headerlink" title="符号定义"></a>符号定义</h5><ul><li><strong>*</strong>:=<strong>*</strong>  _赋值_</li><li><strong>*</strong>=<strong>*</strong>  <em>断言,逻辑运算</em></li><li><strong>*</strong>α<strong>*</strong> <em>学习率，步长，如果α很大，那么梯度下降的就很迅速</em></li><li><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjuzplhs7j305y02u3yl.jpg" alt=""> <em>导数项，暂不讨论</em></li></ul><h5 id="同步更新-θ0和θ1"><a href="#同步更新-θ0和θ1" class="headerlink" title="*!同步更新 θ0和θ1*"></a><strong>*</strong>!同步更新 θ0和θ1<strong>*</strong></h5><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv09yvrnj30vg06u0tx.jpg" alt=""></p><h4 id="深入理解梯度下降算法工作原理"><a href="#深入理解梯度下降算法工作原理" class="headerlink" title="深入理解梯度下降算法工作原理"></a>深入理解梯度下降算法工作原理</h4><h5 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h5><ul><li>导数项<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv0nldp1j314w0pmn0l.jpg" alt=""><br>学习速率<em>导数 === 学习速率</em>斜率</li></ul><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv1lrz2jj30cu09y3yy.jpg" alt=""></p><p>θ经过此次变化后，总向着正确的方向移动了一小点（无论是增加还是减少），也就是更接近最小值。</p><ul><li><p>学习速率α</p><ul><li>步长太小，移动距离很小，需要很多步才能达到全局最优。</li><li>步长太大，移动距离太大，很可能导致无法收敛。</li></ul></li></ul><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv20xh5sj316e0nedje.jpg" alt=""></p><h5 id="解出局部最低点时？"><a href="#解出局部最低点时？" class="headerlink" title="解出局部最低点时？"></a>解出局部最低点时？</h5><p>如果已经处在了最优点（最低点），此时导数项为0，θ不进行更新。（这也是我们所想要的）</p><h5 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h5><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv2ap72qj314q0m4djx.jpg" alt=""></p><p>在越接近局部最优时，导数越来越接近0（曲线越来越平缓），即使α不变，也会逐步找到局部最优解。</p><h3 id="线性回归的梯度下降"><a href="#线性回归的梯度下降" class="headerlink" title="线性回归的梯度下降"></a>线性回归的梯度下降</h3><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv2y8ufyj318g0iiwh2.jpg" alt=""><br>求导：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv3tycqoj315g0jkdj7.jpg" alt=""></p><p><img src="quiver-image-url/8601D7B744201921C6553C44FD41F302.jpg =523x278" alt="IMAGE"></p><h4 id="如何实现？"><a href="#如何实现？" class="headerlink" title="如何实现？"></a>如何实现？</h4><p>问题：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv47z7moj30t20fgdhb.jpg" alt=""></p><p>反复梯度下降：</p><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv4mj385j314s0kmae3.jpg" alt=""><br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv50sw86j31420ii0wo.jpg" alt=""></p><p>越来越符合数据：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv5d9go5j314a0jm789.jpg" alt=""><br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv5pgbrhj314s0jwtcq.jpg" alt=""></p><hr><p>至此，就是batch梯度下降算法，在每一次梯度下降，都会遍历训练集的样本。<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjv6bpto4j30zw0goac7.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;符号定义&quot;&gt;&lt;a href=&quot;#符号定义&quot; class=&quot;headerlink&quot; title=&quot;符号定义&quot;&gt;&lt;/a&gt;符号定义&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;m &lt;em&gt;训练集样本的数量&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;x &lt;em&gt;输入的特征值&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;y &lt;em&gt;输出的变量&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;(x,y) &lt;em&gt;一个训练样本&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;(x(i), y(i)) &lt;em&gt;第i个训练样本&lt;/em&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="单变量线性回归" scheme="http://yoursite.com/tags/%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达机器学习-绪论</title>
    <link href="http://yoursite.com/2019/01/26/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BB%AA%E8%AE%BA/"/>
    <id>http://yoursite.com/2019/01/26/吴恩达机器学习-绪论/</id>
    <published>2019-01-26T03:54:13.000Z</published>
    <updated>2019-01-26T06:22:07.187Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是机器学习？</h2><blockquote><p>在没有明确设置的情况下，使计算机具有学习能力的研究领域。<br>or 计算机程序从经验E中学习，解决某一任务T，进行某一性能度量P，通过P测定在T上的表现因经验E提高。<br><a id="more"></a></p></blockquote><h2 id="不同类型的学习算法"><a href="#不同类型的学习算法" class="headerlink" title="不同类型的学习算法"></a>不同类型的学习算法</h2><ul><li>监督学习<ul><li>我们教给计算机学习某件事</li></ul></li><li>无监督学习<ul><li>让计算机自己学习</li></ul></li><li>等</li></ul><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><blockquote><p>给算法一个数据集，其中包含了正确答案，算法的目的是给出更多的正确答案。</p><ul><li>回归问题： 连续</li><li>分类问题： 离散<br>实质上就是对数据集中的每个样本做算法预测，并得出正确答案.</li></ul></blockquote><h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><blockquote><p>对于给定数据集，算法可判定数据集包含的不同的簇。</p><ul><li>聚类算法</li><li>鸡尾酒会问题</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      什么是机器学习？不同的学习算法如何分类？监督学习和无监督学习的区别是什么?
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>代码共享</title>
    <link href="http://yoursite.com/2019/01/26/%E4%BB%A3%E7%A0%81%E5%85%B1%E4%BA%AB/"/>
    <id>http://yoursite.com/2019/01/26/代码共享/</id>
    <published>2019-01-26T03:52:43.000Z</published>
    <updated>2019-01-26T06:21:54.438Z</updated>
    
    <content type="html"><![CDATA[<h3 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h3><ul><li>主要是和研友一起讨论一些程序问题，发来发去太麻烦，而且windows和osx的编码集还不一样，总之，，诸多问题。</li><li>便萌生了写一个大家都可以在线看代码的网站。</li><li>主要功能就是预览代码。其次可以上传自己的代码</li><li><p>上传文件是一个非常危险的行为，尽管如此，我还是没有做任何过滤措施，这是非常危险的行为（时间有限）。</p><a id="more"></a><h3 id="文件结构"><a href="#文件结构" class="headerlink" title="文件结构"></a>文件结构</h3></li><li><p>cooperate //项目文件</p><ul><li>files.php //获取并返回文件内容</li><li>index.php //入口文件，，主页</li><li>postfile.php //上传文件</li><li>source // 资源文件夹<ul><li>main.css</li><li>main.js</li><li>…</li></ul></li><li>toughter //共享代码文件存储的文件夹</li></ul></li></ul><h3 id="详细代码"><a href="#详细代码" class="headerlink" title="详细代码"></a>详细代码</h3><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// files.php</span></span><br><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">openfile</span><span class="params">($dir)</span></span></span><br><span class="line"><span class="function"></span>&#123;   </span><br><span class="line">    $filepath = dirname(<span class="keyword">__FILE__</span>).<span class="string">"/toghter/"</span>.$dir;</span><br><span class="line">    $f = fopen($filepath,r)<span class="keyword">or</span> <span class="keyword">die</span>(<span class="string">"&lt;b&gt;ERROR！&lt;/b&gt;"</span>);</span><br><span class="line">    $each = <span class="string">""</span>;</span><br><span class="line">    <span class="keyword">while</span>(!feof($f)) &#123;</span><br><span class="line">        $each = $each.fgets($f) . <span class="string">"\n"</span>;  </span><br><span class="line">     &#125;</span><br><span class="line">    fclose($f);</span><br><span class="line">    <span class="keyword">return</span> $each;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">my_scandir</span><span class="params">($dir)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//定义一个数组</span></span><br><span class="line">    $files = <span class="keyword">array</span>();</span><br><span class="line">    <span class="comment">//检测是否存在文件</span></span><br><span class="line">    <span class="keyword">if</span> (is_dir($dir)) &#123;</span><br><span class="line">        <span class="comment">//打开目录</span></span><br><span class="line">        <span class="keyword">if</span> ($handle = opendir($dir)) &#123;</span><br><span class="line">            <span class="comment">//返回当前文件的条目</span></span><br><span class="line">            <span class="keyword">while</span> (($file = readdir($handle)) !== <span class="keyword">false</span>) &#123;</span><br><span class="line">                <span class="comment">//去除特殊目录</span></span><br><span class="line">                <span class="keyword">if</span> ($file != <span class="string">"."</span> &amp;&amp; $file != <span class="string">".."</span>) &#123;</span><br><span class="line">                    <span class="comment">//判断子目录是否还存在子目录</span></span><br><span class="line">                    <span class="keyword">if</span> (is_dir($dir . <span class="string">"/"</span> . $file)) &#123;</span><br><span class="line">                        <span class="comment">//递归调用本函数，再次获取目录</span></span><br><span class="line">                        $files[$file] = my_scandir($dir . <span class="string">"/"</span> . $file);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">//获取目录数组</span></span><br><span class="line">                        $files[] = $dir . <span class="string">"/"</span> . $file;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//关闭文件夹</span></span><br><span class="line">            closedir($handle);</span><br><span class="line">            <span class="comment">//返回文件夹数组</span></span><br><span class="line">            <span class="keyword">return</span> $files;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">echo</span> <span class="string">"&lt;pre&gt;"</span>;</span><br><span class="line"><span class="comment">//var_dump(my_scandir(__DIR__));</span></span><br></pre></td></tr></table></figure><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// index.php</span></span><br><span class="line"></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;meta charset=<span class="string">"utf-8"</span> /&gt;</span><br><span class="line">    &lt;meta http-equiv=<span class="string">"X-UA-Compatible"</span> content=<span class="string">"IE=edge"</span>&gt;</span><br><span class="line">    &lt;title&gt;很开心和你合作~&lt;/title&gt;</span><br><span class="line">    &lt;meta name=<span class="string">"viewport"</span> content=<span class="string">"width=device-width, initial-scale=1"</span>&gt;</span><br><span class="line">    &lt;link rel=<span class="string">"stylesheet"</span> type=<span class="string">"text/css"</span> media=<span class="string">"screen"</span> href=<span class="string">"source/prism.css"</span> /&gt;</span><br><span class="line">    &lt;link rel=<span class="string">"stylesheet"</span> type=<span class="string">"text/css"</span> media=<span class="string">"screen"</span> href=<span class="string">"source/main.css"</span> /&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line">    <span class="keyword">include_once</span> <span class="string">'files.php'</span>;</span><br><span class="line">    $filename = $_GET[<span class="string">"filename"</span>];</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">isset</span>($filename))&#123;</span><br><span class="line">        $code = openfile($filename);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        $code = <span class="string">"\t#include\n\tint main\n\t&#123;\n\t\t/* code */\n\t\tprintf(\"Hello World!\");\n\t\treturn 0;\n\t&#125;"</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">?&gt;</span></span><br><span class="line">&lt;div id = <span class="string">"main"</span>&gt;</span><br><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line">    <span class="keyword">echo</span> <span class="string">"文件列表：\n"</span>;</span><br><span class="line">    $dir = my_scandir(<span class="keyword">__DIR__</span>)[toghter];</span><br><span class="line">    $num = count($dir);</span><br><span class="line">    <span class="keyword">for</span>($i=<span class="number">0</span>;$i&lt;$num;++$i)&#123;</span><br><span class="line">        $filename = basename($dir[$i]);</span><br><span class="line">        <span class="keyword">echo</span> (<span class="string">"&lt;button class= \"fb file-button button button-glow button-border button-rounded button-primary\"&gt;"</span>);</span><br><span class="line">        <span class="keyword">echo</span>($filename);</span><br><span class="line">        <span class="keyword">echo</span>(<span class="string">"&lt;/button&gt;"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">?&gt;</span></span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&lt;a class="modalLink" href="#"&gt;添加文件&lt;/a&gt;</span><br><span class="line">&lt;div class="overlay"&gt;&lt;/div&gt;</span><br><span class="line">&lt;div class="modal"&gt;</span><br><span class="line">    &lt;a href="#" class="closeBtn"&gt;取消&lt;/a&gt;</span><br><span class="line">    方便交流使用，请勿提交恶意代码！</span><br><span class="line">&lt;form enctype=<span class="string">"multipart/form-data"</span> action=<span class="string">"postfile.php"</span> method=<span class="string">"post"</span> &gt;</span><br><span class="line">    &lt;input type=<span class="string">"file"</span> name=<span class="string">"file"</span> id=<span class="string">"file"</span>/&gt;</span><br><span class="line">    &lt;input type=<span class="string">"submit"</span> name=<span class="string">"submit"</span> value=<span class="string">"Submit"</span> /&gt;</span><br><span class="line">&lt;/form&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&lt;div id=<span class="string">"main"</span>&gt;</span><br><span class="line">&lt;pre id = "_code"&gt;&lt;code class="language-C"&gt;&lt;?php echo $code;?&gt;&lt;/code&gt;&lt;/pre&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;script src=<span class="string">"http://code.jquery.com/jquery-latest.js"</span>&gt;&lt;/script&gt;</span><br><span class="line">&lt;script src=<span class="string">"source/prism.js"</span>&gt;&lt;/script&gt;</span><br><span class="line">&lt;script src=<span class="string">"source/main.js"</span>&gt;&lt;/script&gt;</span><br><span class="line">&lt;script type=<span class="string">'text/javascript'</span> src=<span class="string">'source/jquery.modal.js'</span>&gt;&lt;/script&gt;</span><br><span class="line">&lt;script type=<span class="string">'text/javascript'</span> src=<span class="string">'source/site.js'</span>&gt;&lt;/script&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//postfile.php</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line"></span><br><span class="line">$file = $_FILES[<span class="string">"file"</span>];</span><br><span class="line">$path = dirname(<span class="keyword">__FILE__</span>).<span class="string">"/toghter/"</span>.$file[<span class="string">'name'</span>];</span><br><span class="line"><span class="keyword">if</span>(move_uploaded_file($file[<span class="string">'tmp_name'</span>],$path))&#123;</span><br><span class="line">    <span class="keyword">echo</span> <span class="string">"&lt;script&gt;alert(\"文件上传成功\")&lt;/script&gt;"</span>;</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">    $error = $_FILES[<span class="string">'myfile'</span>][<span class="string">'error'</span>];</span><br><span class="line">    <span class="keyword">echo</span> <span class="string">"&lt;script&gt;\"上传错误\""</span>.$error.<span class="string">"&lt;/script&gt;"</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">echo</span> <span class="string">"&lt;script&gt;history.go(-1);&lt;/script&gt;"</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  $myfile = "/private/var/tmp/";</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  if (is_readable ($myfile)) &#123;</span></span><br><span class="line"><span class="comment">    echo "此文件可读。", "\n";</span></span><br><span class="line"><span class="comment">  &#125; else &#123;</span></span><br><span class="line"><span class="comment">    echo "此文件不可读.", "\n";</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  if (is_writable ($myfile)) &#123;</span></span><br><span class="line"><span class="comment">    echo "此文件可写。", "\n";</span></span><br><span class="line"><span class="comment">  &#125; else &#123;</span></span><br><span class="line"><span class="comment">    echo "此文件不可写。", "\n";</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  if (is_executable ($myfile)) &#123;</span></span><br><span class="line"><span class="comment">    echo "此文件可执行!", "\n";</span></span><br><span class="line"><span class="comment">  &#125; else &#123;</span></span><br><span class="line"><span class="comment">    echo "此文件没有可执行权限。", "\n";</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment">?&gt;</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><h5 id="篇幅原因-不再赘述js等文件"><a href="#篇幅原因-不再赘述js等文件" class="headerlink" title="篇幅原因 不再赘述js等文件"></a>篇幅原因 不再赘述js等文件</h5>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;起因&quot;&gt;&lt;a href=&quot;#起因&quot; class=&quot;headerlink&quot; title=&quot;起因&quot;&gt;&lt;/a&gt;起因&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;主要是和研友一起讨论一些程序问题，发来发去太麻烦，而且windows和osx的编码集还不一样，总之，，诸多问题。&lt;/li&gt;
&lt;li&gt;便萌生了写一个大家都可以在线看代码的网站。&lt;/li&gt;
&lt;li&gt;主要功能就是预览代码。其次可以上传自己的代码&lt;/li&gt;
&lt;li&gt;&lt;p&gt;上传文件是一个非常危险的行为，尽管如此，我还是没有做任何过滤措施，这是非常危险的行为（时间有限）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="PHP" scheme="http://yoursite.com/tags/PHP/"/>
    
  </entry>
  
  <entry>
    <title>scripts - 用python格式化json</title>
    <link href="http://yoursite.com/2019/01/26/scripts-%E7%94%A8python%E6%A0%BC%E5%BC%8F%E5%8C%96json/"/>
    <id>http://yoursite.com/2019/01/26/scripts-用python格式化json/</id>
    <published>2019-01-26T03:51:33.000Z</published>
    <updated>2019-01-26T06:21:48.246Z</updated>
    
    <content type="html"><![CDATA[<h4 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h4><ul><li>从终端接收参数</li><li>格式化json</li><li>输出格式化好的json到目标文件里<a id="more"></a><h4 id="完整代码："><a href="#完整代码：" class="headerlink" title="完整代码："></a>完整代码：</h4></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> getopt</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式化json</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strtosjson</span><span class="params">(proformatfile)</span>:</span></span><br><span class="line">    js = json.dumps(proformatfile, sort_keys=<span class="keyword">True</span>,indent=<span class="number">4</span>,separators=(<span class="string">','</span>, <span class="string">':'</span>))</span><br><span class="line">    <span class="keyword">return</span> js</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出用法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">usage</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"Usage: formatjson [-h | --help] | [-i|--ifile] args [-o|--ofile] args"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv)</span>:</span></span><br><span class="line">    inputfile=<span class="string">""</span></span><br><span class="line">    outputfile=<span class="string">""</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        opts,arg = getopt.getopt(argv,<span class="string">"hi:o:"</span>,[<span class="string">"ifile="</span>,<span class="string">"ofile="</span>])</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        getopt.getopt()方法说明：</span></span><br><span class="line"><span class="string">            args:要解析的命令行参数列表</span></span><br><span class="line"><span class="string">            oprtions:字符串的格式 带’:‘表示此选项后必须有附加参数</span></span><br><span class="line"><span class="string">            long_options:列表格式 带'='表示此选项后必须有附加参数</span></span><br><span class="line"><span class="string">            返回值:</span></span><br><span class="line"><span class="string">                - (option,value)</span></span><br><span class="line"><span class="string">                -  参数列表</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">    <span class="keyword">except</span> getopt.GetoptError:</span><br><span class="line">        usage()</span><br><span class="line">        sys.exit(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> opt, arg <span class="keyword">in</span> opts:</span><br><span class="line">        <span class="keyword">if</span> opt == <span class="string">'-h'</span>:</span><br><span class="line">            usage()</span><br><span class="line">            sys.exit()</span><br><span class="line">        <span class="keyword">elif</span> opt <span class="keyword">in</span> (<span class="string">"-i"</span>, <span class="string">"--ifile"</span>):</span><br><span class="line">            inputfile = arg</span><br><span class="line">        <span class="keyword">elif</span> opt <span class="keyword">in</span> (<span class="string">"-o"</span>, <span class="string">"--ofile"</span>):</span><br><span class="line">            outputfile = arg</span><br><span class="line">    infilepath = inputfile</span><br><span class="line">    outfilepath = outputfile</span><br><span class="line">    format(infilepath,outfilepath)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读写文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">format</span><span class="params">(infilepath,outfilepath)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> open(infilepath,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            proformatfile = f.read()</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        print(<span class="string">"No Such file or directory!"</span>)</span><br><span class="line">        usage()</span><br><span class="line">        sys.exit(<span class="number">2</span>)</span><br><span class="line">    js = strtosjson(eval(proformatfile))</span><br><span class="line">    <span class="comment"># eval() 将string转换为dictionary</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> open(outfilepath,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(js)</span><br><span class="line">            print(<span class="string">"json fromat success!"</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        print(<span class="string">"OutPutFileWriteError!"</span>)</span><br><span class="line">        sys.exit(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">   main(sys.argv[<span class="number">1</span>:]) <span class="comment">#除去本身的文件名</span></span><br></pre></td></tr></table></figure><h4 id="测试："><a href="#测试：" class="headerlink" title="测试："></a>测试：</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> python formatjson.py -i /Users/whoami/Desktop/test.txt -o /Users/whoami/Desktop/t.txt</span><br></pre></td></tr></table></figure><p><code>$ json fromat success!</code></p><h4 id="后续改进"><a href="#后续改进" class="headerlink" title="后续改进"></a>后续改进</h4><ul><li>输入输出文件过于繁琐 可以加上默认值</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;思路：&quot;&gt;&lt;a href=&quot;#思路：&quot; class=&quot;headerlink&quot; title=&quot;思路：&quot;&gt;&lt;/a&gt;思路：&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;从终端接收参数&lt;/li&gt;
&lt;li&gt;格式化json&lt;/li&gt;
&lt;li&gt;输出格式化好的json到目标文件里
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="自用脚本" scheme="http://yoursite.com/tags/%E8%87%AA%E7%94%A8%E8%84%9A%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>搭建git服务器</title>
    <link href="http://yoursite.com/2019/01/26/%E6%90%AD%E5%BB%BAgit%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>http://yoursite.com/2019/01/26/搭建git服务器/</id>
    <published>2019-01-26T03:50:09.000Z</published>
    <updated>2019-01-26T06:22:15.562Z</updated>
    
    <content type="html"><![CDATA[<h3 id="安装依赖库和编译工具"><a href="#安装依赖库和编译工具" class="headerlink" title="安装依赖库和编译工具"></a>安装依赖库和编译工具</h3><a id="more"></a><h4 id="为了后续安装能正常进行，我们先来安装一些相关依赖库和编译工具"><a href="#为了后续安装能正常进行，我们先来安装一些相关依赖库和编译工具" class="headerlink" title="为了后续安装能正常进行，我们先来安装一些相关依赖库和编译工具"></a>为了后续安装能正常进行，我们先来安装一些相关依赖库和编译工具</h4><p><code>yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel</code></p><h4 id="安装编译工具"><a href="#安装编译工具" class="headerlink" title="安装编译工具"></a>安装编译工具</h4><p><code>yum install gcc perl-ExtUtils-MakeMaker</code></p><p>###下载 git</p><h4 id="选一个目录，用来放下载下来的安装包，这里将安装包放在-usr-local-src-目录里"><a href="#选一个目录，用来放下载下来的安装包，这里将安装包放在-usr-local-src-目录里" class="headerlink" title="选一个目录，用来放下载下来的安装包，这里将安装包放在 /usr/local/src 目录里"></a>选一个目录，用来放下载下来的安装包，这里将安装包放在 <code>/usr/local/src</code> 目录里</h4><p><code>cd /usr/local/src</code></p><h4 id="到官网找一个新版稳定的源码包下载到-usr-local-src文件夹里"><a href="#到官网找一个新版稳定的源码包下载到-usr-local-src文件夹里" class="headerlink" title="到官网找一个新版稳定的源码包下载到 /usr/local/src文件夹里"></a>到官网找一个新版稳定的源码包下载到 <code>/usr/local/src</code>文件夹里</h4><p><code>wget https://www.kernel.org/pub/software/scm/git/git-2.10.0.tar.gz</code></p><h3 id="解压和编译"><a href="#解压和编译" class="headerlink" title="解压和编译"></a>解压和编译</h3><h4 id="解压下载的源码包"><a href="#解压下载的源码包" class="headerlink" title="解压下载的源码包"></a>解压下载的源码包</h4><p><code>tar -zvxf git-2.10.0.tar.gz</code></p><h4 id="解压后进入-git-2-10-0-文件夹"><a href="#解压后进入-git-2-10-0-文件夹" class="headerlink" title="解压后进入 git-2.10.0 文件夹"></a>解压后进入 <code>git-2.10.0</code> 文件夹</h4><p><code>cd git-2.10.0</code></p><h4 id="执行编译"><a href="#执行编译" class="headerlink" title="执行编译"></a>执行编译</h4><p><code>make all prefix=/usr/local/git</code></p><h4 id="编译完成后-安装到-usr-local-git-目录下"><a href="#编译完成后-安装到-usr-local-git-目录下" class="headerlink" title="编译完成后, 安装到 /usr/local/git 目录下"></a>编译完成后, 安装到 <code>/usr/local/git</code> 目录下</h4><p><code>make install prefix=/usr/local/git</code></p><h3 id="将-git-目录加入-PATH"><a href="#将-git-目录加入-PATH" class="headerlink" title="将 git 目录加入 PATH"></a>将 git 目录加入 PATH</h3><h4 id="将原来的-PATH-指向目录修改为现在的目录"><a href="#将原来的-PATH-指向目录修改为现在的目录" class="headerlink" title="将原来的 PATH 指向目录修改为现在的目录"></a>将原来的 PATH 指向目录修改为现在的目录</h4><p><code>echo &#39;export PATH=$PATH:/usr/local/git/bin&#39; &gt;&gt; /etc/bashrc</code></p><h4 id="生效环境变量"><a href="#生效环境变量" class="headerlink" title="生效环境变量"></a>生效环境变量</h4><p><code>source /etc/bashrc</code></p><h4 id="此时我们能查看-git-版本号，说明我们已经安装成功了。"><a href="#此时我们能查看-git-版本号，说明我们已经安装成功了。" class="headerlink" title="此时我们能查看 git 版本号，说明我们已经安装成功了。"></a>此时我们能查看 git 版本号，说明我们已经安装成功了。</h4><p><code>git --version</code></p><h3 id="创建-git-账号"><a href="#创建-git-账号" class="headerlink" title="创建 git 账号"></a>创建 git 账号</h3><h4 id="为我们刚刚搭建好的-git-创建一个账号"><a href="#为我们刚刚搭建好的-git-创建一个账号" class="headerlink" title="为我们刚刚搭建好的 git 创建一个账号"></a>为我们刚刚搭建好的 git 创建一个账号</h4><p><code>useradd -m studnet</code></p><h4 id="然后为这个账号设置密码"><a href="#然后为这个账号设置密码" class="headerlink" title="然后为这个账号设置密码"></a>然后为这个账号设置密码</h4><p><code>passwd student</code></p><h4 id="输入两次密码"><a href="#输入两次密码" class="headerlink" title="输入两次密码"></a>输入两次密码</h4><h3 id="初始化-git-仓库并配置用户权限"><a href="#初始化-git-仓库并配置用户权限" class="headerlink" title="初始化 git 仓库并配置用户权限"></a>初始化 git 仓库并配置用户权限</h3><h4 id="创建-git-仓库并初始化"><a href="#创建-git-仓库并初始化" class="headerlink" title="创建 git 仓库并初始化"></a>创建 git 仓库并初始化</h4><p>我们创建 <code>/data/repositories</code> 目录用于存放 git 仓库</p><p><code>mkdir -p /data/repositories</code></p><h4 id="创建好后，初始化这个仓库"><a href="#创建好后，初始化这个仓库" class="headerlink" title="创建好后，初始化这个仓库"></a>创建好后，初始化这个仓库</h4><p><code>cd /data/repositories/ &amp;&amp; git init --bare test.git</code></p><h3 id="配置用户权限"><a href="#配置用户权限" class="headerlink" title="配置用户权限"></a>配置用户权限</h3><h4 id="给-git-仓库目录设置用户和用户组并设置权限"><a href="#给-git-仓库目录设置用户和用户组并设置权限" class="headerlink" title="给 git 仓库目录设置用户和用户组并设置权限"></a>给 git 仓库目录设置用户和用户组并设置权限</h4><ul><li><p><code>chown -R gituser:gituser /data/repositories</code></p></li><li><p><code>chmod 755 /data/repositories</code></p></li><li><p>查找 <code>git-shell</code> 所在目录<br>, 编辑 <code>/etc/passwd</code> 文件，将最后一行关于 <code>student</code> 的登录 shell 配置改为 <code>git-shell</code> 的目录<br>如下</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">student r:x:500:500::/home/gituser:/usr/local/git/bin/git-shell</span><br></pre></td></tr></table></figure><h3 id="使用搭建好的-Git-服务"><a href="#使用搭建好的-Git-服务" class="headerlink" title="使用搭建好的 Git 服务"></a>使用搭建好的 Git 服务</h3><h4 id="克隆-test-repo-到本地"><a href="#克隆-test-repo-到本地" class="headerlink" title="克隆 test repo 到本地"></a>克隆 test repo 到本地</h4><p><code>cd ~ &amp;&amp; git clone student@127.0.0.1:/data/repositories/test.git</code></p>]]></content>
    
    <summary type="html">
    
      git是一个分散式版本控制软件，最初由林纳斯·托瓦兹創作，於2005年以GPL釋出。最初目的是为更好地管理Linux内核开发而设计。
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="Ubuntu" scheme="http://yoursite.com/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>搭建wordpress个人博客</title>
    <link href="http://yoursite.com/2019/01/26/%E6%90%AD%E5%BB%BAwordpress%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://yoursite.com/2019/01/26/搭建wordpress个人博客/</id>
    <published>2019-01-26T03:48:18.000Z</published>
    <updated>2019-01-26T06:22:34.811Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境：Ubuntu-16-04-1-LTS-64-位"><a href="#环境：Ubuntu-16-04-1-LTS-64-位" class="headerlink" title="环境：Ubuntu 16.04.1 LTS 64 位"></a>环境：Ubuntu 16.04.1 LTS 64 位</h2><h2 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h2><a id="more"></a><h3 id="1-准备-LAMP-环境"><a href="#1-准备-LAMP-环境" class="headerlink" title="1.准备 LAMP 环境"></a>1.准备 LAMP 环境</h3><p>使用 <code>apt-get</code> 安装 Apache2<br><code>sudo apt-get install apache2 -y</code></p><p>安装 PHP 组件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install php7.0 -y</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libapache2-mod-php7.0</span><br></pre></td></tr></table></figure><p>安装 MySQL 服务</p><p>安装 MySQL 过程中，控制台会提示您输入 MySQL 的密码，需要输入两次密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install mysql-server -y</span><br></pre></td></tr></table></figure><p>安装 php MySQL相关组件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install php7.0-mysql</span><br></pre></td></tr></table></figure><p>安装 phpmyadmin:</p><p>使用 <code>apt-get</code> 安装 phpmyadmin，安装过程中，需要根据提示选择 apache2 ，再输入root密码 和数据库密码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install phpmyadmin -y</span><br></pre></td></tr></table></figure><p>建立 <code>/var/www/html</code> 下的软连接:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -s /usr/share/phpmyadmin /var/www/html/phpmyadmin</span><br></pre></td></tr></table></figure><p>重启 MySQL 服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysql restart</span><br></pre></td></tr></table></figure><p>重启 Apache 服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart apache2.service</span><br></pre></td></tr></table></figure><h3 id="安装并配置-WordPress"><a href="#安装并配置-WordPress" class="headerlink" title="安装并配置 WordPress"></a>安装并配置 WordPress</h3><h3 id="安装-WordPress"><a href="#安装-WordPress" class="headerlink" title="安装 WordPress"></a>安装 WordPress</h3><p>需要下载一个 WordPress 压缩包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://cn.wordpress.org/wordpress-4.7.4-zh_CN.zip</span><br></pre></td></tr></table></figure><p>下载完成后，解压这个压缩包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo unzip wordpress-4.7.4-zh\_CN.zip</span><br></pre></td></tr></table></figure><h3 id="为-wordpress-配置一个数据库"><a href="#为-wordpress-配置一个数据库" class="headerlink" title="为 wordpress 配置一个数据库"></a>为 wordpress 配置一个数据库</h3><p>进入 mysql，输入以下代码后，按提示输入您MySQL密码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br></pre></td></tr></table></figure><p>为 wordpress 创建一个叫 wordpress 的数据库：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE wordpress;</span><br></pre></td></tr></table></figure><p>为 这个数据库设置一个用户为 wordpressuser：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE USER wordpressuser;</span><br></pre></td></tr></table></figure><p>为这个用户配置一个密码为 password123：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET PASSWORD FOR wordpressuser= PASSWORD(&quot;password123&quot;);</span><br></pre></td></tr></table></figure><p>为这个用户配置数据库的访问权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GRANT ALL PRIVILEGES ON wordpress.* TO wordpressuser IDENTIFIED BY&quot;password123&quot;;</span><br></pre></td></tr></table></figure><p>生效这些配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><p>然后退出 mysql</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exit;</span><br></pre></td></tr></table></figure><h3 id="配置-wordpress"><a href="#配置-wordpress" class="headerlink" title="配置 wordpress"></a>配置 wordpress</h3><p>由于PHP默认访问 <em>/var/www/html/</em> 文件夹，所以需要把 wordpress 文件夹里的文件都复制到 <em>/var/www/html/</em> 文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mv wordpress/* /var/www/html/</span><br></pre></td></tr></table></figure><p>修改一下 /var/www/html/ 目录权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod -R 777 /var/www/html/</span><br></pre></td></tr></table></figure><p>将apache指定到index.html</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mv /var/www/html/index.html /var/www/html/index~.html</span><br></pre></td></tr></table></figure><p>重启 Apache 服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart apache2.service</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      WordPress是一款能让您建立出色网站、博客或应用的开源软件。 美观设计、强大功能与自由建立任何您所想的
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="Ubuntu" scheme="http://yoursite.com/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>盗号所启发的</title>
    <link href="http://yoursite.com/2019/01/26/%E7%9B%97%E5%8F%B7%E6%89%80%E5%90%AF%E5%8F%91%E7%9A%84/"/>
    <id>http://yoursite.com/2019/01/26/盗号所启发的/</id>
    <published>2019-01-26T03:38:24.000Z</published>
    <updated>2019-01-26T06:22:42.782Z</updated>
    
    <content type="html"><![CDATA[<p>起因：昨天晚上朋友发了一个消息给我：<br><a id="more"></a><br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzju3ybunjj30fd03qwfd.jpg" alt=""></p><p>晚上喝了点酒，没怎么看就睡了。<br>第二天一大早上想起这个问题，心血来潮想扒一扒这个网站</p><p>首先看这个网址：<br><a href="http://maschada.wbac.ac.th/ui.html?index/=ciquitj?url=s9fwpc&amp;iv" target="_blank" rel="noopener">http://maschada.wbac.ac.th/ui.html?index/=ciquitj?url=s9fwpc&amp;iv</a></p><p>打开是一个QQ安全中心的登陆界面：</p><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzju49p29kj30fd0bgt9m.jpg" alt=""></p><p>常识：QQ安全中心的网址统一都是 <a href="https://aq.qq.com/" target="_blank" rel="noopener">https://aq.qq.com/</a><br>再加上这么奇怪的网站，第一反应就是盗号网址。chrome查看源代码：一眼看去有几个地方比较可疑：</p><h3 id="1-form的提交"><a href="#1-form的提交" class="headerlink" title="1. form的提交"></a>1. form的提交</h3><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzju58eqgaj30fd028t9a.jpg" alt=""></p><h3 id="2-一段被url编码的js"><a href="#2-一段被url编码的js" class="headerlink" title="2. 一段被url编码的js"></a>2. 一段被url编码的js</h3><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzju5wcgdpj30fd01nwf3.jpg" alt=""></p><h3 id="3-奇怪的js引用"><a href="#3-奇怪的js引用" class="headerlink" title="3. 奇怪的js引用"></a>3. 奇怪的js引用</h3><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzju6o15tvj309j00vjrc.jpg" alt=""></p><pre><code>首先搞懂这个url编码的js：</code></pre><p><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzju7igpmhj30fd047js5.jpg" alt=""><br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzju7s55fcj30fd022aa9.jpg" alt=""><br>这里只是控制了输入框的默认值，问题不大。<br>再来看js引用部分：<br>一个很老套的套路：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=&quot;jj.php&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>这里的jj.php并不是php文件，用script引用，所以jj.php是一个js文件！<br>打开：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzju959pkmj30fd07swjh.jpg" alt=""><br>格式整理一下。。。<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzju9h44axj30fd0a5afj.jpg" alt=""><br>前半部分是简单的逻辑，主要判断输入的QQ号和密码是否符合标准。</p><p>关键代码<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzju9prr7dj30fd05n0uo.jpg" alt=""><br>这里就是很明显的套路了：<br>你输入正确的QQ账户和密码后就会跳转到真正的QQ安全中心（这一步如果是输入错误也可能直接登录到QQ安全中心，因为一般QQ或者浏览器都会自动登录，不过这个没关系，不会有安全问题，也算是一个障眼法。）<br>这里注意到一个注释，和和上面的post请求差不多，但是用一个p();<br>所以来看看p()代码：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjuabau58j30fd02b0tb.jpg" alt=""><br>s的值用了ecode64()也就是用了base64编码：<br>QQ号 + t + 密码 + t<br>如果我输入了QQ和密码为 ： 88488848t shabisibabat<br>base64 编码：<br>ODg0ODg4NDhcdCBzaGFiaXNpYmFiYVx0==<br><a href="http://maschada.wbac.ac.th//tpl/36402021580636317/ODg0ODg4NDhcdCBzaGFiaXNpYmFiYVx0==.gif_1474950810000" target="_blank" rel="noopener">http://maschada.wbac.ac.th//tpl/36402021580636317/ODg0ODg4NDhcdCBzaGFiaXNpYmFiYVx0==.gif_1474950810000</a></p><p>到此，分析结束。<br>其他信息：<br><img src="http://ww1.sinaimg.cn/large/006Uvlfaly1fzjuak04lcj30fd07mab8.jpg" alt=""><br>总结：</p><p>小白一只，没有社工库，到这里就全部完了，只是简单分析了一下盗号的一般套路，希望各位保护好自己的账号。<br>网络安全常识和意识不只是搞网络安全的才需要具有的素质。<br>现在这种网络环境，这些隐私信息稍不注意就会被人挖走，好好提高自己的安全意识，尽量不做internet裸奔者。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;起因：昨天晚上朋友发了一个消息给我：&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python-爬虫入门</title>
    <link href="http://yoursite.com/2019/01/26/Python-%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"/>
    <id>http://yoursite.com/2019/01/26/Python-爬虫入门/</id>
    <published>2019-01-26T03:35:28.000Z</published>
    <updated>2019-01-26T06:21:34.156Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="keyword">as</span> bs</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment">#配置</span></span><br><span class="line">headers = &#123;</span><br><span class="line">            <span class="string">'content-type'</span>: <span class="string">'application/json'</span>,</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (X11; Ubuntu; Linux x86\_64; rv:22.0) Gecko/20100101 Firefox/22.0'</span></span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://www.newsweek.com/world"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_req</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="comment">#连接网页 如果失败 等待三秒继续连接</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            re = requests.get(url,headers = headers)</span><br><span class="line">            print(<span class="string">'爬虫准备就绪...'</span>)</span><br><span class="line">            print(re.status_code)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.ConnectionError:</span><br><span class="line">            print(<span class="string">'连接错误 请等待。。。'</span>)</span><br><span class="line">            time.sleep(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.ChunkedEncodingError:</span><br><span class="line">            print(<span class="string">'编码错误 请等待。。。'</span>)</span><br><span class="line">            time.sleep(<span class="number">3</span>)    </span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            print(<span class="string">'未知错误 请等待。。。'</span>)</span><br><span class="line">            time.sleep(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rex</span><span class="params">(re)</span>:</span></span><br><span class="line">    <span class="comment">#处理网页内容</span></span><br><span class="line">    html_text = re.text</span><br><span class="line">    soup = bs(html_text,<span class="string">"lxml"</span>)</span><br><span class="line">    article_list = []</span><br><span class="line">    article_url_list = []</span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> soup.find_all(<span class="string">'a'</span>):</span><br><span class="line">        u = each.get(<span class="string">"href"</span>)</span><br><span class="line">        article_list.append(u)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取真正的文章链接</span></span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> article_list:</span><br><span class="line">        <span class="keyword">if</span>(str(each).count(<span class="string">"-"</span>,<span class="number">0</span>,len(str(each))<span class="number">-1</span>) &gt; <span class="number">3</span>):</span><br><span class="line">            article_url_list.append(str(each))</span><br><span class="line">    <span class="comment">#去掉重复链接并拼接成完整的URL</span></span><br><span class="line">    article_url_list = list(set(article_url_list))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(article_url_list)):</span><br><span class="line">        article_url_list[i] = <span class="string">"http://www.newsweek.com"</span> + article_url_list[i]</span><br><span class="line">    <span class="keyword">return</span> article_url_list</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article</span><span class="params">(x_list)</span>:</span></span><br><span class="line">    all_article = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(x_list)):</span><br><span class="line">        _list = []</span><br><span class="line">        print(<span class="string">"正在处理:"</span>)</span><br><span class="line">        print(<span class="string">"----------------"</span>+x_list[i]+<span class="string">"--------------"</span>)</span><br><span class="line">        re = get_req(x_list[i])</span><br><span class="line">        soup = bs(re.text,<span class="string">"lxml"</span>)</span><br><span class="line">        tags = soup.find(name = <span class="string">'div'</span>,attrs = &#123;<span class="string">"class"</span>:<span class="string">"article-body"</span>&#125;).find_all(<span class="string">"p"</span>)</span><br><span class="line">        \<span class="comment">#处理标签 保留文章</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tags:</span><br><span class="line">            [s.extract() <span class="keyword">for</span> s <span class="keyword">in</span> i(<span class="string">"a"</span>)]</span><br><span class="line">            [s.extract() <span class="keyword">for</span> s <span class="keyword">in</span> i(<span class="string">"picture"</span>)]</span><br><span class="line">            [s.extract() <span class="keyword">for</span> s <span class="keyword">in</span> i(<span class="string">"iframe"</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tags:</span><br><span class="line">            all_article.append(i.get_text())</span><br><span class="line">    <span class="keyword">return</span> all_article</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    article = get_article(rex(get_req(url)))</span><br><span class="line">    article_str = <span class="string">''</span>.join(article)</span><br><span class="line">    f = open(<span class="string">'articles.txt'</span>,<span class="string">'w+'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">    f.write(article_str)</span><br><span class="line">    f.close()</span><br><span class="line">    print(<span class="string">"----OK----"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">f = open(<span class="string">'articles.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">txt = f.read()</span><br><span class="line">_list = txt.split()</span><br><span class="line">_dir = Counter(_list)</span><br><span class="line">_json = json.dumps(_dir)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      代码分为两部分,爬虫部分和写入文件部分, 都是一些比较基本的操作，主要是requests库，beautifulsoup库，urllib库的使用。
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python-微信好友分析</title>
    <link href="http://yoursite.com/2019/01/26/Python-%E5%BE%AE%E4%BF%A1%E5%A5%BD%E5%8F%8B%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2019/01/26/Python-微信好友分析/</id>
    <published>2019-01-26T03:33:49.000Z</published>
    <updated>2019-01-26T06:21:31.240Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>对微信好友进行了一些简单的统计和分析<br><a id="more"></a><br>在使用pip安装wordcloud包时会报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">error: Microsoft Visual C++ 14.0 is required. Get it with &quot;Microsoft Visual C++ Build Tools&quot;: http://landinghub.visualstudio.com/visual-cpp-build-tools</span><br></pre></td></tr></table></figure></p></blockquote><p>解决：</p><p>打开<a href="http://landinghub.visualstudio.com/visual-cpp-build-tools" target="_blank" rel="noopener">http://landinghub.visualstudio.com/visual-cpp-build-tools</a>，下载并安装Visual C++ 2015 Build Tools。</p><p>再次使用pip安装即可：<br><code>pip install wordcloud</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itchat</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span>  pd</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> Pie,Map,Style,Page,Bar</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> wordcloud</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">\<span class="comment">#按照key获得相关的list</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get</span>\<span class="title">_attr</span><span class="params">(friends,key)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> list(map(<span class="keyword">lambda</span> user:user.get(key),friends))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get</span>\<span class="title">_friends</span><span class="params">()</span>:</span></span><br><span class="line">    itchat.auto_login(hotReload=<span class="keyword">True</span>)</span><br><span class="line">    friends = itchat.get_friends()</span><br><span class="line">    users = dict(province=get_attr(friends, <span class="string">"Province"</span>),</span><br><span class="line">                   city=get_attr(friends, <span class="string">"City"</span>),</span><br><span class="line">                   nickname=get_attr(friends, <span class="string">"NickName"</span>),</span><br><span class="line">                   sex=get_attr(friends, <span class="string">"Sex"</span>),</span><br><span class="line">                   signature=get_attr(friends, <span class="string">"Signature"</span>),</span><br><span class="line">                   remarkname=get_attr(friends, <span class="string">"RemarkName"</span>),</span><br><span class="line">                   pyquanpin=get_attr(friends, <span class="string">"PYQuanPin"</span>),</span><br><span class="line">                   displayname=get_attr(friends, <span class="string">"DisplayName"</span>),</span><br><span class="line">                   isowner=get_attr(friends, <span class="string">"IsOwner"</span>))</span><br><span class="line">    <span class="keyword">return</span> users</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sex</span>\<span class="title">_stats</span><span class="params">(users)</span>:</span></span><br><span class="line">    df = pd.DataFrame(users)</span><br><span class="line">    sex_arr = df.groupby([<span class="string">'sex'</span>], as_index=<span class="keyword">True</span>)[<span class="string">'sex'</span>].count()</span><br><span class="line">    data = dict(zip(list(sex_arr.index),list(sex_arr)))</span><br><span class="line">    data[<span class="string">'可能没有性别吧'</span>] = data.pop(<span class="number">0</span>)</span><br><span class="line">    data[<span class="string">'男'</span>] = data.pop(<span class="number">1</span>)</span><br><span class="line">    data[<span class="string">'女'</span>] = data.pop(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> data.keys(),data.values()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create</span>\<span class="title">_sex</span><span class="params">()</span>:</span></span><br><span class="line">    users = get_friends()</span><br><span class="line">    page = Page()</span><br><span class="line">    \<span class="comment">#style = Style(width = 1100,height = 600)</span></span><br><span class="line">    \<span class="comment">#style\_middle = Style(width = 900,height = 500)</span></span><br><span class="line">    data = sex_stats(users)</span><br><span class="line">    attr,value = data</span><br><span class="line">    chart = Pie(<span class="string">'微信性别'</span>)</span><br><span class="line">    chart.add(<span class="string">''</span>, attr, value, center=[<span class="number">50</span>, <span class="number">50</span>],</span><br><span class="line">                radius=[<span class="number">30</span>, <span class="number">70</span>], is_label_show=<span class="keyword">True</span>, legend_orient=<span class="string">'horizontal'</span>, legend_pos=<span class="string">'center'</span>,</span><br><span class="line">                legend_top=<span class="string">'bottom'</span>, is_area_show=<span class="keyword">True</span>)</span><br><span class="line">    page.add(chart)</span><br><span class="line">    page.render()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prov</span>\<span class="title">_stats</span><span class="params">(users)</span>:</span></span><br><span class="line">    prv = pd.DataFrame(users)</span><br><span class="line">    prv_cnt = prv.groupby(<span class="string">'province'</span>, as_index=<span class="keyword">True</span>)[<span class="string">'province'</span>].count().sort_values()</span><br><span class="line">    attr = list(map(<span class="keyword">lambda</span> x: x <span class="keyword">if</span> x != <span class="string">''</span> <span class="keyword">else</span> <span class="string">'未知'</span>, list(prv_cnt.index)))</span><br><span class="line">    <span class="keyword">return</span> attr, list(prv_cnt)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create</span>\<span class="title">_city</span><span class="params">()</span>:</span></span><br><span class="line">    users = get_friends()</span><br><span class="line">    data = prov_stats(users)</span><br><span class="line">    attr, value = data</span><br><span class="line">    page = Page()</span><br><span class="line">    style = Style(width = <span class="number">1100</span>,height = <span class="number">600</span>)</span><br><span class="line">    style_middle = Style(width = <span class="number">900</span>,height = <span class="number">500</span>)</span><br><span class="line">    chart = Map(<span class="string">'中国地图'</span>, **style.init_style)</span><br><span class="line">    chart.add(<span class="string">''</span>, attr, value, is_label_show=<span class="keyword">True</span>, is_visualmap=<span class="keyword">True</span>, visual_text_color=<span class="string">'\#000'</span>)</span><br><span class="line">    page.add(chart)</span><br><span class="line"></span><br><span class="line">    chart = Bar(<span class="string">'柱状图'</span>, **style_middle.init_style)</span><br><span class="line">    chart.add(<span class="string">''</span>, attr, value, is_stack=<span class="keyword">True</span>, is_convert=<span class="keyword">True</span>, label_pos=<span class="string">'inside'</span>, is_legend_show=<span class="keyword">True</span>,</span><br><span class="line">            is_label_show=<span class="keyword">True</span>)</span><br><span class="line">    page.add(chart)</span><br><span class="line">    page.render()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">jieba</span>\<span class="title">_cut</span><span class="params">(users)</span>:</span></span><br><span class="line">    signature = users[<span class="string">'signature'</span>]</span><br><span class="line">    words = <span class="string">''</span>.join(signature)</span><br><span class="line">    res_list = jieba.cut(words,cut_all = <span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> res_list</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create</span>\<span class="title">_wc</span><span class="params">(words\_list)</span>:</span></span><br><span class="line">    res_path = os.path.abspath(<span class="string">'./resource'</span>)</span><br><span class="line">    words = <span class="string">' '</span>.join(word_list)</span><br><span class="line"></span><br><span class="line">    back_pic = numpy.array(Image.open(<span class="string">"%s/wc.png"</span> % res_path))</span><br><span class="line"></span><br><span class="line">    stopwords = set(SETWORDS)</span><br><span class="line">    stopwords = stopwords.union(set([<span class="string">'class'</span>,<span class="string">'span'</span>,<span class="string">'emoji'</span>,<span class="string">'emoji'</span>,<span class="string">'emoji1f388'</span>,<span class="string">'emoji1f604'</span>]))</span><br><span class="line"></span><br><span class="line">    wc = WordCloud(background_color=<span class="string">"white"</span>, margin=<span class="number">0</span>,</span><br><span class="line">                     font_path=<span class="string">'%s/hanyiqihei.ttf'</span> % res_path,</span><br><span class="line">                     mask=back_pic,</span><br><span class="line">                     max_font_size=<span class="number">70</span>,</span><br><span class="line">                     stopwords=stopwords</span><br><span class="line">                     ).generate(words)</span><br><span class="line"></span><br><span class="line">    plt.imshow(wc)</span><br><span class="line"></span><br><span class="line">    plt.imshow(wc.recolor(color_func = image_colors))</span><br><span class="line"></span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    \<span class="comment">#create\_sex()</span></span><br><span class="line">    \<span class="comment">#create\_city()</span></span><br><span class="line">    create_wc()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'\_\_main\_\_'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;对微信好友进行了一些简单的统计和分析&lt;br&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python-鸡汤</title>
    <link href="http://yoursite.com/2019/01/26/Python-%E9%B8%A1%E6%B1%A4/"/>
    <id>http://yoursite.com/2019/01/26/Python-鸡汤/</id>
    <published>2019-01-26T03:32:02.000Z</published>
    <updated>2019-01-26T06:21:38.612Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>调用接口实现每天发送鸡汤和翻译<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals</span><br><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Timer</span><br><span class="line"><span class="keyword">from</span> wxpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> wechat_sender <span class="keyword">import</span> Sender</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">bot = Bot()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get</span>\<span class="title">_news1</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">"http://open.iciba.com/dsapi/"</span></span><br><span class="line">    r = requests.get(url)</span><br><span class="line">    contents = r.json()[<span class="string">'content'</span>]</span><br><span class="line">    translation = r.json()[<span class="string">'translation'</span>]</span><br><span class="line">    <span class="keyword">return</span> contents,translation</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send</span>\<span class="title">_news</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        my_friend = bot.friends().search(<span class="string">u'whoami'</span>)[<span class="number">0</span>] \<span class="comment">#发送对象的微信名</span></span><br><span class="line">        my_friend.send(get_news1()[<span class="number">0</span>])</span><br><span class="line">        my_friend.send(get_news1()[<span class="number">1</span>][<span class="number">5</span>:])</span><br><span class="line">        my_friend.send(<span class="string">u'\_\_send\_By\_Wpk'</span>)</span><br><span class="line">        t = Timer(<span class="number">86400</span>,send_news)</span><br><span class="line">        t.start()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        my_friend = bot.friends().search(<span class="string">'whoami'</span>)[<span class="number">0</span>] \<span class="comment">#自己的微信名</span></span><br><span class="line">        my_friend.send(<span class="string">u"今天的消息发送失败了"</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"\_\_main\_\_"</span>:</span><br><span class="line">    send_news()</span><br></pre></td></tr></table></figure></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;调用接口实现每天发送鸡汤和翻译&lt;br&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python-聊天机器人</title>
    <link href="http://yoursite.com/2019/01/26/Python-%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
    <id>http://yoursite.com/2019/01/26/Python-聊天机器人/</id>
    <published>2019-01-26T03:31:09.000Z</published>
    <updated>2019-01-26T06:21:36.206Z</updated>
    
    <content type="html"><![CDATA[<p>首先要去<a href="http://www.tuling123.com/" target="_blank" rel="noopener">www.tuling123.com</a>注册一个机器人获得一个机器人调用的api_key<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> wxpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment">#图灵机器人</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">talks_robot</span><span class="params">(info = <span class="string">'test'</span>)</span>:</span></span><br><span class="line">    api_url = <span class="string">'http://www.tuling123.com/openapi/api'</span></span><br><span class="line">    apikey = <span class="string">'5xxxxxxxxxxxxxxxxxxxxd'</span></span><br><span class="line">    data = &#123;<span class="string">'key'</span>: apikey,</span><br><span class="line">                <span class="string">'info'</span>: info&#125;</span><br><span class="line">    req = requests.post(api_url, data=data).text</span><br><span class="line">    replys = json.loads(req)[<span class="string">'text'</span>]</span><br><span class="line">    <span class="keyword">return</span> replys</span><br><span class="line"></span><br><span class="line"><span class="comment">#微信自动回复</span></span><br><span class="line">robot = Bot()</span><br><span class="line"><span class="comment"># 回复来自其他好友、群聊和公众号的消息</span></span><br><span class="line"><span class="meta">@robot.register()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reply_my_friend</span><span class="params">(msg)</span>:</span></span><br><span class="line">    message = <span class="string">'&#123;&#125;'</span>.format(msg.text)</span><br><span class="line">    replys = talks_robot(info=message)</span><br><span class="line">    <span class="keyword">return</span> replys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 监听和自动处理消息</span></span><br><span class="line">robot.start()</span><br><span class="line">embed()</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先要去&lt;a href=&quot;http://www.tuling123.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;www.tuling123.com&lt;/a&gt;注册一个机器人获得一个机器人调用的api_key&lt;br&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战-kNN</title>
    <link href="http://yoursite.com/2019/01/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-kNN/"/>
    <id>http://yoursite.com/2019/01/26/机器学习实战-kNN/</id>
    <published>2019-01-26T03:25:47.000Z</published>
    <updated>2019-01-26T06:22:40.098Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>邻近算法，或者说K最近邻(kNN，k-NearestNeighbor)分类算法是<a href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98" target="_blank" rel="noopener">数据挖掘</a>分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。</p></blockquote><a id="more"></a><blockquote><p>kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 kNN方法在类别决策时，只与极少量的相邻样本有关。由于kNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，kNN方法较其他方法更为适合。</p></blockquote><p>用 途 : 用于分类，对未知事物的识别</p><blockquote><p>K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 KNN方法虽然从原理上也依赖于极限定理，但在类别决策时，只与极少量的相邻样本有关。由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。</p></blockquote><p>主要步骤如下：</p><p>对未知类别属性的数据集中的每个点依次执行以下操作:</p><p>(1) 计算已知类别数据集中的点与当前点之间的距离</p><p>(2) 按照距离递增次序排序</p><p>(3) 选取与当前点距离最小的k个点</p><p>(4) 确定前k个点所在类别的出现频率</p><p>(5) 返回前k个点出现频率最高的类别作为当前点的预测分类</p><h3 id="kNN算法的优缺点："><a href="#kNN算法的优缺点：" class="headerlink" title="kNN算法的优缺点："></a>kNN算法的优缺点：</h3><blockquote><p>KNN算法的优点：</p><p>1）简单、有效。<br>2）重新训练的代价较低（类别体系的变化和训练集的变化，在Web环境和电子商务应用中是很常见的）。<br>3）计算时间和空间线性于训练集的规模（在一些场合不算太大）。<br>4）由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。<br>5）该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。</p><p>KNN算法缺点：</p><p>1）KNN算法是懒散学习方法（lazy learning,基本上不学习），一些积极学习的算法要快很多。<br>2）类别评分不是规格化的（不像概率评分）。<br>3）输出的可解释性不强，例如决策树的可解释性较强。<br>4）该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。<br>5）计算量较大。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。</p></blockquote><h3 id="kNN算法的Python实现"><a href="#kNN算法的Python实现" class="headerlink" title="kNN算法的Python实现"></a>kNN算法的Python实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> * <span class="comment">#导入numpy科学计算包</span></span><br><span class="line"><span class="keyword">import</span> operator     <span class="comment">#导入运算符模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#加载数据的方法，返回样本数据（每一行是一个样本）和样本标签</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    group = array([[<span class="number">90</span>,<span class="number">100</span>],[<span class="number">88</span>,<span class="number">90</span>],[<span class="number">85</span>,<span class="number">95</span>],[<span class="number">10</span>,<span class="number">20</span>],[<span class="number">30</span>,<span class="number">40</span>],[<span class="number">50</span>,<span class="number">30</span>]])    \<span class="comment">#样本点数据</span></span><br><span class="line">    labels = [<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'A'</span>，<span class="string">'D'</span>，<span class="string">'D'</span>,<span class="string">'D'</span>]</span><br><span class="line">    <span class="keyword">return</span> group,labels</span><br><span class="line"></span><br><span class="line"><span class="comment">#分类方法 传入的dataset需是array数组</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataSet, labels, k)</span>:</span>    <span class="comment">#inX为输入样本，例如[85,90]</span></span><br><span class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]              <span class="comment">#求出输入数据矩阵的行数（样本个数）</span></span><br><span class="line">    diffMat = tile(inX, (dataSetSize,<span class="number">1</span>)) - dataSet    <span class="comment">#求矩阵差</span></span><br><span class="line">    sqDiffMat = diffMat ** <span class="number">2</span>                  </span><br><span class="line">    sqDistance = sqDiffMat.sum(axis = <span class="number">1</span>)          <span class="comment">#平方和</span></span><br><span class="line">    distance = sqDistance ** <span class="number">0.5</span>               <span class="comment">#测试样本点距离每个样本点的距离</span></span><br><span class="line">    sortedDistance = distance.argsort()          <span class="comment">#将距离按升序排列</span></span><br><span class="line"></span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        voteLabel = labels[sortedDistance[i]]      <span class="comment">#遍历前k个样本的标签</span></span><br><span class="line">        classCount[voteLabel] = classCount.get(voteLabel,<span class="number">0</span>) + <span class="number">1</span>  <span class="comment">#对标签进行计数，即每一类出现的次数</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(<span class="number">1</span>),reverse = <span class="keyword">True</span>)  <span class="comment">#将计数后的标签按降序进行排列，得到元组列表</span></span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>注：</p><p>[1].shape函数是numpy.core.fromnumeric中的函数，它的功能是读取矩阵的长度，比如shape[0]就是读取矩阵第一维度的长度。在这里可以求出矩阵的行数，也就是样本个数。</p><p>[2].diffMat为差值矩阵</p><p>[3].计算平方差后按行（<a href="https://blog.csdn.net/u012005313/article/details/49153261" target="_blank" rel="noopener">axis=1/0，按行/列相加</a>）相加，再开平方。即可求出距离。</p><p>[4].<a href="https://blog.csdn.net/weixin_38705903/article/details/79231551" target="_blank" rel="noopener">Python get()函数作用</a></p><p>[5]. <code>sorted(classCount.items().key = operator.itemgetter(1),reverse = True)</code>的理解：</p><p><a href="https://www.linuxidc.com/Linux/2013-10/90820.htm" target="_blank" rel="noopener">Python中的sorted函数以及operator.itemgetter函数</a></p><p>[6].计算距离公式为<a href="https://blog.csdn.net/Losteng/article/details/50893931" target="_blank" rel="noopener">欧氏距离公式。</a></p><p>最后，将classCount字典分解为元组列表，然后使用程序第二行导入运算符模块的itemgetter方法，按照第二个元素的次序对元组进行排序 此处的排序为逆序，即按照从最大到最小次序排序，最后返回发生频率最高的元素标签。</p><p>为了预测数据所在分类，在Python提示符中输入下列命令:</p><p><code>&gt;&gt;&gt;kNN.classify0([85,90],group,lalels,3)</code></p><p>结果为A。</p><p>这就是kNN的一个简单实例.</p>]]></content>
    
    <summary type="html">
    
      此文章是对于《机器学习实战》一本书中kNN部分的笔记
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="机器学习实战" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
    
  </entry>
  
</feed>
